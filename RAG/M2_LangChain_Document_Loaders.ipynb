{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b05fec9-f87f-4d2b-8be7-0120e7892df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34847caf-ada0-4189-98ce-e7756bed7e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mjyiD8I5r7jB"
   },
   "source": [
    "# Exploring Document Loaders in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7468c5-aafb-4844-9b45-5f590b94974f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install OpenAI, HuggingFace and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "467c9b6b-d928-4aa2-92df-413d78aeedc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.2.0 requires pydantic<2,>=1.8.1, but you have pydantic 2.10.6 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain==0.3.11\n",
    "!pip install -q langchain-openai==0.2.12\n",
    "!pip install -q langchain-community==0.3.11\n",
    "!pip install -q jq==1.7.0\n",
    "!pip install -q pypdf==4.2.0\n",
    "!pip install -q PyMuPDF==1.24.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf757ad-c77d-4cea-9f04-34fda9cadb58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For Windows\n",
    "# !winget install jqlang.jq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aba7b1e-f86c-4553-ac19-465e529e6b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CB6lHzbz5a10",
    "outputId": "4356e612-f06c-4108-c6f8-2bc6774fa65b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# takes 2 - 5 mins to install on Colab\n",
    "!pip install -q \"unstructured[all-docs]==0.14.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fd013f-09e1-4ccf-827e-564255a3fe9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZTFImul36TRH"
   },
   "source": [
    "After installing `unstructured`above remember to restart your session when it shows you the following popup, if it doesn't go to `Runtime`and `Restart Session`\n",
    "\n",
    "![](https://i.imgur.com/UOBaotk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f20ce767-c6ac-4f4f-9959-6245ad3b3296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhEW-tOywUgt",
    "outputId": "1b1eb626-c22d-4358-f41c-ea6e0481d141"
   },
   "outputs": [],
   "source": [
    "# install OCR dependencies for unstructured\n",
    "# !sudo apt-get install tesseract-ocr\n",
    "# !sudo apt-get install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8b3ec1-6027-43f0-a0c7-93799ef3b477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pydantic in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cb31971-3946-41bf-96a8-49828c55f0b2/lib/python3.10/site-packages (2.10.6)\nRequirement already satisfied: typing-extensions>=4.12.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cb31971-3946-41bf-96a8-49828c55f0b2/lib/python3.10/site-packages (from pydantic) (4.12.2)\nRequirement already satisfied: pydantic-core==2.27.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cb31971-3946-41bf-96a8-49828c55f0b2/lib/python3.10/site-packages (from pydantic) (2.27.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cb31971-3946-41bf-96a8-49828c55f0b2/lib/python3.10/site-packages (from pydantic) (0.7.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9885f2eb-7927-489e-8cd7-f79bad518861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytesseract\n",
    "!pip install -q pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1069961-357c-4050-b608-c29eae53334e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70327d5b-cef2-4d6c-826e-910d7510217c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aqX0BkkWZ_e0"
   },
   "source": [
    "## Document Loaders\n",
    "\n",
    "Document loaders are used to import data from various sources into LangChain as `Document` objects. A `Document` typically includes a piece of text along with its associated metadata.\n",
    "\n",
    "### Examples of Document Loaders:\n",
    "\n",
    "- **Text File Loader:** Loads data from a simple `.txt` file.\n",
    "- **Web Page Loader:** Retrieves the text content from any web page.\n",
    "- **YouTube Video Transcript Loader:** Loads transcripts from YouTube videos.\n",
    "\n",
    "### Functionality:\n",
    "\n",
    "- **Load Method:** Each document loader has a `load` method that enables the loading of data as documents from a pre-configured source.\n",
    "- **Lazy Load Option:** Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n",
    "\n",
    "For more detailed information, visit [LangChain's document loader documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eddc6acc-40fa-47e1-8bd1-4b81a19e1e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CEx_nNkHLqZY"
   },
   "source": [
    "### Text Loader\n",
    "\n",
    "The simplest loader reads in a file as text and places it all into one document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a641a21-591f-402a-8d3f-c75d7cf12baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Al7y4r93LKA4",
    "outputId": "9205e51a-6f0e-490c-e1f8-34250e7a4d5a"
   },
   "outputs": [],
   "source": [
    "# !curl -o README.md https://raw.githubusercontent.com/langchain-ai/langchain/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6a5dc9-5c31-424b-ba94-99d44b663e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3ehpI19eLKEo"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"./docs/dummy.txt\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9431bb0-6ceb-488d-90a8-1bd6e11f6268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxR60glzvzQN",
    "outputId": "a6152598-37ca-4d31-cc13-ad18a130256b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9976f03-9e90-439a-a13c-0c87de558dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole documents are: \n[Document(metadata={'source': './docs/dummy.txt'}, page_content='Quod equidem non reprehendo;\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?\\n\\nIam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?\\n\\nQuis istum dolorem timet?\\nSummus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.\\n\\nEx eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.\\nQuod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?\\n\\nQuem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.\\nStulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.\\nEsse enim quam vellet iniquus iustus poterat inpune.\\nQuae autem natura suae primae institutionis oblita est?\\nVerum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;\\nHoc est non modo cor non habere, sed ne palatum quidem.\\nVoluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?\\n\\nIdemne, quod iucunde?\\nHaec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?')]\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"The whole documents are: \\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a61e6f-c842-4352-98e0-4fb4c99dd968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe number of documents : 1\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe number of documents : {len(doc)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deab02e5-0c35-4eb6-aa38-3cc6a4c1ce47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cac3efc2-ac35-4b7d-a7e9-bdaf45cb00ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n Type of first documents : <class 'langchain_core.documents.base.Document'>\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Type of first documents : {type(doc[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65f4e1b-7550-43c0-b721-caf06989f32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of first document : \nQuod equidem non reprehendo;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?\n\nIam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?\n\nQuis istum dolorem timet?\nSummus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.\n\nEx eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.\nQuod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?\n\nQuem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.\nStulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.\nEsse enim quam vellet iniquus iustus poterat inpune.\nQuae autem natura suae primae institutionis oblita est?\nVerum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;\nHoc est non modo cor non habere, sed ne palatum quidem.\nVoluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?\n\nIdemne, quod iucunde?\nHaec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Content of first document : \\n{doc[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34421aa-4fbf-4254-a6ec-41be8960bb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata of first document : \n{'source': './docs/dummy.txt'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Metadata of first document : \\n{doc[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9eb1a91-56e3-42a5-a3eb-0769c633a1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFbJx7Q9LKG4",
    "outputId": "8b9165d3-d585-4f9b-e3b2-e592f20c8898",
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quod equidem non reprehendo;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura \n"
     ]
    }
   ],
   "source": [
    "print(doc[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1c6779-0ac4-4c41-b6f1-c01ee6c77b5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oTf3za4x7CtK"
   },
   "source": [
    "### Markdown Loader\n",
    "\n",
    "Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\n",
    "\n",
    "This showcases how to load Markdown documents into a langchain document format that we can use in our pipelines and chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "165940a3-babf-4448-93e8-1c9f0976707e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sEmzUQK9_640"
   },
   "source": [
    "Load the whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d633da6e-636c-4371-8d3b-0052a0260237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "YJt1cdnFOeP0"
   },
   "source": [
    "Download nltk packages if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02a0379a-9039-4ac4-aa7d-d655cb9a861a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CD-IBE4POUwu",
    "outputId": "5426d839-b8c2-4266-de76-9a3d1c1b2699"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0e70af3-072b-4fd2-939d-85ccf9d17894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "D1uZEc-f8TyV"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"./docs/README.md\", mode='single')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97daa579-bff8-4b20-aa7c-824dd1c80616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPkNbhXv7Z3B",
    "outputId": "e17ac1fd-e26e-4e3d-d46e-75917d897aee"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2836ffc7-4c2e-484c-8642-4664e706e89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "m547b2uc8vy9",
    "outputId": "9422bb91-e0e7-4546-a1b7-873a7c1d559c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e3435e-2df5-4c97-921b-c712531e83fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './docs/README.md'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "265eecdb-10bb-47b7-ba0d-2d6cdb850dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gia_n-T8Ytn",
    "outputId": "34dfafd1-ba2e-4092-b050-6a8854e6b27b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦜️🔗 LangChain\n\n⚡ Build context-aware reasoning applications ⚡\n\nLooking for the JS/TS library? Check \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47245010-23da-40cf-a5f2-d9bb11da37a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AVyZosgZ_-U8"
   },
   "source": [
    "Load document and separate based on elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2873e5-e150-47a1-9085-e1218303c6e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SOPDX8w85MeG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"./docs/README.md\", mode=\"elements\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4032ba16-f0fd-4154-9d97-273ff8bd0381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr4WVaEg-qTr",
    "outputId": "172359b0-0eaa-4c53-fe4a-80eea4670b23"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1782d9f8-145b-4d13-bb50-f042ca39aa44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpfQwXmD-rji",
    "outputId": "a27ece4d-5a9d-4e96-b1c2-e85bbf4a3e8a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a'}, page_content='🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '80d06543c0c2b75ca147f3509e518a47'}, page_content='⚡ Build context-aware reasoning applications ⚡'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'd68276ff4183b272b9dec78754e769b1'}, page_content='Looking for the JS/TS library? Check out LangChain.js.'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'e26407512e7a4e24a519ceb1a9dc980d'}, page_content='To help you ship LangChain apps to production faster, check out LangSmith.\\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\\nFill out this form to speak with our sales team.'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '738df4293c5c58dbaa314f6e31f2c15c'}, page_content='Quick Install'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '17c3a345872b736bc4edb6cdae73f45a'}, page_content='With pip:'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '0dbefd69ddc0b2c597762e19b29f8e27'}, page_content='bash\\npip install langchain'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '3c98bc06e5ef8caa9446bb6f1d33dde0'}, page_content='With conda:'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '3c98bc06e5ef8caa9446bb6f1d33dde0', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'e171db3a99f130885e84934a210c7dd8'}, page_content='bash\\nconda install langchain -c conda-forge'),\n",
       " Document(metadata={'source': './docs/README.md', 'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md', 'category': 'Title', 'element_id': 'a8858c1e5d7fad28685e95bd1bbeacc1'}, page_content='🤔 What is LangChain?')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977bbb73-ddfa-4030-9b5c-ac72520dd3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUcCM72S-vHb",
    "outputId": "0f37dbe2-825f-4a7e-92ce-e8e9cf38a862"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ListItem': 26, 'Title': 20, 'NarrativeText': 17})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([doc.metadata['category'] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8a51f5-e89c-4205-a2ae-da0946e52f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'source': './docs/README.md',\n",
       " 'last_modified': '2025-03-06T02:27:51',\n",
       " 'languages': ['eng'],\n",
       " 'filetype': 'text/markdown',\n",
       " 'file_directory': './docs',\n",
       " 'filename': 'README.md',\n",
       " 'category': 'Title',\n",
       " 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6057d6a-fe19-4b44-8bf6-ed1e06d0cce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7h8Hb5sKABXD"
   },
   "source": [
    "Comparing Unstructured.io loaders vs LangChain wrapper API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c28f2e-20f9-4346-9a2b-09e47bca2ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "RZqKGM8q8RpS"
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "\n",
    "docs = partition_md(filename=\"./docs/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2719fe63-be07-4071-9510-73394cfb0aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQES4WJY80IM",
    "outputId": "e5841ac6-7970-4682-916e-71289182bab9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2159dae1-20df-43ac-9dba-baac3e95517f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88F0QIA2-_96",
    "outputId": "5b9878bd-c9c0-4ec6-a2be-aa35fde42975"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x7f22a98c9a80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f22a98cae30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f22a98c80d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f22a98cb0d0>,\n",
       " <unstructured.documents.elements.Title at 0x7f22a98c9c30>,\n",
       " <unstructured.documents.elements.Title at 0x7f22a98c90f0>,\n",
       " <unstructured.documents.elements.Title at 0x7f22a98c8370>,\n",
       " <unstructured.documents.elements.Title at 0x7f22a98ca860>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x7f22a98cb400>,\n",
       " <unstructured.documents.elements.Title at 0x7f22a98ca3b0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205e187b-c19e-4410-bfa9-5248e0d504de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oVGTYLJf7fgC",
    "outputId": "cfe389b6-fbdf-4ef2-ebfa-e6cf9afbbc49"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'type': 'Title',\n",
       " 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
       " 'text': '🦜️🔗 LangChain',\n",
       " 'metadata': {'last_modified': '2025-03-06T02:27:51',\n",
       "  'languages': ['eng'],\n",
       "  'filetype': 'text/markdown',\n",
       "  'file_directory': './docs',\n",
       "  'filename': 'README.md'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74313d77-a1a7-4538-8d98-bdaa41fa73be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDMICcgV_GiF",
    "outputId": "e2a3115f-6bc1-4a72-94d2-91fd66531434"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'type': 'NarrativeText',\n",
       " 'element_id': '80d06543c0c2b75ca147f3509e518a47',\n",
       " 'text': '⚡ Build context-aware reasoning applications ⚡',\n",
       " 'metadata': {'last_modified': '2025-03-06T02:27:51',\n",
       "  'languages': ['eng'],\n",
       "  'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
       "  'filetype': 'text/markdown',\n",
       "  'file_directory': './docs',\n",
       "  'filename': 'README.md'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dafc7328-63a2-42b8-a554-7b1e51eba2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g87ZcUNG_Ka_",
    "outputId": "6a69dd92-925b-4db9-dec8-0905d5793c72"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='🦜️🔗 LangChain'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='⚡ Build context-aware reasoning applications ⚡'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='Looking for the JS/TS library? Check out LangChain.js.'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='To help you ship LangChain apps to production faster, check out LangSmith.\\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\\nFill out this form to speak with our sales team.'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='Quick Install'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='With pip:'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='bash\\npip install langchain'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='With conda:'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'parent_id': '3c98bc06e5ef8caa9446bb6f1d33dde0', 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='bash\\nconda install langchain -c conda-forge'),\n",
       " Document(metadata={'last_modified': '2025-03-06T02:27:51', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': './docs', 'filename': 'README.md'}, page_content='🤔 What is LangChain?')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "lc_docs = [Document(page_content=doc.text,\n",
    "                    metadata=doc.metadata.to_dict())\n",
    "              for doc in docs]\n",
    "lc_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84a1e6a-e6d3-4c99-8d42-48d669f57f10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZSSBO_Y3fB8P"
   },
   "source": [
    "### CSV Loader\n",
    "\n",
    "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n",
    "\n",
    "LangChain implements a CSV Loader that will load CSV files into a sequence of `Document` objects. Each row of the CSV file is converted to one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e1a945-ba36-40d6-aa77-618b66cb06ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QuoCrfQUODLt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with some dummy real estate data\n",
    "data = {\n",
    "    'Property_ID': [101, 102, 103, 104, 105],\n",
    "    'Address': ['123 Elm St', '456 Oak St', '789 Pine St', '321 Maple St', '654 Cedar St'],\n",
    "    'City': ['Springfield', 'Rivertown', 'Laketown', 'Hillside', 'Sunnyvale'],\n",
    "    'State': ['CA', 'TX', 'FL', 'NY', 'CO'],\n",
    "    'Zip_Code': [98765, 87654, 76543, 65432, 54321],\n",
    "    'Bedrooms': [3, 2, 4, 3, 5],\n",
    "    'Bathrooms': [2, 1, 3, 2, 4],\n",
    "    'Listing_Price': [500000, 350000, 600000, 475000, 750000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('./docs/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fec601dd-a56e-4779-9e13-3f98a9bc0f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hG-_cJFULKI3"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"./docs/data.csv\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aefb420c-07a8-4ed1-b918-4f5a0ee4b4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dgrc_HhzLKMF",
    "outputId": "89d644b1-2224-4e63-9fef-88a8f8d7041b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './docs/data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 1}, page_content='Property_ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip_Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nListing_Price: 350000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 2}, page_content='Property_ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip_Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nListing_Price: 600000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 3}, page_content='Property_ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip_Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 475000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 4}, page_content='Property_ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip_Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nListing_Price: 750000')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bad3ac51-48f6-41d5-ab07-eec708dffc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8vVzwmseaoH",
    "outputId": "8349d972-89ab-4943-adb4-f4b0fc763619"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e5aba8-6967-4f45-a5d9-3b25b7b15016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdENGJXQOXA-",
    "outputId": "d4a5cf0c-5ba9-4591-a831-dc0fbdd18869"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property_ID: 101\nAddress: 123 Elm St\nCity: Springfield\nState: CA\nZip_Code: 98765\nBedrooms: 3\nBathrooms: 2\nListing_Price: 500000\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a49251-1f47-429a-b1ca-1db5d4ff992e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4HVFIEiiB1Wu"
   },
   "source": [
    "`CSVLoader` will accept a `csv_args` kwarg that supports customization of arguments passed to Python's csv.`DictReader`. See the [`csv` module](https://docs.python.org/3/library/csv.html) documentation for more information of what `csv` args are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccfa406f-7c5f-4826-b7a6-444d921bee30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XxLiCAl2CHE2"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"./docs/data.csv\",\n",
    "                   csv_args={\n",
    "                      \"delimiter\": \",\",\n",
    "                      \"quotechar\": '\"',\n",
    "                      \"fieldnames\": [\"Property ID\", \"Address\", \"City\", \"State\",\n",
    "                                     \"Zip Code\", \"Bedrooms\", \"Bathrooms\", \"Price\"],\n",
    "                   },\n",
    "                  )\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b83aa55-48e5-4a6d-9e32-b5fd00ff5446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WuekvnIDShM",
    "outputId": "3aa3f04a-ebb1-4e77-87d0-f14a3907aabd"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './docs/data.csv', 'row': 0}, page_content='Property ID: Property_ID\\nAddress: Address\\nCity: City\\nState: State\\nZip Code: Zip_Code\\nBedrooms: Bedrooms\\nBathrooms: Bathrooms\\nPrice: Listing_Price'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 1}, page_content='Property ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 500000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 2}, page_content='Property ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nPrice: 350000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 3}, page_content='Property ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nPrice: 600000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 4}, page_content='Property ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 475000'),\n",
       " Document(metadata={'source': './docs/data.csv', 'row': 5}, page_content='Property ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nPrice: 750000')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb3e51d-1276-4b8d-af57-2888bcb22265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property ID: Property_ID\nAddress: Address\nCity: City\nState: State\nZip Code: Zip_Code\nBedrooms: Bedrooms\nBathrooms: Bathrooms\nPrice: Listing_Price\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d879ce71-dde4-4add-908d-64aea322f3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property ID: 101\nAddress: 123 Elm St\nCity: Springfield\nState: CA\nZip Code: 98765\nBedrooms: 3\nBathrooms: 2\nPrice: 500000\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e1e490-7ee6-431c-9035-8142c9260cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VXseBM98BgkG"
   },
   "source": [
    "Unstructured.io loads the entire CSV as a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d612b30-f7fe-422f-91f3-8a8d1ca52e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "89SAdDs7A4eR"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredCSVLoader\n",
    "\n",
    "loader = UnstructuredCSVLoader(\"./docs/data.csv\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221a3d6c-3c7c-4711-aec1-df66565b7139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvic6kW8BEn3",
    "outputId": "add1135c-ade4-41c9-db89-bf49bca703c2"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53f2ecc-bbe9-4f8a-81c3-1696f5aaeba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54gFzsugBnad",
    "outputId": "2dcbc766-affd-44cb-ec29-d8eb86020c33"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/data.csv'}, page_content='\\n\\n\\nProperty_ID\\nAddress\\nCity\\nState\\nZip_Code\\nBedrooms\\nBathrooms\\nListing_Price\\n\\n\\n101\\n123 Elm St\\nSpringfield\\nCA\\n98765\\n3\\n2\\n500000\\n\\n\\n102\\n456 Oak St\\nRivertown\\nTX\\n87654\\n2\\n1\\n350000\\n\\n\\n103\\n789 Pine St\\nLaketown\\nFL\\n76543\\n4\\n3\\n600000\\n\\n\\n104\\n321 Maple St\\nHillside\\nNY\\n65432\\n3\\n2\\n475000\\n\\n\\n105\\n654 Cedar St\\nSunnyvale\\nCO\\n54321\\n5\\n4\\n750000\\n\\n\\n')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918c3c30-b5ec-4115-8200-3ab08003bf76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\nProperty_ID\nAddress\nCity\nState\nZip_Code\nBedrooms\nBathrooms\nListing_Price\n\n\n101\n123 Elm St\nSpringfield\nCA\n98765\n3\n2\n500000\n\n\n102\n456 Oak St\nRivertown\nTX\n87654\n2\n1\n350000\n\n\n103\n789 Pine St\nLaketown\nFL\n76543\n4\n3\n600000\n\n\n104\n321 Maple St\nHillside\nNY\n65432\n3\n2\n475000\n\n\n105\n654 Cedar St\nSunnyvale\nCO\n54321\n5\n4\n750000\n\n\n\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f55c2c2a-850f-4a60-b501-1c1417a02903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DrHu4ts4fHgH"
   },
   "source": [
    "### JSON Loader\n",
    "\n",
    "[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n",
    "\n",
    "[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\n",
    "\n",
    "LangChain implements a [JSONLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) to convert JSON and JSONL data into LangChain `Document` objects. It uses a specified [`jq` schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n",
    "\n",
    "It uses the `jq` python package. Check out [this manual](https://jqlang.github.io/jq/manual/) for a detailed documentation of the `jq` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5604798-bdcb-4d9f-97cc-4aad41aab22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ALFBPgsnOd0L"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample data dictionary similar to the one you provided but with modified contents\n",
    "data = {\n",
    "    'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_meeting.jpg'},\n",
    "    'is_still_participant': True,\n",
    "    'joinable_mode': {'link': '', 'mode': 1},\n",
    "    'magic_words': [],\n",
    "    'messages': [\n",
    "        {'content': 'See you soon!',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675597571851},\n",
    "        {'content': 'Thanks for the update! See you then.',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675597435669},\n",
    "        {'content': 'Actually, the green one is sold out.',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675596277579},\n",
    "        {'content': 'I was hoping to purchase the green one!',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675595140251},\n",
    "        {'content': 'I’m really interested in the green one, not the red!',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675595109305},\n",
    "        {'content': 'Here’s the $150 for it.',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675595068468},\n",
    "        {'photos': [{'creation_timestamp': 1675595059,\n",
    "                     'uri': 'image_of_the_item.jpg'}],\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675595060730},\n",
    "        {'content': 'It typically sells for at least $200 online',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675595045152},\n",
    "        {'content': 'How much are you asking?',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675594799696},\n",
    "        {'content': 'Good morning! $50 is far too low.',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675577876645},\n",
    "        {'content': 'Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675549022673}\n",
    "    ],\n",
    "    'participants': [{'name': 'User A'}, {'name': 'User B'}],\n",
    "    'thread_path': 'inbox/User A and User B chat',\n",
    "    'title': 'User A and User B chat'\n",
    "}\n",
    "\n",
    "# Save the modified data to a JSON file\n",
    "with open('./docs/chat_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1d6055-b82c-4a77-8795-9c51e66af8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bZ9YwY4xG7KD"
   },
   "source": [
    "To load the full data as a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bb0c47-006c-4706-b9d3-065edd525234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aD1kjnQRQ0h5"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='./docs/chat_data.json',\n",
    "    jq_schema='.',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98706664-2d7d-4fd9-9c8f-a217156006ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQIAvS8rGpNx",
    "outputId": "e8f05f60-5a26-435f-aa06-f8947a7625c1"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "455c6803-a8d4-4d66-9f8a-23ddf0b4a143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPmHC3aYRgFA",
    "outputId": "5c858b29-c7ba-49b2-a998-9fba40162360"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}, page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ddbf7c-7ebb-454f-a4cc-1f02c9840d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "TY48CWHhG_KF"
   },
   "source": [
    "Suppose we are interested in extracting the values under the `messages` key of the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b21265-5bb5-4163-b11a-488c279fee93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VC5cWidHSFuR",
    "outputId": "76200c4a-b2ca-4652-bf8f-c79a08a1796c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}, page_content='{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 2}, page_content='{\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 3}, page_content='{\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 4}, page_content='{\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 5}, page_content='{\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 6}, page_content='{\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 7}, page_content='{\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 8}, page_content='{\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 9}, page_content='{\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 10}, page_content='{\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 11}, page_content='{\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='./docs/chat_data.json',\n",
    "    jq_schema='.messages[]',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df52034d-a6f5-4135-86c2-2ad99b6aea0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HK59yCK3H-C-"
   },
   "source": [
    "Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb10b5b-8d2e-4c1f-903a-f3834cb3e03f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7Bq2FHlSVb7",
    "outputId": "00159a03-2bfa-40b3-b522-923494450ee3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}, page_content='See you soon!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 2}, page_content='Thanks for the update! See you then.'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 3}, page_content='Actually, the green one is sold out.'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 4}, page_content='I was hoping to purchase the green one!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 5}, page_content='I’m really interested in the green one, not the red!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 6}, page_content='Here’s the $150 for it.'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 7}, page_content=''),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 8}, page_content='It typically sells for at least $200 online'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 9}, page_content='How much are you asking?'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 10}, page_content='Good morning! $50 is far too low.'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 11}, page_content='Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='./docs/chat_data.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18dfd786-5d93-4829-b387-51739d7b5607",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Basic JSON Loading\n",
    "For robust loading, especially with diverse file types, consider these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e254bb-0169-4f1b-a392-9280ac6c3d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a09f5dc-e7f1-4c5f-aae4-df7b0d88a11a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'participants': [{'name': 'User 1'}, {'name': 'User 2'}], 'messages': [{'sender_name': 'User 2', 'timestamp_ms': 1675597571851, 'content': 'Bye!'}, {'sender_name': 'User 1', 'timestamp_ms': 1675597435669, 'content': 'Oh no worries! Bye'}, {'sender_name': 'User 2', 'timestamp_ms': 1675596277579, 'content': 'No Im sorry it was my mistake, the blue one is not for sale'}, {'sender_name': 'User 1', 'timestamp_ms': 1675595140251, 'content': 'I thought you were selling the blue one!'}, {'sender_name': 'User 1', 'timestamp_ms': 1675595109305, 'content': 'Im not interested in this bag. Im interested in the blue one!'}, {'sender_name': 'User 2', 'timestamp_ms': 1675595068468, 'content': 'Here is $129'}, {'sender_name': 'User 2', 'timestamp_ms': 1675595060730, 'content': '', 'photos': [{'uri': 'url_of_some_picture.jpg', 'creation_timestamp': 1675595059}]}, {'sender_name': 'User 2', 'timestamp_ms': 1675595045152, 'content': 'Online is at least $100'}, {'sender_name': 'User 1', 'timestamp_ms': 1675594799696, 'content': 'How much do you want?'}, {'sender_name': 'User 2', 'timestamp_ms': 1675577876645, 'content': 'Goodmorning! $50 is too low.'}, {'sender_name': 'User 1', 'timestamp_ms': 1675549022673, 'content': 'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!'}], 'title': 'User 1 and User 2 chat', 'is_still_participant': True, 'thread_path': 'inbox/User 1 and User 2 chat', 'magic_words': [], 'image': {'uri': 'image_of_the_chat.jpg', 'creation_timestamp': 1675549016}, 'joinable_mode': {'mode': 1, 'link': ''}}\n"
     ]
    }
   ],
   "source": [
    "file_path = './docs/facebook_chat.json'\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ebca45-4114-4646-b1d3-045c74360d64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Using JSONLoader for Structured Retrieval: \n",
    "Use jq_schema to specify the data structure and extract only the required fields (Schema-Based Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a90c6df-7f20-4bb4-b397-bd0d02f24d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 1}, page_content='Bye!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 2}, page_content='Oh no worries! Bye'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 3}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 4}, page_content='I thought you were selling the blue one!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 5}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 6}, page_content='Here is $129'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 7}, page_content=''),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 8}, page_content='Online is at least $100'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 9}, page_content='How much do you want?'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 10}, page_content='Goodmorning! $50 is too low.'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 11}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='./docs/facebook_chat.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bc66369-1f2b-488f-93b1-b224f069f432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Processing JSON Lines (JSONL): \n",
    "Seamlessly handle files where each line represents a separate JSON object by setting json_lines=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69876b57-eee7-4222-bfbe-e23645ed8ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 1}, page_content='Bye!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 2}, page_content='Oh no worries! Bye'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 3}, page_content='No Im sorry it was my mistake, the blue one is not for sale')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example - JSON (Processing JSON Lines)\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='./docs/facebook_chat_messages.jsonl',\n",
    "    jq_schema='.content',\n",
    "    text_content=False,\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e7382c-2bfc-464f-aa3e-32bbabef85f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 1}, page_content='User 2'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 2}, page_content='User 1'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 3}, page_content='User 2')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example - JSON (Use jq_schema='.' and content_key for simpler extraction)\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='./docs/facebook_chat_messages.jsonl',\n",
    "    jq_schema='.',\n",
    "    content_key=\"sender_name\",\n",
    "    text_content=False,\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8e254e-0c67-442f-a5d2-0d2f77c76311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Adding Metadata from JSON: \n",
    "Use custom functions to extract additional metadata, enhancing data context and traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bff206-222e-4ae2-a9de-cf1acfc23056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}, page_content='Bye!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}, page_content='Oh no worries! Bye'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}, page_content='I thought you were selling the blue one!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}, page_content='Here is $129'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}, page_content=''),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}, page_content='Online is at least $100'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}, page_content='How much do you want?'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}, page_content='Goodmorning! $50 is too low.'),\n",
       " Document(metadata={'source': '/Workspace/Users/sourav.banerjee@databricks.com/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example - JSON (Adding Metadata from JSON)\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"sender_name\"] = record.get(\"sender_name\")\n",
    "    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='./docs/facebook_chat.json',\n",
    "    jq_schema='.messages[]',\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func # Add metadata from JSON\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a13f3b4-97de-4648-b3b4-8dccaef03eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tZB2fxI9fKxC"
   },
   "source": [
    "### PDF Loaders\n",
    "\n",
    "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
    "\n",
    "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your use-case and through experimentation.\n",
    "\n",
    "Here we will see how to load PDF documents into the LangChain `Document` format\n",
    "\n",
    "We download a research paper to experiment with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d0273c-3c67-4a99-aff2-aff59a87b4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "p2NWiC51KDbm"
   },
   "source": [
    "If the following command fails you can download the paper manually by going to http://arxiv.org/pdf/2103.15348.pdf, save it as `layoutparser_paper.pdf`and upload it on the left in Colab from the upload files option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b938a1e4-2c3c-408b-ad33-a28f81edb15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_zMe1cES7Tb",
    "outputId": "9e2f1bfe-3adf-4c57-835f-847011c9fd70"
   },
   "outputs": [],
   "source": [
    "# !wget -O 'layoutparser_paper.pdf' 'http://arxiv.org/pdf/2103.15348.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290df96a-94d9-480e-a57b-7c05abb05b70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Qs1ZBxbxfNXq"
   },
   "source": [
    "#### PyPDFLoader\n",
    "\n",
    "Here we load a PDF using `pypdf` into list of documents, where each document contains the page content and metadata with page number. Typically each PDF page becomes one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d91677-bc2b-48c5-9174-b176c5401719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0be_BLqTI0H",
    "outputId": "c9f51f6d-28cc-4572-bd93-e518335d96a8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./docs/layoutparser_paper.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff945b5f-0c81-4851-848c-0c5d955a7a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJZo2Hk9hMgb",
    "outputId": "50bb138e-e856-4e50-f1aa-f860dd939168"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a3dfe3-4405-4f72-83d7-2aceb131e197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrqsL5lhToUY",
    "outputId": "1f08a24d-2b81-4531-a05f-02739c6ba008"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/layoutparser_paper.pdf', 'page': 0}, page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f65e35-c8b7-4d8e-ba46-ff7efbde8e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xU5qyqJT7TB",
    "outputId": "52aa4dff-0efe-4fce-960c-de64dfefd100"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser : A Uniﬁed Toolkit for Deep\nLearning Based Document Image Analysis\nZejiang Shen1( \u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\nLee4, Jacob Carlson3, and Weining Li5\n1Allen Institute for AI\nshannons@allenai.org\n2Brown University\nruochen zhang@brown.edu\n3Harvard University\n{melissadell,jacob carlson }@fas.harvard.edu\n4University of Washington\nbcgl@cs.washington.edu\n5University of Waterloo\nw422li@uwaterloo.ca\nAbstract. Recent advances in document image analysis (DIA) have been\nprimarily driven by the application of neural networks. Ideally, research\noutcomes could be easily deployed in production and extended for further\ninvestigation. However, various factors like loosely organized codebases\nand sophisticated model conﬁgurations complicate the easy reuse of im-\nportant innovations by a wide audience. Though there have been on-going\neﬀorts to improve reusability and simplify deep learning (DL) model\ndevelopment in disciplines like natural language processing and computer\nvision, none of them are optimized for challenges in the domain of DIA.\nThis represents a major gap in the existing toolkit, as DIA is central to\nacademic research across a wide range of disciplines in the social sciences\nand humanities. This paper introduces LayoutParser , an open-source\nlibrary for streamlining the usage of DL in DIA research and applica-\ntions. The core LayoutParser library comes with a set of simple and\nintuitive interfaces for applying and customizing DL models for layout de-\ntection, character recognition, and many other document processing tasks.\nTo promote extensibility, LayoutParser also incorporates a community\nplatform for sharing both pre-trained models and full document digiti-\nzation pipelines. We demonstrate that LayoutParser is helpful for both\nlightweight and large-scale digitization pipelines in real-word use cases.\nThe library is publicly available at https://layout-parser.github.io .\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\n·Character Recognition ·Open Source library ·Toolkit.\n1 Introduction\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86736dc-e0fd-4b02-947e-97f52a6357b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './docs/layoutparser_paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4bfa57-a2dc-4b25-a107-77c6f3850025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcDMgTfBTrZL",
    "outputId": "e79b1ed5-c5df-479c-9ba3-9a2ab99f1208"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\nTable 1: Current layout detection models in the LayoutParser model zoo\nDataset Base Model1Large Model Notes\nPubLayNet [38] F / M M Layouts of modern scientiﬁc documents\nPRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\nNewspaper [17] F - Layouts of scanned US newspapers from the 20th century\nTableBank [18] F F Table region on modern scientiﬁc and business document\nHJDataset [31] F / M - Layouts of history Japanese documents\n1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\nvs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\nbackbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\nR-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\nzoo in coming months.\nlayout data structures , which are optimized for eﬃciency and versatility. 3) When\nnecessary, users can employ existing or customized OCR models via the uniﬁed\nAPI provided in the OCR module . 4)LayoutParser comes with a set of utility\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\nis also highly customizable, via its integration with functions for layout data\nannotation and model training . We now provide detailed descriptions for each\ncomponent.\n3.1 Layout Detection Models\nInLayoutParser , a layout model takes a document image as an input and\ngenerates a list of rectangular boxes for the target content regions. Diﬀerent\nfrom traditional methods, it relies on deep convolutional neural networks rather\nthan manually curated rules to identify content regions. It is formulated as an\nobject detection problem and state-of-the-art models like Faster R-CNN [ 28] and\nMask R-CNN [ 12] are used. This yields prediction results of high accuracy and\nmakes it possible to build a concise, generalized interface for layout detection.\nLayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\nperform layout detection with only four lines of code in Python:\n1import layoutparser as lp\n2image = cv2. imread (\" image_file \") # load images\n3model = lp. Detectron2LayoutModel (\n4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\n5layout = model . detect ( image )\nLayoutParser provides a wealth of pre-trained model weights using various\ndatasets covering diﬀerent languages, time periods, and document types. Due to\ndomain shift [ 7], the prediction performance can notably drop when models are ap-\nplied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\ndocument structures and layouts vary greatly in diﬀerent domains, it is important\nto select models trained on a dataset similar to the test samples. A semantic syntax\nis used for initializing the model weights in LayoutParser , using both the dataset\nname and model name lp://<dataset-name>/<model-architecture-name> .\n"
     ]
    }
   ],
   "source": [
    "print(pages[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f175e2d1-687c-4fda-bf74-afbfa3a3f05f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "x_T0_7KtfUJj"
   },
   "source": [
    "#### PyMuPDFLoader\n",
    "\n",
    "This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. It uses the `pymupdf` library internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac90767-218c-491e-b1e2-c94755cbb04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pymupdf in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cb31971-3946-41bf-96a8-49828c55f0b2/lib/python3.10/site-packages (1.24.5)\nCollecting pymupdf\n  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.0/20.0 MB 49.5 MB/s eta 0:00:00\nInstalling collected packages: pymupdf\n  Attempting uninstall: pymupdf\n    Found existing installation: PyMuPDF 1.24.5\n    Uninstalling PyMuPDF-1.24.5:\n      Successfully uninstalled PyMuPDF-1.24.5\nSuccessfully installed pymupdf-1.25.3\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef728c7-dafc-48a5-8ad2-0fe61618328b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "l3KTMV_3XmfL"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"./docs/layoutparser_paper.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f9c0b4-49cd-4a8a-b5ae-bd521aa0086e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXUAxQnJhuHO",
    "outputId": "f48f2e8a-7585-43dd-89f8-ee89610a1bed"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb597aa1-504b-47a0-bb32-c2ed800829ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZM5poERdRpL",
    "outputId": "f431dce0-4df6-4f05-f2cb-9817abdd5f0a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/layoutparser_paper.pdf', 'file_path': './docs/layoutparser_paper.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84aec607-a6ed-4281-9b66-a67c29ee66cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uhr0Y1C90TH0",
    "outputId": "60569239-3c1e-4b55-fb78-6986cf2d488b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'source': './docs/layoutparser_paper.pdf',\n",
       " 'file_path': './docs/layoutparser_paper.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 16,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'pdfTeX-1.40.21',\n",
       " 'creationDate': 'D:20210622012710Z',\n",
       " 'modDate': 'D:20210622012710Z',\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac4f440-8d85-42a3-8495-1c49508c62d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdaA9hkHXmhs",
    "outputId": "81143811-c0ea-4a48-c7ce-e0287b50f876"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for Deep\nLearning Based Document Image Analysis\nZejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\nLee4, Jacob Carlson3, and Weining Li5\n1 Allen Institute for AI\nshannons@allenai.org\n2 Brown University\nruochen zhang@brown.edu\n3 Harvard University\n{melissadell,jacob carlson}@fas.harvard.edu\n4 University of Washington\nbcgl@cs.washington.edu\n5 University of Waterloo\nw422li@uwaterloo.ca\nAbstract. Recent advances in document image analysis (DIA) have been\nprimarily driven by the application of neural networks. Ideally, research\noutcomes could be easily deployed in production and extended for further\ninvestigation. However, various factors like loosely organized codebases\nand sophisticated model conﬁgurations complicate the easy reuse of im-\nportant innovations by a wide audience. Though there have been on-going\neﬀorts to improve reusability and simplify deep learning (DL) model\ndevelopment in disciplines like natural language processing and computer\nvision, none of them are optimized for challenges in the domain of DIA.\nThis represents a major gap in the existing toolkit, as DIA is central to\nacademic research across a wide range of disciplines in the social sciences\nand humanities. This paper introduces LayoutParser, an open-source\nlibrary for streamlining the usage of DL in DIA research and applica-\ntions. The core LayoutParser library comes with a set of simple and\nintuitive interfaces for applying and customizing DL models for layout de-\ntection, character recognition, and many other document processing tasks.\nTo promote extensibility, LayoutParser also incorporates a community\nplatform for sharing both pre-trained models and full document digiti-\nzation pipelines. We demonstrate that LayoutParser is helpful for both\nlightweight and large-scale digitization pipelines in real-word use cases.\nThe library is publicly available at https://layout-parser.github.io.\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\n· Character Recognition · Open Source library · Toolkit.\n1\nIntroduction\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a275a345-386f-4161-9b05-dde15b9a5ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDxTzEe0Le6I",
    "outputId": "94b7e4fa-71bf-42ad-ce87-75bed10a5371"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n5\nTable 1: Current layout detection models in the LayoutParser model zoo\nDataset\nBase Model1 Large Model\nNotes\nPubLayNet [38]\nF / M\nM\nLayouts of modern scientiﬁc documents\nPRImA [3]\nM\n-\nLayouts of scanned modern magazines and scientiﬁc reports\nNewspaper [17]\nF\n-\nLayouts of scanned US newspapers from the 20th century\nTableBank [18]\nF\nF\nTable region on modern scientiﬁc and business document\nHJDataset [31]\nF / M\n-\nLayouts of history Japanese documents\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\nvs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\nbackbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\nR-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\nzoo in coming months.\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When\nnecessary, users can employ existing or customized OCR models via the uniﬁed\nAPI provided in the OCR module. 4) LayoutParser comes with a set of utility\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\nis also highly customizable, via its integration with functions for layout data\nannotation and model training. We now provide detailed descriptions for each\ncomponent.\n3.1\nLayout Detection Models\nIn LayoutParser, a layout model takes a document image as an input and\ngenerates a list of rectangular boxes for the target content regions. Diﬀerent\nfrom traditional methods, it relies on deep convolutional neural networks rather\nthan manually curated rules to identify content regions. It is formulated as an\nobject detection problem and state-of-the-art models like Faster R-CNN [28] and\nMask R-CNN [12] are used. This yields prediction results of high accuracy and\nmakes it possible to build a concise, generalized interface for layout detection.\nLayoutParser, built upon Detectron2 [35], provides a minimal API that can\nperform layout detection with only four lines of code in Python:\n1 import\nlayoutparser as lp\n2 image = cv2.imread(\"image_file\") # load\nimages\n3 model = lp. Detectron2LayoutModel (\n4\n\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\n5 layout = model.detect(image)\nLayoutParser provides a wealth of pre-trained model weights using various\ndatasets covering diﬀerent languages, time periods, and document types. Due to\ndomain shift [7], the prediction performance can notably drop when models are ap-\nplied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\ndocument structures and layouts vary greatly in diﬀerent domains, it is important\nto select models trained on a dataset similar to the test samples. A semantic syntax\nis used for initializing the model weights in LayoutParser, using both the dataset\nname and model name lp://<dataset-name>/<model-architecture-name>.\n\n"
     ]
    }
   ],
   "source": [
    "print(pages[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885a839a-1ed6-4fb6-830c-a894b4e2ca3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3Zvyk9ACL8fx"
   },
   "source": [
    "#### UnstructuredPDFLoader\n",
    "\n",
    "[Unstructured.io](https://unstructured-io.github.io/unstructured/) supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF. LangChain's [`UnstructuredPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html) integrates with Unstructured to parse PDF documents into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09222f1d-3e0d-44f8-b2b5-9ee2268dfd53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uv4PBKyfR2od"
   },
   "source": [
    "Load PDF as a single document - no complex parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4fc1647-49b0-4cbe-82f2-fa29d4a548ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Z8qz1HKWMRTf"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader('./docs/layoutparser_paper.pdf')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d937e194-99c1-43d2-bcfb-5391c6b540d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDWTGeMFMbI7",
    "outputId": "68ae130a-2190-4898-dfdf-0b2d988ddda0"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f40d167-385d-4359-9853-a7310c4d016b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUmEA9qfMdIc",
    "outputId": "fe2af13e-0618-4e42-a667-fb7384ada142"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0 2\n\nn u J\n\n1 2\n\n]\n\nV C . s c [\n\n2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n\nLayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n\nZejiang Shen1 ((cid:0)), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain Lee4, Jacob Carlson3, and Weining Li5\n\n1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify d\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eee5f09-deeb-4552-983a-929cb7ab06ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Load PDF with complex parsing, table detection and chunking by sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ca05ab-bb00-4b56-ae8c-ee4698163b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Refer to https://community.databricks.com/t5/data-engineering/trying-to-use-pdf2image-on-databricks/td-p/12914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f830190-9939-4630-86ed-deef88145773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#install poppler on the cluster (should be done by init scripts)\n",
    "def install_ocr_on_nodes():\n",
    "    \"\"\"\n",
    "    install poppler on the cluster (should be done by init scripts)\n",
    "    \"\"\"\n",
    "    # from pyspark.sql import SparkSession\n",
    "    import subprocess\n",
    "    num_workers = max(1,int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkers\")))\n",
    "    command = \"sudo rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/* && sudo apt-get clean && sudo apt-get update && sudo apt-get install poppler-utils tesseract-ocr -y\" \n",
    "    def run_subprocess(command):\n",
    "        try:\n",
    "            output = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True)\n",
    "            return output.decode()\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise Exception(\"An error occurred installing OCR libs:\"+ e.output.decode())\n",
    "    #install on the driver\n",
    "    run_subprocess(command)\n",
    "    def run_command(iterator):\n",
    "        for x in iterator:\n",
    "            yield run_subprocess(command)\n",
    "    # spark = SparkSession.builder.getOrCreate()\n",
    "    data = spark.sparkContext.parallelize(range(num_workers), num_workers) \n",
    "    # Use mapPartitions to run command in each partition (worker)\n",
    "    output = data.mapPartitions(run_command)\n",
    "    try:\n",
    "        output.collect();\n",
    "        print(\"OCR libraries installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't install on all node: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2695f6f5-6aee-4aea-b2d1-44b295efe904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR libraries installed\n"
     ]
    }
   ],
   "source": [
    "install_ocr_on_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98694c98-3f9c-4b17-9654-8d5dff45916d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "e606cf098a0c47388ab4a2f387b3beac",
      "bbdf731fc0294dd6a3c392012bc93896",
      "63fc017f50be4f1f8c807f30a7ce11eb",
      "4a47716561f74d8fa006b47f64a6837a",
      "c69e38f11339431cac851d749a68ed9a",
      "b8aab8494a2045bdbc09415b1418bd81",
      "b4c4b57aa1f94e958c0d09c7558b7817",
      "79141af480654819ae6c0981732355a2",
      "8b217d584ee945a9bfedf1a6f33bfb67",
      "e6b2ee0c072e457db0d8989b8ed1aaac",
      "b8bffaaa7c524cacb9cb2a5f67581e7f",
      "dee9511a788d41bab9d5fdd4b6e4f6a9",
      "ff2dfe08ffed46d2a633ac0b7b0eeec5",
      "0f6d1f3099c2428bbce1f41dd04cdc4e",
      "cc62a8b1ab8045caa46adac2bad7aef2",
      "1bb6db9f42c047b8a65dcf4b93d84bbf",
      "7194f73d47a0456a8d462102d8b24288",
      "822a272cf0724edb8e7fdccd5317e86e",
      "43b3d3c90225407e943fc9dcb39ad53e",
      "fb61550b3a774725a24e4e2a9ace15d5",
      "1c7f0cf2749146668bd8df52ffe11fad",
      "63a6b02ceb9340aba44b6891be465f0c",
      "677e724e773541898d7682c8b8d8ff61",
      "67f5f48c907f47f7a64bfdef3fb25c8b",
      "3865ca39e1bc4a50ab761d215de760d7",
      "432ad00a5bd84f1e9ed0cf8c881bbd74",
      "1a85a1b9c4494ccfb66d5c351fdc42b3",
      "43b9cf97a2044db3a65ba7a013da8d80",
      "c5aa729abde54a6692fec1aa39c4f49c",
      "73c52584484948f1a8bd01beb4969315",
      "46b6d128201f4f479c45b231ece02270",
      "34994b355d1841ec9019ec2a7c5dd408",
      "68f35650c8ea40caa773596bf12c1845"
     ]
    },
    "id": "jbH6ZJLcNs0s",
    "outputId": "4a883f84-3780-4638-97d4-bb080b5b2e78"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915def1d8ec04866a5499553cc3a959a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a999428328146d18fd315241aeea4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da018608f524455786676f3e8bbbccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# takes 3-4 mins on Colab\n",
    "loader = UnstructuredPDFLoader('./docs/layoutparser_paper.pdf',\n",
    "                               strategy='hi_res',\n",
    "                               extract_images_in_pdf=False,\n",
    "                               infer_table_structure=True,\n",
    "                               chunking_strategy=\"by_title\",\n",
    "                               max_characters=4000, # max size of chunks\n",
    "                               new_after_n_chars=3800, # preferred size of chunks\n",
    "                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n",
    "                               mode='elements')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cb50b54-fcf1-4dc7-96fa-63cfc8c0da06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WvtzXCgMots",
    "outputId": "9c12de1c-3322-4d7e-ee27-0e82ac01fc69"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b602d212-b778-4288-acb6-cb49acd64229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JM5fCpJNPPiv",
    "outputId": "9d4dfe3e-79ed-4682-e219-99b92ae0fda9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'Table',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'Table',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement',\n",
       " 'CompositeElement']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.metadata['category'] for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995e3978-9a9a-49a6-9c8d-a52d4ab203ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f7zcYUxMq0t",
    "outputId": "de586815-702d-4a29-b348-0702e0fb568b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/layoutparser_paper.pdf', 'filetype': 'application/pdf', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'page_number': 1, 'orig_elements': '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', 'file_directory': './docs', 'filename': 'layoutparser_paper.pdf', 'category': 'CompositeElement', 'element_id': 'c9df346177c79745c1df099f06e1165e'}, page_content='1 2 0 2 n u J 1 2 ] V C . s c [\\n\\n2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\\n\\nLayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\\n\\nZejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li®\\n\\n1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\\n\\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\\n\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1eedfb0-e5b3-46cd-be0f-fed5431145f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdjS77Fh0oKJ",
    "outputId": "d8e88fae-b94c-41d5-9a4c-f22a3fdf0ed5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 0 2 n u J 1 2 ] V C . s c [\n\n2 v 8 4 3 5 1 . 3 0 1 2 : v i X r a\n\nLayoutParser: A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis\n\nZejiang Shen! (4), Ruochen Zhang”, Melissa Dell?, Benjamin Charles Germain Lee*, Jacob Carlson’, and Weining Li®\n\n1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.ca\n\nAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conﬁgurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eﬀorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.\n\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis · Character Recognition · Open Source library · Toolkit.\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21346e6f-d2b1-45f2-adda-68e53e34c073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "six4KwFGPX-S",
    "outputId": "1e8a3a98-7e9f-4dd7-855c-9506f30627f8"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/layoutparser_paper.pdf', 'last_modified': '2025-03-06T02:27:51', 'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\", 'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'}, {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'}, {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'}, {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'}, {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'}, {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'}, {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"}, {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'}, {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'}, {'x': 2, 'y': 1, 'w': 1, 'h': 1, 'content': 'Layouts of modern scientific documents'}, {'x': 2, 'y': 2, 'w': 1, 'h': 1, 'content': 'Layouts of scanned modern magazines and scientific report'}, {'x': 2, 'y': 3, 'w': 1, 'h': 1, 'content': 'Layouts of scanned US newspapers from the 20th century'}, {'x': 2, 'y': 4, 'w': 1, 'h': 1, 'content': 'Table region on modern scientific and business document'}, {'x': 2, 'y': 5, 'w': 1, 'h': 1, 'content': 'Layouts of history Japanese documents'}], 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'orig_elements': 'eJydVlFvmzAQ/isWL3tpEgNJSKqq0qop2qqmqtbuKUPIwCVBA4OwUZu1/e87OwmlAQqdSGR8+Lvzff7uYPVsQAwJcOlFoXFOjNCh4XQKMHBsag7GvmkP/ACsgTOfAWXzycxfj40zYiQgWcgkQ8yzEaRpHkacSRB6HrNdWkhvC9FmK9FiWZQi5mB+jEK5RavpaGuWRlwq3GplO/Ph5IzYM3tI3TNynE8cOpyquWlb5nDcYNgj0GKInZCQqEzuoieI7zMWgPGKD0KQEMgo5V4QMyG8LE99XEaHc2rNTFywjmKQuww09m5p6A3zTcE2OquVAXxjuNoqpJekYbSOQHNmUWsyoPaATh+odW455xNToTNEerxIfMhxFeZhSObH4DHhBRDH2qnijkukX/n5hnwKkAqr+cHx8TA+qa3iuFMjpvMOd1f4N2x3C5JczdyXLrhZg//8kXztQlmnqFt4FBnLMLcOpH2KfFAsXDH+pws5PkV+v+5J0eQU+UKuEEeWaQjxl1aKzDaGF6NlF6ZGayeiRumiC1GjshNRo7BHJg3k3aaqtFtwVhtrN7reBUnXBMsFck5EEOEzLJyAhGlQqL7T6bZGbMWtCBjnEB7dJ2zD/kYcBGE8rAbLIUvzVtVYbQfSEOnXPeFH5QuyztOEyC0Qi8otCRBW5LuuMM0FgXvcYHci+KuTpdLxC6FSEyVzXXFqZ19JZxsJmeY7cs0yhk6h/3FMXlUPlPAkVSPbyiRWri90a7u8QC5YqIbLQ6VejPBezd9XYMWstbWfj47wHP/h5fu+hk9DbUYJl/f9JLZfPtJuD65Vzyu9NPr7nLbqIcoG+bbxj8L0EFY9RtlKG2L8p6rqQcqu23UC7aKqOB3tlaLfyUpFlRdfRSEmuWH55jDZS4S8yWFlz1yijxBvXVIyjR8EjktKUnCK68rt41rTJQsyIku8Fvra3w/wWuC/VU2/i7VPzYqiyCeFcnSwF0sjvIcAyMdnegzSeK6kd/UfP4J0MOPV/Qe09RE1', 'file_directory': './docs', 'filename': 'layoutparser_paper.pdf', 'category': 'Table', 'element_id': '720a11c7a3fa16628248e6b9613d2c2d'}, page_content='Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f96b15-0ac9-446d-a12f-e54ff750fb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "AYe9e5K3Pfje",
    "outputId": "9a804f43-5ec7-4166-c7e8-b727acf81959"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88817c7c-46bf-428d-98b9-663c82dea5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "nFI5BBDjPoHK",
    "outputId": "44047255-ab79-48bb-ba22-d9ad390e8598"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(data[5].metadata['text_as_html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a496aca8-8014-4e07-a17a-25a3ba7cfebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JbzDKmPzSA1H"
   },
   "source": [
    "Load using raw unstructured.io APIs for PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee90df63-ef28-4e84-b8e5-7694d4809d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_-wpOxHUMwOw"
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Get elements - takes 3-4 mins\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=\"./docs/layoutparser_paper.pdf\",\n",
    "    strategy='hi_res',\n",
    "    # Unstructured first finds embedded image blocks\n",
    "    extract_images_in_pdf=False,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=\"./\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662e9f8a-8f0d-49c2-be69-5e7cbd5e1f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7QC4wt-St_K",
    "outputId": "b19e5870-d93b-4463-faa7-ccb8eda69504"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_pdf_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8705824-d757-4630-9f87-c0ca57d44ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bC6seR7TSZY4",
    "outputId": "9a455dc2-a0e8-42cb-8af2-d85ed339facb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x7f2200641de0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f2200640b80>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f22006a7040>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f9343d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f9362c0>,\n",
       " <unstructured.documents.elements.Table at 0x7f235f936ef0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f937a90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f935960>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f936cb0>,\n",
       " <unstructured.documents.elements.Table at 0x7f235f9347c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f936ec0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f935c60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f937c40>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f9373a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f9340a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f936e90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f935720>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x7f235f408d60>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "086198c3-22d7-42e8-a6ce-0a36518347f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iPAwY5YS0CC",
    "outputId": "3b132c46-51a6-46f2-c413-35d71cbb9875"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'type': 'Table',\n",
       " 'element_id': '720a11c7a3fa16628248e6b9613d2c2d',\n",
       " 'text': 'Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents',\n",
       " 'metadata': {'last_modified': '2025-03-06T02:27:51',\n",
       "  'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\",\n",
       "  'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'},\n",
       "   {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'},\n",
       "   {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'},\n",
       "   {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'},\n",
       "   {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'},\n",
       "   {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'},\n",
       "   {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"},\n",
       "   {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'},\n",
       "   {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'},\n",
       "   {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'},\n",
       "   {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'},\n",
       "   {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'},\n",
       "   {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'},\n",
       "   {'x': 2,\n",
       "    'y': 1,\n",
       "    'w': 1,\n",
       "    'h': 1,\n",
       "    'content': 'Layouts of modern scientific documents'},\n",
       "   {'x': 2,\n",
       "    'y': 2,\n",
       "    'w': 1,\n",
       "    'h': 1,\n",
       "    'content': 'Layouts of scanned modern magazines and scientific report'},\n",
       "   {'x': 2,\n",
       "    'y': 3,\n",
       "    'w': 1,\n",
       "    'h': 1,\n",
       "    'content': 'Layouts of scanned US newspapers from the 20th century'},\n",
       "   {'x': 2,\n",
       "    'y': 4,\n",
       "    'w': 1,\n",
       "    'h': 1,\n",
       "    'content': 'Table region on modern scientific and business document'},\n",
       "   {'x': 2,\n",
       "    'y': 5,\n",
       "    'w': 1,\n",
       "    'h': 1,\n",
       "    'content': 'Layouts of history Japanese documents'}],\n",
       "  'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 5,\n",
       "  'orig_elements': 'eJydVm1vmzAQ/isnvuxLkxjIW6uq0qop2qqmqtbuU4aQwU6CBgZhozZr8993dhJKA5R0IpHx4efO9/i5g8WLxWOecKH8iFkXYDFGho49DXv2lJz3hsux2wvscdgLlywcTe0JHZLAOgMr4YoyqihiXqwwTXMWCaq4NPOYbtJC+WserdYKLY5DCGL25qeIqTVa7YmxZmkklMYtFu7kvD86A3fq9ol3Bof5aEL6Yz23XcfuDxsMOwRaLLmRiic6k/vomccPGQ25tcUHjCseqigVfhhTKf0sTwNcRvrnxJnauGAZxVxtMm6w93PLbFisCroyWS0sLlaWZ6xS+UnKomXEDWcOcUY94vbI+JE4F87kYmRrdIZIXxRJwHNchXlYigYx96n0Qx7HxqnmTiikX/v5hnxKrjTW8IPj03581lvFcaNHTOcd7r4Ibunmjiu4nnqvXXC7Bv/5I/nahXKOUXf8SWY0w9w6kO4x8lGzcE3Fny7k8Bj5/eZEikbHyFe4RhzMU8bjL60U2W0MzwbzLkyN1k5EjdJZF6JGZSeiRuEJmTSQd5fq0m7BOW2s3Zp6l5AuAcuF5wJkGOEzLJwQWBoWuu90uq0RW3ErQyoEZwf3CV3Rv5HgEqhg1WA5z9K8VTVO24E0RPr1AOKgfAnLPE1ArTk4RK0hRFiRb7rCNBcE7nGF3QnwVydLpxMUUqcmS+a64tTOvpLOOpIqzTdwQzOKTvnpxzHa6h6o+LPSjWytkli7vjSt7eoSuaBMD1f7Sr0c4L2ev6/AitloazcfHOA5/tnV+76GT5kxo4TL+9Mktls+MG73rnXPK700+vuctuohygb5tvGPwpwgrHqMspU2xPhPVdWDlF236wTaRVVxOtgpxbyTtYoqL76KQmy4pflqP9lJBN7ksHCnHpgjxFsPSqbxg2DiQUkKTnFduX1ca3swgwHM8ZqZa3ffw2uG/1Y1/S6WAbErioJPCuXgYCeWRvgJAoCPz/QQpPFc4eTqP3wEmWDW1vsHG9kRJg==',\n",
       "  'file_directory': './docs',\n",
       "  'filename': 'layoutparser_paper.pdf'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements[5].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdec2de-940b-4baa-8834-bb0e3147b443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QLKjlNBQTHwE"
   },
   "source": [
    "Convert into LangChain `document`format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be2813b-4cc5-408c-a837-f55eb1e432c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iCFjyenoTMC7",
    "outputId": "cf91521e-1ec2-4020-9c09-ac7514ae8c28"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'last_modified': '2025-03-06T02:27:51', 'text_as_html': \"<table><thead><th>Dataset</th><th>| Base Model'|</th><th>| Notes</th></thead><tr><td>PubLayNet B8]|</td><td>F/M</td><td>Layouts of modern scientific documents</td></tr><tr><td>PRImA</td><td>M</td><td>Layouts of scanned modern magazines and scientific report</td></tr><tr><td>Newspaper</td><td>F</td><td>Layouts of scanned US newspapers from the 20th century</td></tr><tr><td>TableBank</td><td>F</td><td>Table region on modern scientific and business document</td></tr><tr><td>HJDataset</td><td>F/M</td><td>Layouts of history Japanese documents</td></tr></table>\", 'table_as_cells': [{'x': 0, 'y': 0, 'w': 1, 'h': 1, 'content': 'Dataset'}, {'x': 0, 'y': 1, 'w': 1, 'h': 1, 'content': 'PubLayNet B8]|'}, {'x': 0, 'y': 2, 'w': 1, 'h': 1, 'content': 'PRImA'}, {'x': 0, 'y': 3, 'w': 1, 'h': 1, 'content': 'Newspaper'}, {'x': 0, 'y': 4, 'w': 1, 'h': 1, 'content': 'TableBank'}, {'x': 0, 'y': 5, 'w': 1, 'h': 1, 'content': 'HJDataset'}, {'x': 1, 'y': 0, 'w': 1, 'h': 1, 'content': \"| Base Model'|\"}, {'x': 1, 'y': 1, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 1, 'y': 2, 'w': 1, 'h': 1, 'content': 'M'}, {'x': 1, 'y': 3, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 4, 'w': 1, 'h': 1, 'content': 'F'}, {'x': 1, 'y': 5, 'w': 1, 'h': 1, 'content': 'F/M'}, {'x': 2, 'y': 0, 'w': 1, 'h': 1, 'content': '| Notes'}, {'x': 2, 'y': 1, 'w': 1, 'h': 1, 'content': 'Layouts of modern scientific documents'}, {'x': 2, 'y': 2, 'w': 1, 'h': 1, 'content': 'Layouts of scanned modern magazines and scientific report'}, {'x': 2, 'y': 3, 'w': 1, 'h': 1, 'content': 'Layouts of scanned US newspapers from the 20th century'}, {'x': 2, 'y': 4, 'w': 1, 'h': 1, 'content': 'Table region on modern scientific and business document'}, {'x': 2, 'y': 5, 'w': 1, 'h': 1, 'content': 'Layouts of history Japanese documents'}], 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'orig_elements': 'eJydVlFvmzAQ/isnXvbSJAZCQ6uq0qop2qqmqtbuKUPIwU6CBgZhozZr+993dhJKA5R0IpHx4e/O9/m7g/mzxROecqHCmFnnYI29JWfEdwfukvqDMfPsAT11nUFEx5SwMzfyfGKdgJVyRRlVFDHPVpRlBYsFVVyaeUI3WanCNY9Xa4UWxyEEMTvzY8zUGq32xFjzLBZK4+Zzd3I29E7A9d0hCU5gP/cmZHiq57br2MNxi2GLQIslN1LxVGdyFz/x5D6nEbde8QHjikcqzkQYJVTKMC+yBS4jwzPi+DYuWMYJV5ucG+zdzDIbFquSrkxWc4uLlRUYq1RhmrF4GXPDmUMcb0DcATl9IM65Mzn3bI3OERmKMl3wAldhHpaii4SHVIYRTxLjVHMnFNKv/XxDPiVXGmv4wfFxNz7preK40SOm8w53Vy5u6OaWK7jyg5c+uN2A//yRfu1DOYeoW/4oc5pjbj1I9xD5oFm4ouJPH3J8iPx+fSRF3iHyBa4QB7OM8eRLJ0V2F8PT0awP06C1F9GgdNqHaFDZi2hQeEQmLeTdZrq0O3BOF2s3pt4lZEvAcuGFABnF+AwLJwKWRaXuO71uG8TW3MqICsHZ3n1KV/RvLLgEKlg9WMHzrOhUjdN1IC2Rft2D2CtfwrLIUlBrDg5Ra4gQVhabvjDtBYF7XGF3Avw1ydLpLEqpU5MVc31xGmdfS2cdS5UVG7imOUWn/Pjj8F51D1T8SelGtlZpol1fmNZ2eYFcUKaHy12lXozwXs/fV2DNbLS1nY/28AL/7PJ9X8OnzJhRwtX9cRLbLh8ZtzvXuudVXlr9fU5bzRBVg3zb+EdhjhBWM0bVSlti/KeqmkGqrtt3At2iqjkdbZVi3slaRbUXX00hNtzQYrWbbCUCb3KYu34A5gjxNoCKafwgmARQkYJTXFdtH9faAUxhBDO8puba3g/wmuK/U02/y+WC2DVFwSeFsnewFUsr/AgBwMdnug/Seq5wdPXvP4JMMOs1+AcsRBEn', 'file_directory': './docs', 'filename': 'layoutparser_paper.pdf'}, page_content='Dataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiﬁc documents Layouts of scanned modern magazines and scientiﬁc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiﬁc and business document Layouts of history Japanese documents')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "lc_docs = [Document(page_content=doc.text,\n",
    "                    metadata=doc.metadata.to_dict())\n",
    "              for doc in raw_pdf_elements]\n",
    "lc_docs[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4168b7d3-420a-4f64-bd4d-8df566de98fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QQV0X4grW971"
   },
   "source": [
    "### Microsoft Office Document Loaders\n",
    "\n",
    "The Microsoft Office suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n",
    "\n",
    "[Unstructured.io](https://docs.unstructured.io/open-source/introduction/overview) provides a variety of document loaders to load MS Office documents. Check them out [here](https://docs.unstructured.io/open-source/core-functionality/partitioning).\n",
    "\n",
    "Here we will leverage LangChain's [`UnstructuredWordDocumentLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.word_document.UnstructuredWordDocumentLoader.html) to load data from a MS Word document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f486635e-1d8d-4a82-a64b-8e8677cc165e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V37elBlCT6Mn",
    "outputId": "479921a3-1c8a-4534-f578-13441241b863"
   },
   "outputs": [],
   "source": [
    "# !gdown 1DEz13a7k4yX9yFrWaz3QJqHdfecFYRV-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd7fa264-d58b-4ce9-a0be-7af5b332cd8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uv2b1SZcX4zS"
   },
   "source": [
    "Load word doc as a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4ac950-20da-48fe-9df2-cef1170315ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "BqYBSoqHT6TY"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "loader = UnstructuredWordDocumentLoader('./docs/Intel Strategy.docx')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c49c6d-95a1-4a72-9e5d-be756ee7fe35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zfi8_V9MT6Vl",
    "outputId": "89376739-a47f-4591-b5a3-e2c4f85089bb"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c878e42-8e23-4571-b08d-68e751c007fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "kznFjlj6T6Xw",
    "outputId": "ac3bdf6e-c72a-4534-8023-d7c6f3fdef24"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every indust'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a89c46-a773-4715-909c-d4cb57df4779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ILz-pXIyX8e0"
   },
   "source": [
    "Load word doc with complex parsing and section based chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a024e9b5-8642-4e65-8c0a-6d19a6a23003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7cKYWNjAUo1A"
   },
   "outputs": [],
   "source": [
    "loader = UnstructuredWordDocumentLoader('./docs/Intel Strategy.docx',\n",
    "                                        strategy='fast',\n",
    "                                        chunking_strategy=\"by_title\",\n",
    "                                        max_characters=3000, # max limit of a document chunk\n",
    "                                        new_after_n_chars=2500, # preferred document chunk size\n",
    "                                        mode='elements')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0154f7-b17e-476d-a5dd-39641778ee01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}, page_content='Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\\n\\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\\n\\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.'),\n",
       " Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['AI and Cloud-to-Edge Infrastructure', 'IPU roadmap', 'Single GPU solution for media transcode, visual graphics and inference in the cloud:'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'ac0ef24a663cbac16d56ffc905957d57'}, page_content='AI and Cloud-to-Edge Infrastructure: This allows for high performance computing to be immediately available, which is the backbone of ubiquitous compute. “With the unlimited reach of the intelligent edge, we can have low latency, high bandwidth, and real-time inference capabilities anywhere we want them,” he explained.  Intelligent Edge could make visual experience of streaming services, cloud gaming, and visual workloads possible, however, there are hurdles to be overcome for that. Intel stands to overcome the challenges of deploying a complete cloud to edge infrastructure in today’s time with the launch of Habana Gaudi2 AI processor for training data centre workloads, and 12th Gen Intel Core HX processors for hybrid work.Habana Gaudi2 and Greco AI Accelerators are built on a single software stack, Synapse AI, that supports different architectures, enabling end-users to take advantage of the processors’ performance and efficiency. In addition, Gaudi2 delivers two times better AI training performance compared with current in-market A100-based offerings for key vision and NLP workloads, the company announced. The company also announced the shipment of the 4th Gen Intel Xeon Scalable processors, which will support DDR5, PCIe Gen5 and CXL 1.1, and are equipped with new integrated accelerators that deliver up to 30x performance versus the prior generation through software and hardware optimizations for AI workloads, along with new capabilities that deliver upto two times capacity gains for virtual radio access network (vRAN) deployments, for telco networks. Also, in partnership with Accenture, Intel has kickstarted Project Apollo, a program that will provide enterprises with more than 30 opensource AI solutions kits that are designed to make AI more accessible to customers in on-prem, cloud and edge environments. The company also unveiled its IPU roadmap, featuring new FPGA + Intel architecture platforms (code-named Hot Springs Canyon) and the Mount Morgan (MMG) ASIC, as well as next generation 800GB products. IPUs are dedicated products with hardened acceleration for infrastructure compute needs, allowing businesses to accomplish tasks quicker and solve problems faster.\\n\\nSingle GPU solution for media transcode, visual graphics and inference in the cloud:\\xa0Intel’s data center GPU, code-named Arctic Sound-M (ATS-M), is the industry’s first discrete GPU with an AV1 hardware encoder. ATS-M is a versatile GPU with leadership transcode quality and performance targeting 150 trillion operations per second (TOPS). Developers will be able to easily design for ATS-M with an open software stack through oneAPI. ATS-M will be available in two form factors and in more than 15 system designs from partners including Dell Technologies, Supermicro, Cisco, HPE,\\xa0Inspur\\xa0and H3C. It will launch in 2022’s third quarter.'),\n",
       " Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['New 12th Gen Intel Core HX processors for hybrid work:', 'High performance computing to solve the world’s most complex challenges:', 'Confidence with confidential computing:', 'Agriculture autonomy with private wireless networks'], 'emphasized_text_tags': ['b', 'b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': 'eJzVV9tu3DYQ/RVin1pg5Wq1V/nNTlPbQBIEsVsHSAKDIkcSYYpUSWovCfLvnaG0a2+aAkbgh+bJK3I413Nmxh++jEBDAybcKTk6ZaNsleaZKKcJZHyWzGReJMUEFomczni6XJX4ezYas1EDgUseOL75MhI8QGXd7k5CG2o8SlECmrbmXn0GeRdgG+6ENQHteLz+MHoDGzbJQs0uwLArvNDshXXALt+z1lkB3lvnWWkdq3eFU5JtrLs/HX36jt7Aq15nEa9LpeFOKgcioEcU0slv0go/Gu4Mb4BOe6PXwZHvuxMU2ZKI5qbqeAW9SjBVVKq5D3eNlapU0KcpzeZJOk3SxU2anWbL0/lk9BUFySO6/7H4PnZpytObGpiwTcvNLv7VEECygKcHjSVvlN6xjcJvutC8M6Jmtoxf5inGT9gLB5w0kxd4XuK5soZrzza1RSV41fCtarqGteBQquFGAOMGn2jYqkJpFXYsoCxfqwp1Mb4PB8xaOWsIWOP4IrratSQ9WWBYDnw8F9qKe+ZbtOaH+zm7uPw8jpE8cl7lySTL0/RxEEz5KIbJ0/Jjl6WT3LMCfGCNRe/ixb0PPGBcrNU8UBAnVOawayMKXikfrgI0VLxvqbCa56sVn0IyzbDSs+VSJCuZzxJRwHSeTdIsL/JnoMKlquqjBFPNu6BMRcnwVq/h30E2FoPswbFlouYagY2g/YkY8qxh98Q5c5U1BtibWHCu2SteWPTeOoVoQ6wgCjAaxBtakKDVGhzLGGx5qW3riT4t8Psjrw4UO+sc6mK+w9veVXzsOmPIY1J8gOt7ILTtMTpGfyUklFTJrnnb1ph59o63CgEftcdMnCMZNkri52tosCzsl8vz17/23HlQTTBjAqGDti/e/nmk+y2Biv0FQtTKjnvV/Str4OztFbm0VpLclbAGjBiwBXngjUZHmd955AFT+KJyMYFPJko5mZTAl6tksZwUyWxa8GSVTycJrAq+WvA0z7PpMxDlhTWlknCoihi+g8JSH9DzE1HgiQH14Gbn1mOLJzz0XgmrdY9vLD4ijjPsqcAdCkGJ+A09ymOl8fJI+dkVMayLfTHUPDBkkt34wQa+w4iVYSp4nAOdwxcGQmymR3pwBKke+W1XaCWom3fyhN1YVoNGqwhFiDxG8qEVL7iG8WAFu3obvWfv9n5jyVjRKR2+4+++fbPWbsBhyMVuSMS1LcOGI6kuOu4ke7lFyNAkQ5NrrjBHesjvFG8fRmKk6TV6FAX26v2TUZ8tV2I5S0UyLXJIZmnJkzyfLZMVl2UqFsUik5NnQP1Z5ZTodOgwQN4Fa2wzDP7WqTWlb4M4jhTe1+jnYcAPBHfal0+rClPEQFZwgLJHAA1To7VhgE4PZuMjdkprEZ7nugN2WyuCni1sUMIfWqJE6NH+RAiggTB4ZTtE08FZVHtMn4MFlGKVI4TuRxYuSj5ONfi7Uy3hh3qsRTlcoiB8Y4PGU6DVUNAkE3HxQ1kT+7LDHCQF93ExM5j9qO2w17BbSlwkXulsM8CcGsYfIKHvE7dDOsffTULD5Z6skauRGceRasyTo8Lvyddw7DQvqQwPrSlS6/fHmy7dISK13fmhPpgiij0W8GH0k5yvOfEbd0IRHC6fmAARV1X06r9gERsg7sslxyrjj02NPeLJVAbBV9lykSbZYsKT2ayEpIBslczlFGaZmIvZ5IlU/n/QqteuaF9ax6xSQ42AJLdoP2+5gHE8oZaoPUphPTxXSI9e8LbGTrmzHeHB3DOcNB1NFHo9xsGwhzenBU0O13GN72ttwFW7+LPG0iHnurCfFfvxZchdXFSg10UtQHsEe9vqXfSarWMa+/kX/7fBQAbN9E+Lh9CDhvUZJ5BGbpGVsqOuctLH+BgIb7ij/WYNN5Str5/+AalNIr4=', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b55d958ee4e244f7d5c5471ffbfac525'}, page_content='New 12th Gen Intel Core HX processors for hybrid work:\\xa0The company completed the 12th Gen family with the launch of the new 12th Gen Intel Core HX processors. Created for professionals who need maximum performance and flexibility to navigate a hybrid environment, and with up to 16 cores and clock speeds up to 5 GHz, the Intel Core i9-12900HX processor is the world’s best mobile workstation platform.\\n\\nHigh performance computing to solve the world’s most complex challenges:\\xa0Argonne National Laboratories is on track to deliver 2 exaflops of peak performance with the Aurora supercomputer running on the Intel Xeon processor, code-named Sapphire Rapids with High Bandwidth Memory (HBM), and the Intel data center GPU, code-named Ponte Vecchio, with Intel oneAPI providing developers seamless system integration.\\n\\nConfidence with confidential computing:\\xa0 Bosch and Intel collaborated on a research effort to develop a confidential AI solution that allows Bosch to train its neural networks confidentially in the public cloud. To help achieve this at scale, Bosch Corporate Research has built a confidential AI platform powered by Intel Software Guard Extensions available with 3rd Gen Intel Xeon Scalable platforms.\\n\\nAgriculture autonomy with private wireless networks: Intelligent edge solutions have the potential to transform food. Blue White Robotics developed a new type of autonomous agricultural solution that transforms a grower’s existing equipment into a fleet of autonomous tractors connected to an internet-based management platform. With help from Intel and Federated Wireless, Blue White Robotics made this a scalable solution that leverages Intel Smart Edge and Intel Xeon D processors and employs the power of edge computing and shared spectrum to create a private wireless network on any farm anywhere.\\n\\nIntel is moving at a “torrid pace,” Gelsinger said. “When you think about torrid, it’s a word about speed and energy and heat. But in the Intel context, we’re also applying a vector\\xa0of that energy for setting a direction into the future.”'),\n",
       " Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['Intel IDM 2.0', 'Intel’s global, internal factory network for at-scale manufacturing', 'Expanded use of third-party foundry capacity.', 'Intel Foundry Services.', 'Expanding in the U.S. and Europe'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '9b9fec55d420d43d84e13646715885e6'}, page_content='Intel IDM 2.0\\n\\nThe Intel IDM 2.0 strategy revealed by new company CEO Pat Gelsinger\\xa0is an ambitious plan to restore the company’s leadership in semiconductor production. Gelsinger described IDM 2.0 as the second generation of Intel’s integrated device manufacturing model.\\n\\nIntel’s global, internal factory network for at-scale manufacturing\\xa0is a key competitive advantage that enables product optimization, improved economics and supply resilience. Gelsinger re-affirmed the company’s expectation to continue manufacturing most of its products internally. The company’s 7 Nanometer Processors development is driven by increased use of extreme ultraviolet lithography (EUV) in a rearchitected, simplified process flow.\\n\\nExpanded use of third-party foundry capacity.\\xa0 Gelsinger said he expects Intel’s engagement with third-party foundries to grow and to include manufacturing for a range of modular tiles on advanced process technologies, including products at the core of Intel’s computing offerings for both client and data center segments beginning in 2023. This will provide the increased flexibility and scale needed to optimize Intel’s roadmaps for cost, performance, schedule and supply, giving the company a unique competitive advantage.\\n\\nIntel Foundry Services.\\xa0The launch of Intel Foundry Services means the company is not only going to manufacture its own chips, but it will also produce them for other semiconductor companies, including its competitors. \\xa0Intel announced plans to become a major provider of U.S. and Europe-based foundry capacity to serve the global demand for semiconductor manufacturing. Hence, Intel is establishing a new standalone business unit, Intel Foundry Services (IFS), led by semiconductor industry veteran Dr. Randhir Thakur, who will report directly to Gelsinger. IFS will be differentiated from other foundry offerings with a combination of leading-edge process technology and packaging, committed capacity in the U.S. and Europe, and a world-class IP portfolio for customers, including x86 cores as well as ARM and RISC-V ecosystem IPs. Gelsinger noted that Intel’s foundry plans have received strong statements of support from across the industry. Intel conservatively sizes the foundry opportunity as a $100 billion addressable market by 2025.\\n\\nExpanding in the U.S. and Europe. Intel is expanding its manufacturing capacity in the U.S. and Europe to provide less dependence on any specific region. Noting that 80% of leading-edge foundry capacity is concentrated in Asia, Gelsinger believes “the industry needs more geographically balanced manufacturing capacity.”'),\n",
       " Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['Intel Sustainability'], 'emphasized_text_tags': ['b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': 'eJzVVttu3DYQ/ZXBopcX09V9Jb+5TRAECIqicZ6SwBiSIy0RilRIyptN0H/vUN60SZEC7lv9JnA4F55zeKjXn3ZkaSaXbo3eXcFOjr3qu1GKsta1aNqqE8PQ9GKoBmoq3XZjPewuYDdTQo0JOefTTmGiyYfTraYlHXip4B00LweM5iPp20Qf0q3yLnGfyOHXu+f8beHlGhMah9JYk067t9/ISjjdZ8gtPBpLt9oEUon75YEvf9Jexd055nCmvHqun0Ke7HTJWz7kLRbdtOJE9yXJTVtRizHdzl6b0dAGQlVUrShqUXQ3RXVV7a/acvcHb8wTfVH96+lz/LRs3W9MspQz/onu2Na6qEcUeqylaIqqEZL2lWio3de6bItSy4eh+/8A4s1aFaW6ORCYeUGVwI+grJm5GagD9+BABHSwholRgMl6iRbSIRCmS/gt+MQHMG4CvwZYeCpKoGlGpyOXnEmbXIorG++4joYxUDxwAePe5TSUfk1w8EdeIjj6YDX4hfJp4yVcR/CO8lB/RX+MYAl1zo00GxalXjOE3DWa6b4Ht19H7rmGvE15PpozFC/gHkw+keGNsDrzfiVYfDTbeMlz5jueFhjAkQI5ReA8g+LsKafkM/qjOw/IKVxS8vhbtSOeeEpMWw3ukID4HlCAkYdTLDU/U+CEBUNy/LVNmiseD94S3KFdN8zNNkjKg8zEc7tpXO1nBJP3FxtnGp6RjRzkBhGNvoQv9fsrhjzgHd1knr+h44oqrYjYJSRq0chqFFJ1JIpCFkgdKlXuH5OObzywIi3bTpYWYzvLzOkaYfJoPxPPxsSiSZuYRm+tP2Z9GI4FM8PM0zJLjsnLlFVFXVy9cW/WosDiP4E7YN9V7VCKRvckGiy0kKPsRK9JdVr1sq71YwL3Wh0M3RGURfE9w+zoiJIVy4dWKRjF3glrzJc8+JiFHz/bxN/35PJLAF+YmJ4nmr+FXV83Yzt0g2g7ydhV3V4M+2IU/b7ErpZ9u68elcE+d3csKsBlCf7D5qtsJd/VRZHlZvOV5vvOkIbpxEblIoU7vPfKtCHJNpZfJ3av7Ap4ZqIBec5W67zaTY3wzrCgMWU3Xdle2DTPdSPesc4fzoGWra6kbkXZ60EwHyQGqktREtZj0w1jO9Jj4uDn1fCrwrLNaPIgGcztKfoK25nYGV69hGeByMGWk93hF7863gYvnj59AkziFHAG/m9wGoNmOzdO2XXbyadkDJledC4nkeZgZj9Duz052XdevbyAp2vgq7HNcB0NPpgYxKYe9yrbtGbXbstWYL/v+T9kqFDVdd9XzWMi5gUyTAd+OzfnEIaf8pjCCX7/4QnDxczcC5vJMZpRMOMJpswOv3nqkB9/tBGOJh2Alc+LZ9s5YpgzIYvP/6uGVzLSXEXz3bF+2aSAko+UsQV6v5olf/0LD2//BAM2usw=', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b8ae863a5df10800d4307a8d409a5034'}, page_content=\"Intel Sustainability\\n\\n“The impact of climate change is an urgent global threat. Protecting our planet demands immediate action and fresh thinking about how the world operates. As one of the world's leading semiconductor design and manufacturing companies, Intel is in a unique position to make a difference not only in our own operations, but in a way that makes it easier for customers, partners and our whole value chain to take meaningful action too,” Gelsinger said. \\n\\nTo realize this ambitious goal, Intel has set the following interim milestones for 2030:\\n\\xa0\\n\\nAchieve 100% renewable electricity use across its global operations.\\n\\nInvest approximately $300 million in energy conservation at its facilities to achieve 4 billion cumulative kilowatt hours of energy savings.\\n\\nBuild new factories and facilities to meet US Green Building Council LEED program standards, including recently announced investments in the US, Europe and Asia.\\n\\nLaunch a cross-industry R&D initiative to identify greener chemicals with lower global warming potential and to develop new abatement equipment.\")]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f38da6d-ce7e-48ce-80fb-e161457beabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CkOn6t0QVOyE",
    "outputId": "ff6531ab-db3f-455d-d3ab-fc667189655e"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372140ab-f95f-44a5-94cc-1353f715bd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IV4cp6yqVQZ1",
    "outputId": "126aae17-dc43-4625-d6c4-43a2b3424a79"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Intel Strategy\n\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\n\nThe Superpowers\n\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\n\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\n\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\n\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.' metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e421338-b563-4d80-b653-ed52f02c57b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Strategy\n\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\n\nThe Superpowers\n\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\n\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\n\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\n\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15dc8a9-6e35-4354-a86e-56dc5734da7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}\n"
     ]
    }
   ],
   "source": [
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e99035-f155-4c3f-8a56-42d18a22e211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}, page_content='Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\\n\\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\\n\\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7adf71e1-127d-4540-88a8-c75e9a764819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['AI and Cloud-to-Edge Infrastructure', 'IPU roadmap', 'Single GPU solution for media transcode, visual graphics and inference in the cloud:'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'ac0ef24a663cbac16d56ffc905957d57'}, page_content='AI and Cloud-to-Edge Infrastructure: This allows for high performance computing to be immediately available, which is the backbone of ubiquitous compute. “With the unlimited reach of the intelligent edge, we can have low latency, high bandwidth, and real-time inference capabilities anywhere we want them,” he explained.  Intelligent Edge could make visual experience of streaming services, cloud gaming, and visual workloads possible, however, there are hurdles to be overcome for that. Intel stands to overcome the challenges of deploying a complete cloud to edge infrastructure in today’s time with the launch of Habana Gaudi2 AI processor for training data centre workloads, and 12th Gen Intel Core HX processors for hybrid work.Habana Gaudi2 and Greco AI Accelerators are built on a single software stack, Synapse AI, that supports different architectures, enabling end-users to take advantage of the processors’ performance and efficiency. In addition, Gaudi2 delivers two times better AI training performance compared with current in-market A100-based offerings for key vision and NLP workloads, the company announced. The company also announced the shipment of the 4th Gen Intel Xeon Scalable processors, which will support DDR5, PCIe Gen5 and CXL 1.1, and are equipped with new integrated accelerators that deliver up to 30x performance versus the prior generation through software and hardware optimizations for AI workloads, along with new capabilities that deliver upto two times capacity gains for virtual radio access network (vRAN) deployments, for telco networks. Also, in partnership with Accenture, Intel has kickstarted Project Apollo, a program that will provide enterprises with more than 30 opensource AI solutions kits that are designed to make AI more accessible to customers in on-prem, cloud and edge environments. The company also unveiled its IPU roadmap, featuring new FPGA + Intel architecture platforms (code-named Hot Springs Canyon) and the Mount Morgan (MMG) ASIC, as well as next generation 800GB products. IPUs are dedicated products with hardened acceleration for infrastructure compute needs, allowing businesses to accomplish tasks quicker and solve problems faster.\\n\\nSingle GPU solution for media transcode, visual graphics and inference in the cloud:\\xa0Intel’s data center GPU, code-named Arctic Sound-M (ATS-M), is the industry’s first discrete GPU with an AV1 hardware encoder. ATS-M is a versatile GPU with leadership transcode quality and performance targeting 150 trillion operations per second (TOPS). Developers will be able to easily design for ATS-M with an open software stack through oneAPI. ATS-M will be available in two form factors and in more than 15 system designs from partners including Dell Technologies, Supermicro, Cisco, HPE,\\xa0Inspur\\xa0and H3C. It will launch in 2022’s third quarter.')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7b2e37-d850-4c97-bd0d-9bb4f77a005c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['New 12th Gen Intel Core HX processors for hybrid work:', 'High performance computing to solve the world’s most complex challenges:', 'Confidence with confidential computing:', 'Agriculture autonomy with private wireless networks'], 'emphasized_text_tags': ['b', 'b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': 'eJzVV9tu3DYQ/RVin1pg5Wq1V/nNTlPbQBIEsVsHSAKDIkcSYYpUSWovCfLvnaG0a2+aAkbgh+bJK3I413Nmxh++jEBDAybcKTk6ZaNsleaZKKcJZHyWzGReJMUEFomczni6XJX4ezYas1EDgUseOL75MhI8QGXd7k5CG2o8SlECmrbmXn0GeRdgG+6ENQHteLz+MHoDGzbJQs0uwLArvNDshXXALt+z1lkB3lvnWWkdq3eFU5JtrLs/HX36jt7Aq15nEa9LpeFOKgcioEcU0slv0go/Gu4Mb4BOe6PXwZHvuxMU2ZKI5qbqeAW9SjBVVKq5D3eNlapU0KcpzeZJOk3SxU2anWbL0/lk9BUFySO6/7H4PnZpytObGpiwTcvNLv7VEECygKcHjSVvlN6xjcJvutC8M6Jmtoxf5inGT9gLB5w0kxd4XuK5soZrzza1RSV41fCtarqGteBQquFGAOMGn2jYqkJpFXYsoCxfqwp1Mb4PB8xaOWsIWOP4IrratSQ9WWBYDnw8F9qKe+ZbtOaH+zm7uPw8jpE8cl7lySTL0/RxEEz5KIbJ0/Jjl6WT3LMCfGCNRe/ixb0PPGBcrNU8UBAnVOawayMKXikfrgI0VLxvqbCa56sVn0IyzbDSs+VSJCuZzxJRwHSeTdIsL/JnoMKlquqjBFPNu6BMRcnwVq/h30E2FoPswbFlouYagY2g/YkY8qxh98Q5c5U1BtibWHCu2SteWPTeOoVoQ6wgCjAaxBtakKDVGhzLGGx5qW3riT4t8Psjrw4UO+sc6mK+w9veVXzsOmPIY1J8gOt7ILTtMTpGfyUklFTJrnnb1ph59o63CgEftcdMnCMZNkri52tosCzsl8vz17/23HlQTTBjAqGDti/e/nmk+y2Biv0FQtTKjnvV/Str4OztFbm0VpLclbAGjBiwBXngjUZHmd955AFT+KJyMYFPJko5mZTAl6tksZwUyWxa8GSVTycJrAq+WvA0z7PpMxDlhTWlknCoihi+g8JSH9DzE1HgiQH14Gbn1mOLJzz0XgmrdY9vLD4ijjPsqcAdCkGJ+A09ymOl8fJI+dkVMayLfTHUPDBkkt34wQa+w4iVYSp4nAOdwxcGQmymR3pwBKke+W1XaCWom3fyhN1YVoNGqwhFiDxG8qEVL7iG8WAFu3obvWfv9n5jyVjRKR2+4+++fbPWbsBhyMVuSMS1LcOGI6kuOu4ke7lFyNAkQ5NrrjBHesjvFG8fRmKk6TV6FAX26v2TUZ8tV2I5S0UyLXJIZmnJkzyfLZMVl2UqFsUik5NnQP1Z5ZTodOgwQN4Fa2wzDP7WqTWlb4M4jhTe1+jnYcAPBHfal0+rClPEQFZwgLJHAA1To7VhgE4PZuMjdkprEZ7nugN2WyuCni1sUMIfWqJE6NH+RAiggTB4ZTtE08FZVHtMn4MFlGKVI4TuRxYuSj5ONfi7Uy3hh3qsRTlcoiB8Y4PGU6DVUNAkE3HxQ1kT+7LDHCQF93ExM5j9qO2w17BbSlwkXulsM8CcGsYfIKHvE7dDOsffTULD5Z6skauRGceRasyTo8Lvyddw7DQvqQwPrSlS6/fHmy7dISK13fmhPpgiij0W8GH0k5yvOfEbd0IRHC6fmAARV1X06r9gERsg7sslxyrjj02NPeLJVAbBV9lykSbZYsKT2ayEpIBslczlFGaZmIvZ5IlU/n/QqteuaF9ax6xSQ42AJLdoP2+5gHE8oZaoPUphPTxXSI9e8LbGTrmzHeHB3DOcNB1NFHo9xsGwhzenBU0O13GN72ttwFW7+LPG0iHnurCfFfvxZchdXFSg10UtQHsEe9vqXfSarWMa+/kX/7fBQAbN9E+Lh9CDhvUZJ5BGbpGVsqOuctLH+BgIb7ij/WYNN5Str5/+AalNIr4=', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b55d958ee4e244f7d5c5471ffbfac525'}, page_content='New 12th Gen Intel Core HX processors for hybrid work:\\xa0The company completed the 12th Gen family with the launch of the new 12th Gen Intel Core HX processors. Created for professionals who need maximum performance and flexibility to navigate a hybrid environment, and with up to 16 cores and clock speeds up to 5 GHz, the Intel Core i9-12900HX processor is the world’s best mobile workstation platform.\\n\\nHigh performance computing to solve the world’s most complex challenges:\\xa0Argonne National Laboratories is on track to deliver 2 exaflops of peak performance with the Aurora supercomputer running on the Intel Xeon processor, code-named Sapphire Rapids with High Bandwidth Memory (HBM), and the Intel data center GPU, code-named Ponte Vecchio, with Intel oneAPI providing developers seamless system integration.\\n\\nConfidence with confidential computing:\\xa0 Bosch and Intel collaborated on a research effort to develop a confidential AI solution that allows Bosch to train its neural networks confidentially in the public cloud. To help achieve this at scale, Bosch Corporate Research has built a confidential AI platform powered by Intel Software Guard Extensions available with 3rd Gen Intel Xeon Scalable platforms.\\n\\nAgriculture autonomy with private wireless networks: Intelligent edge solutions have the potential to transform food. Blue White Robotics developed a new type of autonomous agricultural solution that transforms a grower’s existing equipment into a fleet of autonomous tractors connected to an internet-based management platform. With help from Intel and Federated Wireless, Blue White Robotics made this a scalable solution that leverages Intel Smart Edge and Intel Xeon D processors and employs the power of edge computing and shared spectrum to create a private wireless network on any farm anywhere.\\n\\nIntel is moving at a “torrid pace,” Gelsinger said. “When you think about torrid, it’s a word about speed and energy and heat. But in the Intel context, we’re also applying a vector\\xa0of that energy for setting a direction into the future.”')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8a9413-c180-40e3-9762-0191a5928601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mW5UhN9BVTaE",
    "outputId": "06ca2b28-9e0b-4e6b-8de9-8ac5427f9668"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['Intel IDM 2.0', 'Intel’s global, internal factory network for at-scale manufacturing', 'Expanded use of third-party foundry capacity.', 'Intel Foundry Services.', 'Expanding in the U.S. and Europe'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '9b9fec55d420d43d84e13646715885e6'}, page_content='Intel IDM 2.0\\n\\nThe Intel IDM 2.0 strategy revealed by new company CEO Pat Gelsinger\\xa0is an ambitious plan to restore the company’s leadership in semiconductor production. Gelsinger described IDM 2.0 as the second generation of Intel’s integrated device manufacturing model.\\n\\nIntel’s global, internal factory network for at-scale manufacturing\\xa0is a key competitive advantage that enables product optimization, improved economics and supply resilience. Gelsinger re-affirmed the company’s expectation to continue manufacturing most of its products internally. The company’s 7 Nanometer Processors development is driven by increased use of extreme ultraviolet lithography (EUV) in a rearchitected, simplified process flow.\\n\\nExpanded use of third-party foundry capacity.\\xa0 Gelsinger said he expects Intel’s engagement with third-party foundries to grow and to include manufacturing for a range of modular tiles on advanced process technologies, including products at the core of Intel’s computing offerings for both client and data center segments beginning in 2023. This will provide the increased flexibility and scale needed to optimize Intel’s roadmaps for cost, performance, schedule and supply, giving the company a unique competitive advantage.\\n\\nIntel Foundry Services.\\xa0The launch of Intel Foundry Services means the company is not only going to manufacture its own chips, but it will also produce them for other semiconductor companies, including its competitors. \\xa0Intel announced plans to become a major provider of U.S. and Europe-based foundry capacity to serve the global demand for semiconductor manufacturing. Hence, Intel is establishing a new standalone business unit, Intel Foundry Services (IFS), led by semiconductor industry veteran Dr. Randhir Thakur, who will report directly to Gelsinger. IFS will be differentiated from other foundry offerings with a combination of leading-edge process technology and packaging, committed capacity in the U.S. and Europe, and a world-class IP portfolio for customers, including x86 cores as well as ARM and RISC-V ecosystem IPs. Gelsinger noted that Intel’s foundry plans have received strong statements of support from across the industry. Intel conservatively sizes the foundry opportunity as a $100 billion addressable market by 2025.\\n\\nExpanding in the U.S. and Europe. Intel is expanding its manufacturing capacity in the U.S. and Europe to provide less dependence on any specific region. Noting that 80% of leading-edge foundry capacity is concentrated in Asia, Gelsinger believes “the industry needs more geographically balanced manufacturing capacity.”')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa621ed2-a036-4202-99a6-96cea477c572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVP9nzW_WwhO",
    "outputId": "25416826-7929-4031-c1a8-98d0b4a29494"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': './docs/Intel Strategy.docx', 'emphasized_text_contents': ['Intel Sustainability'], 'emphasized_text_tags': ['b'], 'file_directory': './docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T02:27:51', 'orig_elements': 'eJzVVttu3DYQ/ZXBopcX09V9Jb+5TRAECIqicZ6SwBiSIy0RilRIyptN0H/vUN60SZEC7lv9JnA4F55zeKjXn3ZkaSaXbo3eXcFOjr3qu1GKsta1aNqqE8PQ9GKoBmoq3XZjPewuYDdTQo0JOefTTmGiyYfTraYlHXip4B00LweM5iPp20Qf0q3yLnGfyOHXu+f8beHlGhMah9JYk067t9/ISjjdZ8gtPBpLt9oEUon75YEvf9Jexd055nCmvHqun0Ke7HTJWz7kLRbdtOJE9yXJTVtRizHdzl6b0dAGQlVUrShqUXQ3RXVV7a/acvcHb8wTfVH96+lz/LRs3W9MspQz/onu2Na6qEcUeqylaIqqEZL2lWio3de6bItSy4eh+/8A4s1aFaW6ORCYeUGVwI+grJm5GagD9+BABHSwholRgMl6iRbSIRCmS/gt+MQHMG4CvwZYeCpKoGlGpyOXnEmbXIorG++4joYxUDxwAePe5TSUfk1w8EdeIjj6YDX4hfJp4yVcR/CO8lB/RX+MYAl1zo00GxalXjOE3DWa6b4Ht19H7rmGvE15PpozFC/gHkw+keGNsDrzfiVYfDTbeMlz5jueFhjAkQI5ReA8g+LsKafkM/qjOw/IKVxS8vhbtSOeeEpMWw3ukID4HlCAkYdTLDU/U+CEBUNy/LVNmiseD94S3KFdN8zNNkjKg8zEc7tpXO1nBJP3FxtnGp6RjRzkBhGNvoQv9fsrhjzgHd1knr+h44oqrYjYJSRq0chqFFJ1JIpCFkgdKlXuH5OObzywIi3bTpYWYzvLzOkaYfJoPxPPxsSiSZuYRm+tP2Z9GI4FM8PM0zJLjsnLlFVFXVy9cW/WosDiP4E7YN9V7VCKRvckGiy0kKPsRK9JdVr1sq71YwL3Wh0M3RGURfE9w+zoiJIVy4dWKRjF3glrzJc8+JiFHz/bxN/35PJLAF+YmJ4nmr+FXV83Yzt0g2g7ydhV3V4M+2IU/b7ErpZ9u68elcE+d3csKsBlCf7D5qtsJd/VRZHlZvOV5vvOkIbpxEblIoU7vPfKtCHJNpZfJ3av7Ap4ZqIBec5W67zaTY3wzrCgMWU3Xdle2DTPdSPesc4fzoGWra6kbkXZ60EwHyQGqktREtZj0w1jO9Jj4uDn1fCrwrLNaPIgGcztKfoK25nYGV69hGeByMGWk93hF7863gYvnj59AkziFHAG/m9wGoNmOzdO2XXbyadkDJledC4nkeZgZj9Duz052XdevbyAp2vgq7HNcB0NPpgYxKYe9yrbtGbXbstWYL/v+T9kqFDVdd9XzWMi5gUyTAd+OzfnEIaf8pjCCX7/4QnDxczcC5vJMZpRMOMJpswOv3nqkB9/tBGOJh2Alc+LZ9s5YpgzIYvP/6uGVzLSXEXz3bF+2aSAko+UsQV6v5olf/0LD2//BAM2usw=', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b8ae863a5df10800d4307a8d409a5034'}, page_content=\"Intel Sustainability\\n\\n“The impact of climate change is an urgent global threat. Protecting our planet demands immediate action and fresh thinking about how the world operates. As one of the world's leading semiconductor design and manufacturing companies, Intel is in a unique position to make a difference not only in our own operations, but in a way that makes it easier for customers, partners and our whole value chain to take meaningful action too,” Gelsinger said. \\n\\nTo realize this ambitious goal, Intel has set the following interim milestones for 2030:\\n\\xa0\\n\\nAchieve 100% renewable electricity use across its global operations.\\n\\nInvest approximately $300 million in energy conservation at its facilities to achieve 4 billion cumulative kilowatt hours of energy savings.\\n\\nBuild new factories and facilities to meet US Green Building Council LEED program standards, including recently announced investments in the US, Europe and Asia.\\n\\nLaunch a cross-industry R&D initiative to identify greener chemicals with lower global warming potential and to develop new abatement equipment.\")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3dc4d6-bc35-4c46-acf1-69df0b57607e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ewBJD6nbfYxg"
   },
   "source": [
    "### Directory Loaders\n",
    "\n",
    "LangChain's [`DirectoryLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.directory.DirectoryLoader.html) implements functionality for reading files from disk into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4d75c6-cffc-4793-93b4-b66181e06ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uvblyc6OZqpe",
    "outputId": "69876af8-1402-4bee-a199-782a4404186a"
   },
   "outputs": [],
   "source": [
    "# !wget -O 'Vision Transformers.pdf' 'https://arxiv.org/pdf/2010.11929.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3109be-0ec8-4aaf-8fcb-f526e43300f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eCXVqm3Ba2I8"
   },
   "source": [
    "We first define and assign specific loaders which can be used by LangChain when processing the files for a specific file type. We follow this format\n",
    "\n",
    "```\n",
    "loaders = {\n",
    "  'file_format_extension' : (LoaderClass, LoaderKeywordArguments)\n",
    "}\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `file_format_extension` can be anything like `.docx`, `.pdf`etc.\n",
    "- `LoaderClass` is a specific data loader like `PyMuPDFLoader`\n",
    "- `LoaderKeywordArguments` are any specific keyword arguments which needs to be passed into that loader at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5348db-a46b-461d-a8ee-6c5a3f2527fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1zzlBioka1HR"
   },
   "outputs": [],
   "source": [
    "# Define a dictionary to map file extensions to their respective loaders\n",
    "loaders = {\n",
    "    '.pdf': (PyMuPDFLoader, {}),\n",
    "    '.docx': (UnstructuredWordDocumentLoader, {'strategy': 'fast',\n",
    "                                              'chunking_strategy' : 'by_title',\n",
    "                                              'max_characters' : 3000, # max limit of a document chunk\n",
    "                                              'new_after_n_chars' : 2500, # preferred document chunk size\n",
    "                                              'mode' : 'elements'\n",
    "                                              })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c3d60e-02b4-43a9-bd49-56cedff4c98e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EBJZJtsGbnr7"
   },
   "source": [
    "`DirectoryLoader` accepts a `loader_cls` argument, which defaults to `UnstructuredLoader` but we can pass our own loaders which we defined above in the `loader_cls`argument and any keyword args for the loader can be passed in the `loader_kwargs` argument.\n",
    "\n",
    "We can also show a progress bar by setting `show_progress=True`\n",
    "\n",
    "We can use the `glob` parameter to control which files to load based on file patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ef32ee-d9cf-4af0-9f1d-a46cf66e08ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-mOmj9K-cB14"
   },
   "source": [
    "Here we create two separate loaders to load files which are word documents and PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0428c1e-e7d8-44b8-97a7-4b786e7d0c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbnt25HdXmkc",
    "outputId": "f17c2aef-1db8-48a1-e00b-47eadfdcfb61"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/7 [00:00<?, ?it/s]\r 14%|█▍        | 1/7 [00:00<00:03,  2.00it/s]\r 29%|██▊       | 2/7 [00:00<00:02,  2.03it/s]\r 43%|████▎     | 3/7 [00:01<00:02,  1.38it/s]\r 57%|█████▋    | 4/7 [00:05<00:05,  1.92s/it]\r 71%|███████▏  | 5/7 [00:11<00:06,  3.36s/it]\r 86%|████████▌ | 6/7 [00:13<00:02,  2.70s/it]\r100%|██████████| 7/7 [00:16<00:00,  2.97s/it]\r100%|██████████| 7/7 [00:16<00:00,  2.38s/it]\n\r  0%|          | 0/1 [00:00<?, ?it/s]\r100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\r100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Define a function to create a DirectoryLoader for a specific file type\n",
    "def create_directory_loader(file_type, directory_path):\n",
    "    return DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=f\"**/*{file_type}\",\n",
    "        loader_cls=loaders[file_type][0],\n",
    "        loader_kwargs=loaders[file_type][1],\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "# Create DirectoryLoader instances for each file type\n",
    "pdf_loader = create_directory_loader('.pdf', './docs')\n",
    "docx_loader = create_directory_loader('.docx', './docs')\n",
    "\n",
    "# Load the files\n",
    "pdf_documents = pdf_loader.load()\n",
    "docx_documents = docx_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c01bef-16ce-4168-bfef-e8e946eb7d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXYc4p_eXmmt",
    "outputId": "2c5aeae6-a64b-4104-93e6-dce2374ba599"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd62f4d6-d7c6-41a5-9606-36fbc13aa020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='How and Why to Use LLMs for\\nChunk-Based Information Retrieval\\nCarlo Peron\\nPublished in Towards Data Science · 9 min read · 1 day ago\\n24\\nRetrieve pipeline — Image by the author\\nIn this article, I aim to explain how and why it’s beneficial to use a Large\\nLanguage Model (LLM) for chunk-based information retrieval.\\nI use OpenAI’s GPT-4 model as an example, but this approach can be applied\\nwith any other LLM, such as those from Hugging Face, Claude, and others.\\nEveryone can access this article for free.\\nConsiderations on standard information retrieval\\nThe primary concept involves having a list of documents (chunks of text)\\nstored in a database, which could be retrieve based on some filter and\\nconditions.\\nSearch\\nWrite\\n60\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 1, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='Typically, a tool is used to enable hybrid search (such as Azure AI Search,\\nLlamaIndex, etc.), which allows:\\nperforming a text-based search using term frequency algorithms like TF-\\nIDF (e.g., BM25);\\nconducting a vector-based search, which identifies similar concepts even\\nwhen different terms are used, by calculating vector distances (typically\\ncosine similarity);\\ncombining elements from steps 1 and 2, weighting them to highlight the\\nmost relevant results.\\nFigure 1- Default hybrid search pipeline — Image by the author\\nFigure 1 shows the classic retrieval pipeline:\\nthe user asks the system a question: “I would like to talk about Paris”;\\nthe system receives the question, converts it into an embedding vector\\n(using the same model applied in the ingestion phase), and finds the\\nchunks with the smallest distances;\\nthe system also performs a text-based search based on frequency;\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 2, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='the chunks returned from both processes undergo further evaluation and\\nare reordered based on a ranking formula.\\nThis solution achieves good results but has some limitations:\\nnot all relevant chunks are always retrieved;\\nsometime some chunks contain anomalies that affect the final response.\\nAn example of a typical retrieval issue\\nLet’s consider the “documents” array, which represents an example of a\\nknowledge base that could lead to incorrect chunk selection.\\ndocuments = [\\n\"Chunk 1: This document contains information about topic A.\",\\n\"Chunk 2: Insights related to topic B can be found here.\",\\n\"Chunk 3: This chunk discusses topic C in detail.\",\\n\"Chunk 4: Further insights on topic D are covered here.\",\\n\"Chunk 5: Another chunk with more data on topic E.\",\\n\"Chunk 6: Extensive research on topic F is presented.\",\\n\"Chunk 7: Information on topic G is explained here.\",\\n\"Chunk 8: This document expands on topic H. It also talk about topic B\",\\n\"Chunk 9: Nothing about topic B are given.\",\\n\"Chunk 10: Finally, a discussion of topic J. This document doesn\\'t contain i\\n]\\nLet’s assume we have a RAG system, consisting of a vector database with\\nhybrid search capabilities and an LLM-based prompt, to which the user\\nposes the following question: “I need to know something about topic B.”\\nAs shown in Figure 2, the search also returns an incorrect chunk that, while\\nsemantically relevant, is not suitable for answering the question and, in\\nsome cases, could even confuse the LLM tasked with providing a response.\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 3, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='Figure 2 — Example of information retrieval that can lead to errors — Image by the author\\nIn this example, the user requests information about “topic B,” and the\\nsearch returns chunks that include “This document expands on topic H. It also\\ntalks about topic B” and “Insights related to topic B can be found here.” as well as\\nthe chunk stating, “Nothing about topic B are given”.\\nWhile this is the expected behavior of hybrid search (as chunks reference\\n“topic B”), it is not the desired outcome, as the third chunk is returned\\nwithout recognizing that it isn’t helpful for answering the question.\\nThe retrieval didn’t produce the intended result, not only because the BM25\\nsearch found the term “topic B” in the third Chunk but also because the\\nvector search yielded a high cosine similarity.\\nTo understand this, refer to Figure 3, which shows the cosine similarity\\nvalues of the chunks relative to the question, using OpenAI’s text-\\nembedding-ada-002 model for embeddings.\\nFigure 3 — Cosine similarity with text-embedding-ada-002- Image by the author\\nIt is evident that the cosine similarity value for “Chunk 9” is among the\\nhighest, and that between this chunk and chunk 10, which references “topic\\nB,” there is also chunk 1, which does not mention “topic B”.\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 4, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='This situation remains unchanged even when measuring distance using a\\ndifferent method, as seen in the case of Minkowski distance.\\nUtilizing LLMs for Information Retrieval: An Example\\nThe solution I will describe is inspired by what has been published in my\\nGitHub repository https://github.com/peronc/LLMRetriever/.\\nThe idea is to have the LLM analyze which chunks are useful for answering\\nthe user’s question, not by ranking the returned chunks (as in the case of\\nRankGPT) but by directly evaluating all the available chunks.\\nFigure 4- LLM Retrieve pipeline — Image by the author\\nIn summary, as shown in Figure 4, the system receives a list of documents to\\nanalyze, which can come from any data source, such as file storage,\\nrelational databases, or vector databases.\\nThe chunks are divided into groups and processed in parallel by a number of\\nthreads proportional to the total amount of chunks.\\nThe logic for each thread includes a loop that iterates through the input\\nchunks, calling an OpenAI prompt for each one to check its relevance to the\\nuser’s question.\\nThe prompt returns the chunk along with a boolean value: true if it is\\nrelevant and false if it is not.\\nLets’go coding\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 5, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='To explain the code, I will simplify by using the chunks present in the\\ndocuments array (I will reference a real case in the conclusions).\\nFirst of all, I import the necessary standard libraries, including os,\\nlangchain, and dotenv.\\nimport os\\nfrom langchain_openai.chat_models.azure import AzureChatOpenAI\\nfrom dotenv import load_dotenv\\nNext, I import my LLMRetrieverLib/llm_retrieve.py class, which provides\\nseveral static methods essential for performing the analysis.\\nfrom LLMRetrieverLib.retriever import llm_retriever\\nFollowing that, I need to import the necessary variables required for\\nutilizing Azure OpenAI GPT-4o model.\\nload_dotenv()\\nazure_deployment = os.getenv(\"AZURE_DEPLOYMENT\")\\ntemperature = float(os.getenv(\"TEMPERATURE\"))\\napi_key\\n= os.getenv(\"AZURE_OPENAI_API_KEY\")\\nendpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\\napi_version = os.getenv(\"API_VERSION\")\\nNext, I proceed with the initialization of the LLM.\\n# Initialize the LLM\\nllm = AzureChatOpenAI(api_key=api_key, azure_endpoint=endpoint, azure_deployment\\nWe are ready to begin: the user asks a question to gather additional\\ninformation about Topic B.\\nquestion = \"I need to know something about topic B\"\\nAt this point, the search for relevant chunks begins, and to do this, I use the\\nfunction llm_retrieve.process_chunks_in_parallel from the\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 6, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='LLMRetrieverLib/retriever.py library, which is also found in the same\\nrepository.\\nrelevant_chunks = LLMRetrieverLib.retriever.llm_retriever.process_chunks_in_para\\nTo optimize performance, the function\\nllm_retrieve.process_chunks_in_parallel employs multi-threading to\\ndistribute chunk analysis across multiple threads.\\nThe main idea is to assign each thread a subset of chunks extracted from the\\ndatabase and have each thread analyze the relevance of those chunks based\\non the user’s question.\\nAt the end of the processing, the returned chunks are exactly as expected:\\n[\\'Chunk 2: Insights related to topic B can be found here.\\',\\n\\'Chunk 8: This document expands on topic H. It also talk about topic B\\']\\nFinally, I ask the LLM to provide an answer to the user’s question:\\nfinal_answer = LLMRetrieverLib.retriever.llm_retriever.generate_final_answer_wit\\nprint(\"Final answer:\")\\nprint(final_answer)\\nBelow is the LLM’s response, which is trivial since the content of the chunks,\\nwhile relevant, is not exhaustive on the topic of Topic B:\\nTopic B is covered in both Chunk 2 and Chunk 8.\\nChunk 2 provides insights specifically related to topic B, offering detailed inf\\nChunk 8 expands on topic H but also includes discussions on topic B, potentially\\nScoring Scenario\\nNow let’s try asking the same question but using an approach based on\\nscoring.\\n'),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 7, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content=\"I ask the LLM to assign a score from 1 to 10 to evaluate the relevance\\nbetween each chunk and the question, considering only those with a\\nrelevance higher than 5.\\nTo do this, I call the function llm_retriever.process_chunks_in_parallel ,\\npassing three additional parameters that indicate, respectively, that scoring\\nwill be applied, that the threshold for being considered valid must be greater\\nthan or equal to 5, and that I want a printout of the chunks with their\\nrespective scores.\\nrelevant_chunks = llm_retriever.process_chunks_in_parallel(llm, question, docume\\nThe retrieval phase with scoring produces the following result:\\nscore: 1 - Chunk 1: This document contains information about topic A.\\nscore: 1 - Chunk 7: Information on topic G is explained here.\\nscore: 1 - Chunk 4: Further insights on topic D are covered here.\\nscore: 9 - Chunk 2: Insights related to topic B can be found here.\\nscore: 7 - Chunk 8: This document expands on topic H. It also talk about topic B\\nscore: 1 - Chunk 5: Another chunk with more data on topic E.\\nscore: 1 - Chunk 9: Nothing about topic B are given.\\nscore: 1 - Chunk 3: This chunk discusses topic C in detail.\\nscore: 1 - Chunk 6: Extensive research on topic F is presented.\\nscore: 1 - Chunk 10: Finally, a discussion of topic J. This document doesn't con\\nIt’s the same as before, but with an interesting score\\n.\\nFinally, I once again ask the LLM to provide an answer to the user’s question,\\nand the result is similar to the previous one:\\nChunk 2 provides insights related to topic B, offering foundational information\\nChunk 8 expands on topic B further, possibly providing additional context or det\\nTogether, these chunks should give you a well-rounded understanding of topic B.\\nConsiderations\\nThis retrieval approach has emerged as a necessity following some previous\\nexperiences.\\nI have noticed that pure vector-based searches produce useful results but are\\noften insufficient when the embedding is performed in a language other\\nthan English.\\n\"),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 8, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content=\"Using OpenAI with sentences in Italian makes it clear that the tokenization\\nof terms is often incorrect; for example, the term “canzone,” which means\\n“song” in Italian, gets tokenized into two distinct words: “can” and “zone”.\\nThis leads to the construction of an embedding array that is far from what\\nwas intended.\\nIn cases like this, hybrid search, which also incorporates term frequency\\ncounting, leads to improved results, but they are not always as expected.\\nSo, this retrieval methodology can be utilized in the following ways:\\nas the primary search method: where the database is queried for all\\nchunks or a subset based on a filter (e.g., a metadata filter);\\nas a refinement in the case of hybrid search: (this is the same approach\\nused by RankGPT) in this way, the hybrid search can extract a large\\nnumber of chunks, and the system can filter them so that only the\\nrelevant ones reach the LLM while also adhering to the input token limit;\\nas a fallback: in situations where a hybrid search does not yield the\\ndesired results, all chunks can be analyzed.\\nLet’s discuss costs and performance\\nOf course, all that glitters is not gold, as one must consider response times\\nand costs.\\nIn a real use case, I retrieved the chunks from a relational database\\nconsisting of 95 text segments semantically split using my\\nLLMChunkizerLib/chunkizer.py  library from two Microsoft Word documents,\\ntotaling 33 pages.\\nThe analysis of the relevance of the 95 chunks to the question was conducted\\nby calling OpenAI's APIs from a local PC with non-guaranteed bandwidth,\\naveraging around 10Mb, resulting in response times that varied from 7 to 20\\nseconds.\\nNaturally, on a cloud system or by using local LLMs on GPUs, these times can\\nbe significantly reduced.\\nI believe that considerations regarding response times are highly subjective:\\nin some cases, it is acceptable to take longer to provide a correct answer,\\nwhile in others, it is essential not to keep users waiting too long.\\nSimilarly, considerations about costs are also quite subjective, as one must\\ntake a broader perspective to evaluate whether it is more important to\\n\"),\n",
       " Document(metadata={'source': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'file_path': 'docs/WEB_How_and_Why_to_UseLLMs_for_Chunk_Based_Information_Retrieval_Carlo_Peron_Oct_2024_TowardsDataScience.pdf', 'page': 9, 'total_pages': 10, 'format': 'PDF 1.7', 'title': 'How and Why to Use LLMs for Chunk-Based Information Retrieval | by Carlo Peron | Oct, 2024 | Towards Data Science', 'author': 'cperon', 'subject': '', 'keywords': '', 'creator': 'PDF24 Creator', 'producer': 'GPL Ghostscript 10.01.2', 'creationDate': \"D:20241030091728+01'00'\", 'modDate': \"D:20241030091728+01'00'\", 'trapped': ''}, page_content='provide as accurate answers as possible or if some errors are acceptable.\\nIn certain fields, the damage to one’s reputation caused by incorrect or\\nmissing answers can outweigh the expense of tokens.\\nFurthermore, even though the costs of OpenAI and other providers have\\nbeen steadily decreasing in recent years, those who already have a GPU-\\nbased infrastructure, perhaps due to the need to handle sensitive or\\nconfidential data, will likely prefer to use a local LLM.\\nConclusions\\nIn conclusion, I hope to have provided my perspective on how retrieval can\\nbe approached.\\nIf nothing else, I aim to be helpful and perhaps inspire others to explore new\\nmethods in their own work.\\nRemember, the world of information retrieval is vast, and with a little\\ncreativity and the right tools, we can uncover knowledge in ways we never\\nimagined!\\nIf you’d like to discuss this further, feel free to connect with me on LinkedIn\\nGitHub repositories can be found here:\\n• https://github.com/peronc/LLMRetriever/\\n• https://github.com/peronc/LLMChunkizer/\\nWritten by Carlo Peron\\n58 Followers · Writer for Towards Data Science\\nYou can get more information about me on https://www.linkedin.com/in/carlo-peron\\nEdit profile\\nChunking\\nRetrieval\\nArtificial Intelligence\\nLlm\\nMachine Learning\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 0, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='An Introduction to Convolutional Neural Networks\\nKeiron O’Shea1 and Ryan Nash2\\n1 Department of Computer Science, Aberystwyth University, Ceredigion, SY23 3DB\\nkeo7@aber.ac.uk\\n2 School of Computing and Communications, Lancaster University, Lancashire, LA1\\n4YW\\nnashrd@live.lancs.ac.uk\\nAbstract. The ﬁeld of machine learning has taken a dramatic twist in re-\\ncent times, with the rise of the Artiﬁcial Neural Network (ANN). These\\nbiologically inspired computational models are able to far exceed the per-\\nformance of previous forms of artiﬁcial intelligence in common machine\\nlearning tasks. One of the most impressive forms of ANN architecture is\\nthat of the Convolutional Neural Network (CNN). CNNs are primarily\\nused to solve difﬁcult image-driven pattern recognition tasks and with\\ntheir precise yet simple architecture, offers a simpliﬁed method of getting\\nstarted with ANNs.\\nThis document provides a brief introduction to CNNs, discussing recently\\npublished papers and newly formed techniques in developing these bril-\\nliantly fantastic image recognition models. This introduction assumes you\\nare familiar with the fundamentals of ANNs and machine learning.\\nKeywords: Pattern recognition, artiﬁcial neural networks, machine learn-\\ning, image analysis\\n1\\nIntroduction\\nArtiﬁcial Neural Networks (ANNs) are computational processing systems of\\nwhich are heavily inspired by way biological nervous systems (such as the hu-\\nman brain) operate. ANNs are mainly comprised of a high number of intercon-\\nnected computational nodes (referred to as neurons), of which work entwine in\\na distributed fashion to collectively learn from the input in order to optimise its\\nﬁnal output.\\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would\\nload the input, usually in the form of a multidimensional vector to the input\\nlayer of which will distribute it to the hidden layers. The hidden layers will then\\nmake decisions from the previous layer and weigh up how a stochastic change\\nwithin itself detriments or improves the ﬁnal output, and this is referred to as\\nthe process of learning. Having multiple hidden layers stacked upon each-other\\nis commonly called deep learning.\\narXiv:1511.08458v2  [cs.NE]  2 Dec 2015\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 1, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='2\\nKeiron O’Shea et al.\\nInput 1\\nInput 2\\nInput 3\\nInput 4\\nInput Layer\\nHidden Layer\\nOutput Layer\\nOutput\\nFig. 1: A simple three layered feedforward neural network (FNN), comprised\\nof a input layer, a hidden layer and an output layer. This structure is the basis\\nof a number of common ANN architectures, included but not limited to Feed-\\nforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and\\nRecurrent Neural Networks (RNNs).\\nThe two key learning paradigms in image processing tasks are supervised and\\nunsupervised learning. Supervised learning is learning through pre-labelled\\ninputs, which act as targets. For each training example there will be a set of\\ninput values (vectors) and one or more associated designated output values.\\nThe goal of this form of training is to reduce the models overall classiﬁcation\\nerror, through correct calculation of the output value of training example by\\ntraining.\\nUnsupervised learning differs in that the training set does not include any la-\\nbels. Success is usually determined by whether the network is able to reduce or\\nincrease an associated cost function. However, it is important to note that most\\nimage-focused pattern-recognition tasks usually depend on classiﬁcation using\\nsupervised learning.\\nConvolutional Neural Networks (CNNs) are analogous to traditional ANNs\\nin that they are comprised of neurons that self-optimise through learning. Each\\nneuron will still receive an input and perform a operation (such as a scalar\\nproduct followed by a non-linear function) - the basis of countless ANNs. From\\nthe input raw image vectors to the ﬁnal output of the class score, the entire of\\nthe network will still express a single perceptive score function (the weight).\\nThe last layer will contain loss functions associated with the classes, and all of\\nthe regular tips and tricks developed for traditional ANNs still apply.\\nThe only notable difference between CNNs and traditional ANNs is that CNNs\\nare primarily used in the ﬁeld of pattern recognition within images. This allows\\nus to encode image-speciﬁc features into the architecture, making the network\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 2, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Introduction to Convolutional Neural Networks\\n3\\nmore suited for image-focused tasks - whilst further reducing the parameters\\nrequired to set up the model.\\nOne of the largest limitations of traditional forms of ANN is that they tend to\\nstruggle with the computational complexity required to compute image data.\\nCommon machine learning benchmarking datasets such as the MNIST database\\nof handwritten digits are suitable for most forms of ANN, due to its relatively\\nsmall image dimensionality of just 28 × 28. With this dataset a single neuron in\\nthe ﬁrst hidden layer will contain 784 weights (28×28×1 where 1 bare in mind\\nthat MNIST is normalised to just black and white values), which is manageable\\nfor most forms of ANN.\\nIf you consider a more substantial coloured image input of 64 × 64, the number\\nof weights on just a single neuron of the ﬁrst layer increases substantially to\\n12, 288. Also take into account that to deal with this scale of input, the network\\nwill also need to be a lot larger than one used to classify colour-normalised\\nMNIST digits, then you will understand the drawbacks of using such models.\\n1.1\\nOverﬁtting\\nBut why does it matter? Surely we could just increase the number of hidden lay-\\ners in our network, and perhaps increase the number of neurons within them?\\nThe simple answer to this question is no. This is down to two reasons, one be-\\ning the simple problem of not having unlimited computational power and time\\nto train these huge ANNs.\\nThe second reason is stopping or reducing the effects of overﬁtting. Overﬁtting\\nis basically when a network is unable to learn effectively due to a number of\\nreasons. It is an important concept of most, if not all machine learning algo-\\nrithms and it is important that every precaution is taken as to reduce its effects.\\nIf our models were to exhibit signs of overﬁtting then we may see a reduced\\nability to pinpoint generalised features for not only our training dataset, but\\nalso our test and prediction sets.\\nThis is the main reason behind reducing the complexity of our ANNs. The less\\nparameters required to train, the less likely the network will overﬁt - and of\\ncourse, improve the predictive performance of the model.\\n2\\nCNN architecture\\nAs noted earlier, CNNs primarily focus on the basis that the input will be com-\\nprised of images. This focuses the architecture to be set up in way to best suit\\nthe need for dealing with the speciﬁc type of data.\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 3, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='4\\nKeiron O’Shea et al.\\nOne of the key differences is that the neurons that the layers within the CNN\\nare comprised of neurons organised into three dimensions, the spatial dimen-\\nsionality of the input (height and the width) and the depth. The depth does not\\nrefer to the total number of layers within the ANN, but the third dimension of a\\nactivation volume. Unlike standard ANNS, the neurons within any given layer\\nwill only connect to a small region of the layer preceding it.\\nIn practice this would mean that for the example given earlier, the input ’vol-\\nume’ will have a dimensionality of 64 × 64 × 3 (height, width and depth), lead-\\ning to a ﬁnal output layer comprised of a dimensionality of 1 × 1 × n (where\\nn represents the possible number of classes) as we would have condensed the\\nfull input dimensionality into a smaller volume of class scores ﬁled across the\\ndepth dimension.\\n2.1\\nOverall architecture\\nCNNs are comprised of three types of layers. These are convolutional layers,\\npooling layers and fully-connected layers. When these layers are stacked, a\\nCNN architecture has been formed. A simpliﬁed CNN architecture for MNIST\\nclassiﬁcation is illustrated in Figure 2.\\ninput\\n0\\n9\\nconvolution\\n w/ReLu\\npooling\\noutput\\nfully-connected\\nw/ ReLu\\nfully-connected\\n...\\nFig. 2: An simple CNN architecture, comprised of just ﬁve layers\\nThe basic functionality of the example CNN above can be broken down into\\nfour key areas.\\n1. As found in other forms of ANN, the input layer will hold the pixel values\\nof the image.\\n2. The convolutional layer will determine the output of neurons of which are\\nconnected to local regions of the input through the calculation of the scalar\\nproduct between their weights and the region connected to the input vol-\\nume. The rectiﬁed linear unit (commonly shortened to ReLu) aims to apply\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 4, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Introduction to Convolutional Neural Networks\\n5\\nan ’elementwise’ activation function such as sigmoid to the output of the\\nactivation produced by the previous layer.\\n3. The pooling layer will then simply perform downsampling along the spa-\\ntial dimensionality of the given input, further reducing the number of pa-\\nrameters within that activation.\\n4. The fully-connected layers will then perform the same duties found in\\nstandard ANNs and attempt to produce class scores from the activations,\\nto be used for classiﬁcation. It is also suggested that ReLu may be used\\nbetween these layers, as to improve performance.\\nThrough this simple method of transformation, CNNs are able to transform\\nthe original input layer by layer using convolutional and downsampling tech-\\nniques to produce class scores for classiﬁcation and regression purposes.\\nFig. 3: Activations taken from the ﬁrst convolutional layer of a simplistic deep\\nCNN, after training on the MNIST database of handwritten digits. If you look\\ncarefully, you can see that the network has successfully picked up on character-\\nistics unique to speciﬁc numeric digits.\\nHowever, it is important to note that simply understanding the overall archi-\\ntecture of a CNN architecture will not sufﬁce. The creation and optimisation\\nof these models can take quite some time, and can be quite confusing. We will\\nnow explore in detail the individual layers, detailing their hyperparameters\\nand connectivities.\\n2.2\\nConvolutional layer\\nAs the name implies, the convolutional layer plays a vital role in how CNNs\\noperate. The layers parameters focus around the use of learnable kernels.\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 5, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='6\\nKeiron O’Shea et al.\\nThese kernels are usually small in spatial dimensionality, but spreads along the\\nentirety of the depth of the input. When the data hits a convolutional layer,\\nthe layer convolves each ﬁlter across the spatial dimensionality of the input to\\nproduce a 2D activation map. These activation maps can be visualised, as seen\\nin Figure 3.\\nAs we glide through the input, the scalar product is calculated for each value in\\nthat kernel. (Figure 4) From this the network will learn kernels that ’ﬁre’ when\\nthey see a speciﬁc feature at a given spatial position of the input. These are\\ncommonly known as activations.\\n0\\n0\\n0\\n1\\n0\\n2\\n0\\n1\\n1\\n4\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n-4\\n-8\\nPooled Vector\\nKernel\\nDestination Pixel\\n0\\n0\\n0\\n1\\n0\\n2\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n2\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n1\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\nInput Vector\\nFig. 4: A visual representation of a convolutional layer. The centre element of the\\nkernel is placed over the input vector, of which is then calculated and replaced\\nwith a weighted sum of itself and any nearby pixels.\\nEvery kernel will have a corresponding activation map, of which will be stacked\\nalong the depth dimension to form the full output volume from the convolu-\\ntional layer.\\nAs we alluded to earlier, training ANNs on inputs such as images results in\\nmodels of which are too big to train effectively. This comes down to the fully-\\nconnected manner of standard ANN neurons, so to mitigate against this every\\nneuron in a convolutional layer is only connected to small region of the input\\nvolume. The dimensionality of this region is commonly referred to as the re-\\nceptive ﬁeld size of the neuron. The magnitude of the connectivity through the\\ndepth is nearly always equal to the depth of the input.\\nFor example, if the input to the network is an image of size 64 × 64 × 3 (a RGB-\\ncoloured image with a dimensionality of 64 × 64) and we set the receptive ﬁeld\\nsize as 6 × 6, we would have a total of 108 weights on each neuron within the\\nconvolutional layer. (6 × 6 × 3 where 3 is the magnitude of connectivity across\\nthe depth of the volume) To put this into perspective, a standard neuron seen\\nin other forms of ANN would contain 12, 288 weights each.\\nConvolutional layers are also able to signiﬁcantly reduce the complexity of the\\nmodel through the optimisation of its output. These are optimised through\\nthree hyperparameters, the depth, the stride and setting zero-padding.\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 6, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Introduction to Convolutional Neural Networks\\n7\\nThe depth of the output volume produced by the convolutional layers can be\\nmanually set through the number of neurons within the layer to a the same\\nregion of the input. This can be seen with other forms of ANNs, where the\\nall of the neurons in the hidden layer are directly connected to every single\\nneuron beforehand. Reducing this hyperparameter can signiﬁcantly minimise\\nthe total number of neurons of the network, but it can also signiﬁcantly reduce\\nthe pattern recognition capabilities of the model.\\nWe are also able to deﬁne the stride in which we set the depth around the spatial\\ndimensionality of the input in order to place the receptive ﬁeld. For example if\\nwe were to set a stride as 1, then we would have a heavily overlapped receptive\\nﬁeld producing extremely large activations. Alternatively, setting the stride to a\\ngreater number will reduce the amount of overlapping and produce an output\\nof lower spatial dimensions.\\nZero-padding is the simple process of padding the border of the input, and\\nis an effective method to give further control as to the dimensionality of the\\noutput volumes.\\nIt is important to understand that through using these techniques, we will alter\\nthe spatial dimensionality of the convolutional layers output. To calculate this,\\nyou can make use of the following formula:\\n(V −R) + 2Z\\nS + 1\\nWhere V represents the input volume size (height×width×depth), R represents\\nthe receptive ﬁeld size, Z is the amount of zero padding set and S referring to\\nthe stride. If the calculated result from this equation is not equal to a whole\\ninteger then the stride has been incorrectly set, as the neurons will be unable to\\nﬁt neatly across the given input.\\nDespite our best efforts so far we will still ﬁnd that our models are still enor-\\nmous if we use an image input of any real dimensionality. However, methods\\nhave been developed as to greatly curtail the overall number of parameters\\nwithin the convolutional layer.\\nParameter sharing works on the assumption that if one region feature is useful\\nto compute at a set spatial region, then it is likely to be useful in another region.\\nIf we constrain each individual activation map within the output volume to the\\nsame weights and bias, then we will see a massive reduction in the number of\\nparameters being produced by the convolutional layer.\\nAs a result of this as the backpropagation stage occurs, each neuron in the out-\\nput will represent the overall gradient of which can be totalled across the depth\\n- thus only updating a single set of weights, as opposed to every single one.\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 7, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='8\\nKeiron O’Shea et al.\\n2.3\\nPooling layer\\nPooling layers aim to gradually reduce the dimensionality of the representa-\\ntion, and thus further reduce the number of parameters and the computational\\ncomplexity of the model.\\nThe pooling layer operates over each activation map in the input, and scales\\nits dimensionality using the “MAX” function. In most CNNs, these come in the\\nform of max-pooling layers with kernels of a dimensionality of 2 × 2 applied\\nwith a stride of 2 along the spatial dimensions of the input. This scales the\\nactivation map down to 25% of the original size - whilst maintaining the depth\\nvolume to its standard size.\\nDue to the destructive nature of the pooling layer, there are only two generally\\nobserved methods of max-pooling. Usually, the stride and ﬁlters of the pooling\\nlayers are both set to 2 × 2, which will allow the layer to extend through the\\nentirety of the spatial dimensionality of the input. Furthermore overlapping\\npooling may be utilised, where the stride is set to 2 with a kernel size set to\\n3. Due to the destructive nature of pooling, having a kernel size above 3 will\\nusually greatly decrease the performance of the model.\\nIt is also important to understand that beyond max-pooling, CNN architectures\\nmay contain general-pooling. General pooling layers are comprised of pooling\\nneurons that are able to perform a multitude of common operations including\\nL1/L2-normalisation, and average pooling. However, this tutorial will primar-\\nily focus on the use of max-pooling.\\n2.4\\nFully-connected layer\\nThe fully-connected layer contains neurons of which are directly connected to\\nthe neurons in the two adjacent layers, without being connected to any layers\\nwithin them. This is analogous to way that neurons are arranged in traditional\\nforms of ANN. (Figure 1)\\n3\\nRecipes\\nDespite the relatively small number of layers required to form a CNN, there\\nis no set way of formulating a CNN architecture. That being said, it would be\\nidiotic to simply throw a few of layers together and expect it to work. Through\\nreading of related literature it is obvious that much like other forms of ANNs,\\nCNNs tend to follow a common architecture. This common architecture is illus-\\ntrated in Figure 2, where convolutional layers are stacked, followed by pooling\\nlayers in a repeated manner before feeding forward to fully-connected layers.\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Introduction to Convolutional Neural Networks\\n9\\nAnother common CNN architecture is to stack two convolutional layers before\\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\\nstacking multiple convolutional layers allows for more complex features of the\\ninput vector to be selected.\\ninput\\nconvolution w/ ReLu\\npooling\\nconvolution\\nw/ ReLu\\npooling\\nfully-connected\\nw/ ReLu\\nfully-connected\\nconvolution w/ ReLu\\npooling\\n0\\n9\\noutput \\n...\\nFig. 5: A common form of CNN architecture in which convolutional layers are\\nstacked between ReLus continuously before being passed through the pooling\\nlayer, before going between one or many fully connected ReLus.\\nIt is also advised to split large convolutional layers up into many smaller sized\\nconvolutional layers. This is to reduce the amount of computational complexity\\nwithin a given convolutional layer. For example, if you were to stack three con-\\nvolutional layers on top of each other with a receptive ﬁeld of 3×3. Each neuron\\nof the ﬁrst convolutional layer will have a 3×3 view of the input vector. A neu-\\nron on the second convolutional layer will then have a 5 × 5 view of the input\\nvector. A neuron on the third convolutional layer will then have a 7 × 7 view of\\nthe input vector. As these stacks feature non-linearities which in turn allows us\\nto express stronger features of the input with fewer parameters. However, it is\\nimportant to understand that this does come with a distinct memory allocation\\nproblem - especially when making use of the backpropagation algorithm.\\nThe input layer should be recursively divisible by two. Common numbers in-\\nclude 32 × 32, 64 × 64, 96 × 96, 128 × 128 and 224 × 224.\\nWhilst using small ﬁlters, set stride to one and make use of zero-padding as to\\nensure that the convolutional layers do not reconﬁgure any of the dimension-\\nality of the input. The amount of zero-padding to be used should be calculated\\nby taking one away from the receptive ﬁeld size and dividing by two.activation\\nCNNs are extremely powerful machine learning algorithms, however they can\\nbe horrendously resource-heavy. An example of this problem could be in ﬁlter-\\ning a large image (anything over 128 × 128 could be considered large), so if the\\ninput is 227 × 227 (as seen with ImageNet) and we’re ﬁltering with 64 kernels\\neach with a zero padding of then the result will be three activation vectors of\\nsize 227 × 227 × 64 - which calculates to roughly 10 million activations - or an\\nenormous 70 megabytes of memory per image. In this case you have two op-\\ntions. Firstly, you can reduce the spatial dimensionality of the input images by\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 9, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='10\\nKeiron O’Shea et al.\\nresizing the raw images to something a little less heavy. Alternatively, you can\\ngo against everything we stated earlier in this document and opt for larger ﬁlter\\nsizes with a larger stride (2, as opposed to 1).\\nIn addition to the few rules-of-thumb outlined above, it is also important to ac-\\nknowledge a few ’tricks’ about generalised ANN training techniques. The au-\\nthors suggest a read of Geoffrey Hinton’s excellent “Practical Guide to Training\\nRestricted Boltzmann Machines”.\\n4\\nConclusion\\nConvolutional Neural Networks differ to other forms of Artiﬁcal Neural Net-\\nwork in that instead of focusing on the entirety of the problem domain, knowl-\\nedge about the speciﬁc type of input is exploited. This in turn allows for a much\\nsimpler network architecture to be set up.\\nThis paper has outlined the basic concepts of Convolutional Neural Networks,\\nexplaining the layers required to build one and detailing how best to structure\\nthe network in most image analysis tasks.\\nResearch in the ﬁeld of image analysis using neural networks has somewhat\\nslowed in recent times. This is partly due to the incorrect belief surrounding the\\nlevel of complexity and knowledge required to begin modelling these superbly\\npowerful machine learning algorithms. The authors hope that this paper has\\nin some way reduced this confusion, and made the ﬁeld more accessible to\\nbeginners.\\nAcknowledgements\\nThe authors would like to thank Dr. Chuan Lu and Nicholas Dimonaco for\\nuseful discussion and suggestions.\\nReferences\\n1. Ciresan, D., Meier, U., Schmidhuber, J.: Multi-column deep neural networks for im-\\nage classiﬁcation. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE\\nConference on. pp. 3642–3649. IEEE (2012)\\n2. Cires¸an, D.C., Giusti, A., Gambardella, L.M., Schmidhuber, J.: Mitosis detection in\\nbreast cancer histology images with deep neural networks. In: Medical Image Com-\\nputing and Computer-Assisted Intervention–MICCAI 2013, pp. 411–418. Springer\\n(2013)\\n3. Ciresan, D.C., Meier, U., Masci, J., Maria Gambardella, L., Schmidhuber, J.: Flexible,\\nhigh performance convolutional neural networks for image classiﬁcation. In: IJCAI\\nProceedings-International Joint Conference on Artiﬁcial Intelligence. vol. 22, p. 1237\\n(2011)\\n'),\n",
       " Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 10, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Introduction to Convolutional Neural Networks\\n11\\n4. Cires¸an, D.C., Meier, U., Gambardella, L.M., Schmidhuber, J.: Convolutional neural\\nnetwork committees for handwritten character classiﬁcation. In: Document Analysis\\nand Recognition (ICDAR), 2011 International Conference on. pp. 1135–1139. IEEE\\n(2011)\\n5. Egmont-Petersen, M., de Ridder, D., Handels, H.: Image processing with neural net-\\nworksa review. Pattern recognition 35(10), 2279–2301 (2002)\\n6. Farabet, C., Martini, B., Akselrod, P., Talay, S., LeCun, Y., Culurciello, E.: Hardware\\naccelerated convolutional neural networks for synthetic vision systems. In: Circuits\\nand Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on. pp.\\n257–260. IEEE (2010)\\n7. Hinton, G.: A practical guide to training restricted boltzmann machines. Momentum\\n9(1), 926 (2010)\\n8. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Im-\\nproving neural networks by preventing co-adaptation of feature detectors. arXiv\\npreprint arXiv:1207.0580 (2012)\\n9. Ji, S., Xu, W., Yang, M., Yu, K.: 3d convolutional neural networks for human action\\nrecognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35(1),\\n221–231 (2013)\\n10. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-\\nscale video classiﬁcation with convolutional neural networks. In: Computer Vision\\nand Pattern Recognition (CVPR), 2014 IEEE Conference on. pp. 1725–1732. IEEE\\n(2014)\\n11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In: Advances in neural information processing systems.\\npp. 1097–1105 (2012)\\n12. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel,\\nL.D.: Backpropagation applied to handwritten zip code recognition. Neural compu-\\ntation 1(4), 541–551 (1989)\\n13. LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to doc-\\nument recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998)\\n14. Nebauer, C.: Evaluation of convolutional neural networks for visual recognition.\\nNeural Networks, IEEE Transactions on 9(4), 685–696 (1998)\\n15. Simard, P.Y., Steinkraus, D., Platt, J.C.: Best practices for convolutional neural net-\\nworks applied to visual document analysis. In: null. p. 958. IEEE (2003)\\n16. Srivastava, N.: Improving neural networks with dropout. Ph.D. thesis, University of\\nToronto (2013)\\n17. Szarvas, M., Yoshizawa, A., Yamamoto, M., Ogata, J.: Pedestrian detection with con-\\nvolutional neural networks. In: Intelligent Vehicles Symposium, 2005. Proceedings.\\nIEEE. pp. 224–229. IEEE (2005)\\n18. Szegedy, C., Toshev, A., Erhan, D.: Deep neural networks for object detection. In:\\nAdvances in Neural Information Processing Systems. pp. 2553–2561 (2013)\\n19. Tivive, F.H.C., Bouzerdoum, A.: A new class of convolutional neural networks\\n(siconnets) and their application of face detection. In: Neural Networks, 2003. Pro-\\nceedings of the International Joint Conference on. vol. 3, pp. 2157–2162. IEEE (2003)\\n20. Zeiler, M.D., Fergus, R.: Stochastic pooling for regularization of deep convolutional\\nneural networks. arXiv preprint arXiv:1301.3557 (2013)\\n21. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks. In:\\nComputer Vision–ECCV 2014, pp. 818–833. Springer (2014)\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 0, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='Deep Residual Learning for Image Recognition\\nKaiming He\\nXiangyu Zhang\\nShaoqing Ren\\nJian Sun\\nMicrosoft Research\\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\\nAbstract\\nDeeper neural networks are more difﬁcult to train. We\\npresent a residual learning framework to ease the training\\nof networks that are substantially deeper than those used\\npreviously. We explicitly reformulate the layers as learn-\\ning residual functions with reference to the layer inputs, in-\\nstead of learning unreferenced functions. We provide com-\\nprehensive empirical evidence showing that these residual\\nnetworks are easier to optimize, and can gain accuracy from\\nconsiderably increased depth. On the ImageNet dataset we\\nevaluate residual nets with a depth of up to 152 layers—8×\\ndeeper than VGG nets [41] but still having lower complex-\\nity. An ensemble of these residual nets achieves 3.57% error\\non the ImageNet test set. This result won the 1st place on the\\nILSVRC 2015 classiﬁcation task. We also present analysis\\non CIFAR-10 with 100 and 1000 layers.\\nThe depth of representations is of central importance\\nfor many visual recognition tasks. Solely due to our ex-\\ntremely deep representations, we obtain a 28% relative im-\\nprovement on the COCO object detection dataset. Deep\\nresidual nets are foundations of our submissions to ILSVRC\\n& COCO 2015 competitions1, where we also won the 1st\\nplaces on the tasks of ImageNet detection, ImageNet local-\\nization, COCO detection, and COCO segmentation.\\n1. Introduction\\nDeep convolutional neural networks [22, 21] have led\\nto a series of breakthroughs for image classiﬁcation [21,\\n50, 40]. Deep networks naturally integrate low/mid/high-\\nlevel features [50] and classiﬁers in an end-to-end multi-\\nlayer fashion, and the “levels” of features can be enriched\\nby the number of stacked layers (depth). Recent evidence\\n[41, 44] reveals that network depth is of crucial importance,\\nand the leading results [41, 44, 13, 16] on the challenging\\nImageNet dataset [36] all exploit “very deep” [41] models,\\nwith a depth of sixteen [41] to thirty [16]. Many other non-\\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\\n1http://image-net.org/challenges/LSVRC/2015/\\nand\\nhttp://mscoco.org/dataset/#detections-challenge2015.\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0 \\n10\\n20\\niter. (1e4)\\ntraining error (%)\\n \\n \\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n10\\n20\\niter. (1e4)\\ntest error (%)\\n \\n \\n56-layer\\n20-layer\\n56-layer\\n20-layer\\nFigure 1. Training error (left) and test error (right) on CIFAR-10\\nwith 20-layer and 56-layer “plain” networks. The deeper network\\nhas higher training error, and thus test error. Similar phenomena\\non ImageNet is presented in Fig. 4.\\ngreatly beneﬁted from very deep models.\\nDriven by the signiﬁcance of depth, a question arises: Is\\nlearning better networks as easy as stacking more layers?\\nAn obstacle to answering this question was the notorious\\nproblem of vanishing/exploding gradients [1, 9], which\\nhamper convergence from the beginning.\\nThis problem,\\nhowever, has been largely addressed by normalized initial-\\nization [23, 9, 37, 13] and intermediate normalization layers\\n[16], which enable networks with tens of layers to start con-\\nverging for stochastic gradient descent (SGD) with back-\\npropagation [22].\\nWhen deeper networks are able to start converging, a\\ndegradation problem has been exposed: with the network\\ndepth increasing, accuracy gets saturated (which might be\\nunsurprising) and then degrades rapidly.\\nUnexpectedly,\\nsuch degradation is not caused by overﬁtting, and adding\\nmore layers to a suitably deep model leads to higher train-\\ning error, as reported in [11, 42] and thoroughly veriﬁed by\\nour experiments. Fig. 1 shows a typical example.\\nThe degradation (of training accuracy) indicates that not\\nall systems are similarly easy to optimize. Let us consider a\\nshallower architecture and its deeper counterpart that adds\\nmore layers onto it. There exists a solution by construction\\nto the deeper model: the added layers are identity mapping,\\nand the other layers are copied from the learned shallower\\nmodel. The existence of this constructed solution indicates\\nthat a deeper model should produce no higher training error\\nthan its shallower counterpart. But experiments show that\\nour current solvers on hand are unable to ﬁnd solutions that\\n1\\narXiv:1512.03385v1  [cs.CV]  10 Dec 2015\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 1, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='identity\\nweight layer\\nweight layer\\nrelu\\nrelu\\nF(x)\\x01+\\x01x\\nx\\nF(x)\\nx\\nFigure 2. Residual learning: a building block.\\nare comparably good or better than the constructed solution\\n(or unable to do so in feasible time).\\nIn this paper, we address the degradation problem by\\nintroducing a deep residual learning framework.\\nIn-\\nstead of hoping each few stacked layers directly ﬁt a\\ndesired underlying mapping, we explicitly let these lay-\\ners ﬁt a residual mapping. Formally, denoting the desired\\nunderlying mapping as H(x), we let the stacked nonlinear\\nlayers ﬁt another mapping of F(x) := H(x)−x. The orig-\\ninal mapping is recast into F(x)+x. We hypothesize that it\\nis easier to optimize the residual mapping than to optimize\\nthe original, unreferenced mapping. To the extreme, if an\\nidentity mapping were optimal, it would be easier to push\\nthe residual to zero than to ﬁt an identity mapping by a stack\\nof nonlinear layers.\\nThe formulation of F(x)+x can be realized by feedfor-\\nward neural networks with “shortcut connections” (Fig. 2).\\nShortcut connections [2, 34, 49] are those skipping one or\\nmore layers. In our case, the shortcut connections simply\\nperform identity mapping, and their outputs are added to\\nthe outputs of the stacked layers (Fig. 2). Identity short-\\ncut connections add neither extra parameter nor computa-\\ntional complexity. The entire network can still be trained\\nend-to-end by SGD with backpropagation, and can be eas-\\nily implemented using common libraries (e.g., Caffe [19])\\nwithout modifying the solvers.\\nWe present comprehensive experiments on ImageNet\\n[36] to show the degradation problem and evaluate our\\nmethod. We show that: 1) Our extremely deep residual nets\\nare easy to optimize, but the counterpart “plain” nets (that\\nsimply stack layers) exhibit higher training error when the\\ndepth increases; 2) Our deep residual nets can easily enjoy\\naccuracy gains from greatly increased depth, producing re-\\nsults substantially better than previous networks.\\nSimilar phenomena are also shown on the CIFAR-10 set\\n[20], suggesting that the optimization difﬁculties and the\\neffects of our method are not just akin to a particular dataset.\\nWe present successfully trained models on this dataset with\\nover 100 layers, and explore models with over 1000 layers.\\nOn the ImageNet classiﬁcation dataset [36], we obtain\\nexcellent results by extremely deep residual nets. Our 152-\\nlayer residual net is the deepest network ever presented on\\nImageNet, while still having lower complexity than VGG\\nnets [41].\\nOur ensemble has 3.57% top-5 error on the\\nImageNet test set, and won the 1st place in the ILSVRC\\n2015 classiﬁcation competition. The extremely deep rep-\\nresentations also have excellent generalization performance\\non other recognition tasks, and lead us to further win the\\n1st places on: ImageNet detection, ImageNet localization,\\nCOCO detection, and COCO segmentation in ILSVRC &\\nCOCO 2015 competitions. This strong evidence shows that\\nthe residual learning principle is generic, and we expect that\\nit is applicable in other vision and non-vision problems.\\n2. Related Work\\nResidual Representations. In image recognition, VLAD\\n[18] is a representation that encodes by the residual vectors\\nwith respect to a dictionary, and Fisher Vector [30] can be\\nformulated as a probabilistic version [18] of VLAD. Both\\nof them are powerful shallow representations for image re-\\ntrieval and classiﬁcation [4, 48]. For vector quantization,\\nencoding residual vectors [17] is shown to be more effec-\\ntive than encoding original vectors.\\nIn low-level vision and computer graphics, for solv-\\ning Partial Differential Equations (PDEs), the widely used\\nMultigrid method [3] reformulates the system as subprob-\\nlems at multiple scales, where each subproblem is respon-\\nsible for the residual solution between a coarser and a ﬁner\\nscale. An alternative to Multigrid is hierarchical basis pre-\\nconditioning [45, 46], which relies on variables that repre-\\nsent residual vectors between two scales. It has been shown\\n[3, 45, 46] that these solvers converge much faster than stan-\\ndard solvers that are unaware of the residual nature of the\\nsolutions. These methods suggest that a good reformulation\\nor preconditioning can simplify the optimization.\\nShortcut Connections. Practices and theories that lead to\\nshortcut connections [2, 34, 49] have been studied for a long\\ntime. An early practice of training multi-layer perceptrons\\n(MLPs) is to add a linear layer connected from the network\\ninput to the output [34, 49]. In [44, 24], a few interme-\\ndiate layers are directly connected to auxiliary classiﬁers\\nfor addressing vanishing/exploding gradients. The papers\\nof [39, 38, 31, 47] propose methods for centering layer re-\\nsponses, gradients, and propagated errors, implemented by\\nshortcut connections. In [44], an “inception” layer is com-\\nposed of a shortcut branch and a few deeper branches.\\nConcurrent with our work, “highway networks” [42, 43]\\npresent shortcut connections with gating functions [15].\\nThese gates are data-dependent and have parameters, in\\ncontrast to our identity shortcuts that are parameter-free.\\nWhen a gated shortcut is “closed” (approaching zero), the\\nlayers in highway networks represent non-residual func-\\ntions.\\nOn the contrary, our formulation always learns\\nresidual functions; our identity shortcuts are never closed,\\nand all information is always passed through, with addi-\\ntional residual functions to be learned. In addition, high-\\n2\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 2, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='way networks have not demonstrated accuracy gains with\\nextremely increased depth (e.g., over 100 layers).\\n3. Deep Residual Learning\\n3.1. Residual Learning\\nLet us consider H(x) as an underlying mapping to be\\nﬁt by a few stacked layers (not necessarily the entire net),\\nwith x denoting the inputs to the ﬁrst of these layers. If one\\nhypothesizes that multiple nonlinear layers can asymptoti-\\ncally approximate complicated functions2, then it is equiv-\\nalent to hypothesize that they can asymptotically approxi-\\nmate the residual functions, i.e., H(x) −x (assuming that\\nthe input and output are of the same dimensions).\\nSo\\nrather than expect stacked layers to approximate H(x), we\\nexplicitly let these layers approximate a residual function\\nF(x) := H(x) −x. The original function thus becomes\\nF(x)+x. Although both forms should be able to asymptot-\\nically approximate the desired functions (as hypothesized),\\nthe ease of learning might be different.\\nThis reformulation is motivated by the counterintuitive\\nphenomena about the degradation problem (Fig. 1, left). As\\nwe discussed in the introduction, if the added layers can\\nbe constructed as identity mappings, a deeper model should\\nhave training error no greater than its shallower counter-\\npart.\\nThe degradation problem suggests that the solvers\\nmight have difﬁculties in approximating identity mappings\\nby multiple nonlinear layers. With the residual learning re-\\nformulation, if identity mappings are optimal, the solvers\\nmay simply drive the weights of the multiple nonlinear lay-\\ners toward zero to approach identity mappings.\\nIn real cases, it is unlikely that identity mappings are op-\\ntimal, but our reformulation may help to precondition the\\nproblem. If the optimal function is closer to an identity\\nmapping than to a zero mapping, it should be easier for the\\nsolver to ﬁnd the perturbations with reference to an identity\\nmapping, than to learn the function as a new one. We show\\nby experiments (Fig. 7) that the learned residual functions in\\ngeneral have small responses, suggesting that identity map-\\npings provide reasonable preconditioning.\\n3.2. Identity Mapping by Shortcuts\\nWe adopt residual learning to every few stacked layers.\\nA building block is shown in Fig. 2. Formally, in this paper\\nwe consider a building block deﬁned as:\\ny = F(x, {Wi}) + x.\\n(1)\\nHere x and y are the input and output vectors of the lay-\\ners considered.\\nThe function F(x, {Wi}) represents the\\nresidual mapping to be learned. For the example in Fig. 2\\nthat has two layers, F = W2σ(W1x) in which σ denotes\\n2This hypothesis, however, is still an open question. See [28].\\nReLU [29] and the biases are omitted for simplifying no-\\ntations. The operation F + x is performed by a shortcut\\nconnection and element-wise addition. We adopt the sec-\\nond nonlinearity after the addition (i.e., σ(y), see Fig. 2).\\nThe shortcut connections in Eqn.(1) introduce neither ex-\\ntra parameter nor computation complexity. This is not only\\nattractive in practice but also important in our comparisons\\nbetween plain and residual networks. We can fairly com-\\npare plain/residual networks that simultaneously have the\\nsame number of parameters, depth, width, and computa-\\ntional cost (except for the negligible element-wise addition).\\nThe dimensions of x and F must be equal in Eqn.(1).\\nIf this is not the case (e.g., when changing the input/output\\nchannels), we can perform a linear projection Ws by the\\nshortcut connections to match the dimensions:\\ny = F(x, {Wi}) + Wsx.\\n(2)\\nWe can also use a square matrix Ws in Eqn.(1). But we will\\nshow by experiments that the identity mapping is sufﬁcient\\nfor addressing the degradation problem and is economical,\\nand thus Ws is only used when matching dimensions.\\nThe form of the residual function F is ﬂexible. Exper-\\niments in this paper involve a function F that has two or\\nthree layers (Fig. 5), while more layers are possible. But if\\nF has only a single layer, Eqn.(1) is similar to a linear layer:\\ny = W1x + x, for which we have not observed advantages.\\nWe also note that although the above notations are about\\nfully-connected layers for simplicity, they are applicable to\\nconvolutional layers. The function F(x, {Wi}) can repre-\\nsent multiple convolutional layers. The element-wise addi-\\ntion is performed on two feature maps, channel by channel.\\n3.3. Network Architectures\\nWe have tested various plain/residual nets, and have ob-\\nserved consistent phenomena. To provide instances for dis-\\ncussion, we describe two models for ImageNet as follows.\\nPlain Network. Our plain baselines (Fig. 3, middle) are\\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\\nleft). The convolutional layers mostly have 3×3 ﬁlters and\\nfollow two simple design rules: (i) for the same output\\nfeature map size, the layers have the same number of ﬁl-\\nters; and (ii) if the feature map size is halved, the num-\\nber of ﬁlters is doubled so as to preserve the time com-\\nplexity per layer. We perform downsampling directly by\\nconvolutional layers that have a stride of 2. The network\\nends with a global average pooling layer and a 1000-way\\nfully-connected layer with softmax. The total number of\\nweighted layers is 34 in Fig. 3 (middle).\\nIt is worth noticing that our model has fewer ﬁlters and\\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\\nis only 18% of VGG-19 (19.6 billion FLOPs).\\n3\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 3, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='7x7 conv, 64, /2\\npool, /2\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 128, /2\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 256, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 512, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\navg pool\\nfc 1000\\nimage\\n3x3 conv, 512\\n3x3 conv, 64\\n3x3 conv, 64\\npool, /2\\n3x3 conv, 128\\n3x3 conv, 128\\npool, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\npool, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\npool, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\npool, /2\\nfc 4096\\nfc 4096\\nfc 1000\\nimage\\noutput \\nsize: 112\\noutput \\nsize: 224\\noutput \\nsize: 56\\noutput \\nsize: 28\\noutput \\nsize: 14\\noutput \\nsize: 7\\noutput \\nsize: 1\\nVGG-19\\n34-layer plain\\n7x7 conv, 64, /2\\npool, /2\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 64\\n3x3 conv, 128, /2\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 128\\n3x3 conv, 256, /2\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 256\\n3x3 conv, 512, /2\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\n3x3 conv, 512\\navg pool\\nfc 1000\\nimage\\n34-layer residual\\nFigure 3. Example network architectures for ImageNet. Left: the\\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\\nRight: a residual network with 34 parameter layers (3.6 billion\\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\\nmore details and other variants.\\nResidual Network. Based on the above plain network, we\\ninsert shortcut connections (Fig. 3, right) which turn the\\nnetwork into its counterpart residual version. The identity\\nshortcuts (Eqn.(1)) can be directly used when the input and\\noutput are of the same dimensions (solid line shortcuts in\\nFig. 3). When the dimensions increase (dotted line shortcuts\\nin Fig. 3), we consider two options: (A) The shortcut still\\nperforms identity mapping, with extra zero entries padded\\nfor increasing dimensions. This option introduces no extra\\nparameter; (B) The projection shortcut in Eqn.(2) is used to\\nmatch dimensions (done by 1×1 convolutions). For both\\noptions, when the shortcuts go across feature maps of two\\nsizes, they are performed with a stride of 2.\\n3.4. Implementation\\nOur implementation for ImageNet follows the practice\\nin [21, 41]. The image is resized with its shorter side ran-\\ndomly sampled in [256, 480] for scale augmentation [41].\\nA 224×224 crop is randomly sampled from an image or its\\nhorizontal ﬂip, with the per-pixel mean subtracted [21]. The\\nstandard color augmentation in [21] is used. We adopt batch\\nnormalization (BN) [16] right after each convolution and\\nbefore activation, following [16]. We initialize the weights\\nas in [13] and train all plain/residual nets from scratch. We\\nuse SGD with a mini-batch size of 256. The learning rate\\nstarts from 0.1 and is divided by 10 when the error plateaus,\\nand the models are trained for up to 60 × 104 iterations. We\\nuse a weight decay of 0.0001 and a momentum of 0.9. We\\ndo not use dropout [14], following the practice in [16].\\nIn testing, for comparison studies we adopt the standard\\n10-crop testing [21]. For best results, we adopt the fully-\\nconvolutional form as in [41, 13], and average the scores\\nat multiple scales (images are resized such that the shorter\\nside is in {224, 256, 384, 480, 640}).\\n4. Experiments\\n4.1. ImageNet Classiﬁcation\\nWe evaluate our method on the ImageNet 2012 classiﬁ-\\ncation dataset [36] that consists of 1000 classes. The models\\nare trained on the 1.28 million training images, and evalu-\\nated on the 50k validation images. We also obtain a ﬁnal\\nresult on the 100k test images, reported by the test server.\\nWe evaluate both top-1 and top-5 error rates.\\nPlain Networks. We ﬁrst evaluate 18-layer and 34-layer\\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\\n18-layer plain net is of a similar form. See Table 1 for de-\\ntailed architectures.\\nThe results in Table 2 show that the deeper 34-layer plain\\nnet has higher validation error than the shallower 18-layer\\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\\npare their training/validation errors during the training pro-\\ncedure. We have observed the degradation problem - the\\n4\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 4, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='layer name output size\\n18-layer\\n34-layer\\n50-layer\\n101-layer\\n152-layer\\nconv1\\n112×112\\n7×7, 64, stride 2\\nconv2 x\\n56×56\\n3×3 max pool, stride 2\\n\\x14\\n3×3, 64\\n3×3, 64\\n\\x15\\n×2\\n\\x14\\n3×3, 64\\n3×3, 64\\n\\x15\\n×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 64\\n3×3, 64\\n1×1, 256\\n\\uf8f9\\n\\uf8fb×3\\nconv3 x\\n28×28\\n\\x14\\n3×3, 128\\n3×3, 128\\n\\x15\\n×2\\n\\x14\\n3×3, 128\\n3×3, 128\\n\\x15\\n×4\\n\\uf8ee\\n\\uf8f0\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n\\uf8f9\\n\\uf8fb×4\\n\\uf8ee\\n\\uf8f0\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n\\uf8f9\\n\\uf8fb×4\\n\\uf8ee\\n\\uf8f0\\n1×1, 128\\n3×3, 128\\n1×1, 512\\n\\uf8f9\\n\\uf8fb×8\\nconv4 x\\n14×14\\n\\x14\\n3×3, 256\\n3×3, 256\\n\\x15\\n×2\\n\\x14\\n3×3, 256\\n3×3, 256\\n\\x15\\n×6\\n\\uf8ee\\n\\uf8f0\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n\\uf8f9\\n\\uf8fb×6\\n\\uf8ee\\n\\uf8f0\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n\\uf8f9\\n\\uf8fb×23\\n\\uf8ee\\n\\uf8f0\\n1×1, 256\\n3×3, 256\\n1×1, 1024\\n\\uf8f9\\n\\uf8fb×36\\nconv5 x\\n7×7\\n\\x14\\n3×3, 512\\n3×3, 512\\n\\x15\\n×2\\n\\x14\\n3×3, 512\\n3×3, 512\\n\\x15\\n×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n\\uf8f9\\n\\uf8fb×3\\n\\uf8ee\\n\\uf8f0\\n1×1, 512\\n3×3, 512\\n1×1, 2048\\n\\uf8f9\\n\\uf8fb×3\\n1×1\\naverage pool, 1000-d fc, softmax\\nFLOPs\\n1.8×109\\n3.6×109\\n3.8×109\\n7.6×109\\n11.3×109\\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\\n0\\n10\\n20\\n30\\n40\\n50\\n20\\n30\\n40\\n50\\n60\\niter. (1e4)\\nerror (%)\\n \\n \\nplain-18\\nplain-34\\n0\\n10\\n20\\n30\\n40\\n50\\n20\\n30\\n40\\n50\\n60\\niter. (1e4)\\nerror (%)\\n \\n \\nResNet-18\\nResNet-34\\n18-layer\\n34-layer\\n18-layer\\n34-layer\\nFigure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\\ntheir plain counterparts.\\nplain\\nResNet\\n18 layers\\n27.94\\n27.88\\n34 layers\\n28.54\\n25.03\\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\\nHere the ResNets have no extra parameter compared to their plain\\ncounterparts. Fig. 4 shows the training procedures.\\n34-layer plain net has higher training error throughout the\\nwhole training procedure, even though the solution space\\nof the 18-layer plain network is a subspace of that of the\\n34-layer one.\\nWe argue that this optimization difﬁculty is unlikely to\\nbe caused by vanishing gradients. These plain networks are\\ntrained with BN [16], which ensures forward propagated\\nsignals to have non-zero variances. We also verify that the\\nbackward propagated gradients exhibit healthy norms with\\nBN. So neither forward nor backward signals vanish. In\\nfact, the 34-layer plain net is still able to achieve compet-\\nitive accuracy (Table 3), suggesting that the solver works\\nto some extent. We conjecture that the deep plain nets may\\nhave exponentially low convergence rates, which impact the\\nreducing of the training error3. The reason for such opti-\\nmization difﬁculties will be studied in the future.\\nResidual Networks. Next we evaluate 18-layer and 34-\\nlayer residual nets (ResNets). The baseline architectures\\nare the same as the above plain nets, expect that a shortcut\\nconnection is added to each pair of 3×3 ﬁlters as in Fig. 3\\n(right). In the ﬁrst comparison (Table 2 and Fig. 4 right),\\nwe use identity mapping for all shortcuts and zero-padding\\nfor increasing dimensions (option A). So they have no extra\\nparameter compared to the plain counterparts.\\nWe have three major observations from Table 2 and\\nFig. 4. First, the situation is reversed with residual learn-\\ning – the 34-layer ResNet is better than the 18-layer ResNet\\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\\nconsiderably lower training error and is generalizable to the\\nvalidation data. This indicates that the degradation problem\\nis well addressed in this setting and we manage to obtain\\naccuracy gains from increased depth.\\nSecond, compared to its plain counterpart, the 34-layer\\n3We have experimented with more training iterations (3×) and still ob-\\nserved the degradation problem, suggesting that this problem cannot be\\nfeasibly addressed by simply using more iterations.\\n5\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 5, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='model\\ntop-1 err.\\ntop-5 err.\\nVGG-16 [41]\\n28.07\\n9.33\\nGoogLeNet [44]\\n-\\n9.15\\nPReLU-net [13]\\n24.27\\n7.38\\nplain-34\\n28.54\\n10.02\\nResNet-34 A\\n25.03\\n7.76\\nResNet-34 B\\n24.52\\n7.46\\nResNet-34 C\\n24.19\\n7.40\\nResNet-50\\n22.85\\n6.71\\nResNet-101\\n21.75\\n6.05\\nResNet-152\\n21.43\\n5.71\\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\\nthat only uses projections for increasing dimensions.\\nmethod\\ntop-1 err.\\ntop-5 err.\\nVGG [41] (ILSVRC’14)\\n-\\n8.43†\\nGoogLeNet [44] (ILSVRC’14)\\n-\\n7.89\\nVGG [41] (v5)\\n24.4\\n7.1\\nPReLU-net [13]\\n21.59\\n5.71\\nBN-inception [16]\\n21.99\\n5.81\\nResNet-34 B\\n21.84\\n5.71\\nResNet-34 C\\n21.53\\n5.60\\nResNet-50\\n20.74\\n5.25\\nResNet-101\\n19.87\\n4.60\\nResNet-152\\n19.38\\n4.49\\nTable 4. Error rates (%) of single-model results on the ImageNet\\nvalidation set (except † reported on the test set).\\nmethod\\ntop-5 err. (test)\\nVGG [41] (ILSVRC’14)\\n7.32\\nGoogLeNet [44] (ILSVRC’14)\\n6.66\\nVGG [41] (v5)\\n6.8\\nPReLU-net [13]\\n4.94\\nBN-inception [16]\\n4.82\\nResNet (ILSVRC’15)\\n3.57\\nTable 5. Error rates (%) of ensembles. The top-5 error is on the\\ntest set of ImageNet and reported by the test server.\\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\\nfrom the successfully reduced training error (Fig. 4 right vs.\\nleft). This comparison veriﬁes the effectiveness of residual\\nlearning on extremely deep systems.\\nLast, we also note that the 18-layer plain/residual nets\\nare comparably accurate (Table 2), but the 18-layer ResNet\\nconverges faster (Fig. 4 right vs. left). When the net is “not\\noverly deep” (18 layers here), the current SGD solver is still\\nable to ﬁnd good solutions to the plain net. In this case, the\\nResNet eases the optimization by providing faster conver-\\ngence at the early stage.\\nIdentity vs. Projection Shortcuts. We have shown that\\n3x3, 64\\n1x1, 64\\nrelu\\n1x1, 256\\nrelu\\nrelu\\n3x3, 64\\n3x3, 64\\nrelu\\nrelu\\n64-d\\n256-d\\nFigure 5. A deeper residual function F for ImageNet. Left: a\\nbuilding block (on 56×56 feature maps) as in Fig. 3 for ResNet-\\n34. Right: a “bottleneck” building block for ResNet-50/101/152.\\nparameter-free, identity shortcuts help with training. Next\\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\\ncompare three options: (A) zero-padding shortcuts are used\\nfor increasing dimensions, and all shortcuts are parameter-\\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\\ntion shortcuts are used for increasing dimensions, and other\\nshortcuts are identity; and (C) all shortcuts are projections.\\nTable 3 shows that all three options are considerably bet-\\nter than the plain counterpart. B is slightly better than A. We\\nargue that this is because the zero-padded dimensions in A\\nindeed have no residual learning. C is marginally better than\\nB, and we attribute this to the extra parameters introduced\\nby many (thirteen) projection shortcuts. But the small dif-\\nferences among A/B/C indicate that projection shortcuts are\\nnot essential for addressing the degradation problem. So we\\ndo not use option C in the rest of this paper, to reduce mem-\\nory/time complexity and model sizes. Identity shortcuts are\\nparticularly important for not increasing the complexity of\\nthe bottleneck architectures that are introduced below.\\nDeeper Bottleneck Architectures. Next we describe our\\ndeeper nets for ImageNet. Because of concerns on the train-\\ning time that we can afford, we modify the building block\\nas a bottleneck design4. For each residual function F, we\\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\\nare 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers\\nare responsible for reducing and then increasing (restoring)\\ndimensions, leaving the 3×3 layer a bottleneck with smaller\\ninput/output dimensions. Fig. 5 shows an example, where\\nboth designs have similar time complexity.\\nThe parameter-free identity shortcuts are particularly im-\\nportant for the bottleneck architectures. If the identity short-\\ncut in Fig. 5 (right) is replaced with projection, one can\\nshow that the time complexity and model size are doubled,\\nas the shortcut is connected to the two high-dimensional\\nends. So identity shortcuts lead to more efﬁcient models\\nfor the bottleneck designs.\\n50-layer ResNet: We replace each 2-layer block in the\\n4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy\\nfrom increased depth (as shown on CIFAR-10), but are not as economical\\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\\nto practical considerations. We further note that the degradation problem\\nof plain nets is also witnessed for the bottleneck designs.\\n6\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 6, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='34-layer net with this 3-layer bottleneck block, resulting in\\na 50-layer ResNet (Table 1). We use option B for increasing\\ndimensions. This model has 3.8 billion FLOPs.\\n101-layer and 152-layer ResNets: We construct 101-\\nlayer and 152-layer ResNets by using more 3-layer blocks\\n(Table 1). Remarkably, although the depth is signiﬁcantly\\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\\nhas lower complexity than VGG-16/19 nets (15.3/19.6 bil-\\nlion FLOPs).\\nThe 50/101/152-layer ResNets are more accurate than\\nthe 34-layer ones by considerable margins (Table 3 and 4).\\nWe do not observe the degradation problem and thus en-\\njoy signiﬁcant accuracy gains from considerably increased\\ndepth. The beneﬁts of depth are witnessed for all evaluation\\nmetrics (Table 3 and 4).\\nComparisons with State-of-the-art Methods. In Table 4\\nwe compare with the previous best single-model results.\\nOur baseline 34-layer ResNets have achieved very compet-\\nitive accuracy. Our 152-layer ResNet has a single-model\\ntop-5 validation error of 4.49%. This single-model result\\noutperforms all previous ensemble results (Table 5). We\\ncombine six models of different depth to form an ensemble\\n(only with two 152-layer ones at the time of submitting).\\nThis leads to 3.57% top-5 error on the test set (Table 5).\\nThis entry won the 1st place in ILSVRC 2015.\\n4.2. CIFAR-10 and Analysis\\nWe conducted more studies on the CIFAR-10 dataset\\n[20], which consists of 50k training images and 10k test-\\ning images in 10 classes. We present experiments trained\\non the training set and evaluated on the test set. Our focus\\nis on the behaviors of extremely deep networks, but not on\\npushing the state-of-the-art results, so we intentionally use\\nsimple architectures as follows.\\nThe plain/residual architectures follow the form in Fig. 3\\n(middle/right). The network inputs are 32×32 images, with\\nthe per-pixel mean subtracted. The ﬁrst layer is 3×3 convo-\\nlutions. Then we use a stack of 6n layers with 3×3 convo-\\nlutions on the feature maps of sizes {32, 16, 8} respectively,\\nwith 2n layers for each feature map size. The numbers of\\nﬁlters are {16, 32, 64} respectively. The subsampling is per-\\nformed by convolutions with a stride of 2. The network ends\\nwith a global average pooling, a 10-way fully-connected\\nlayer, and softmax. There are totally 6n+2 stacked weighted\\nlayers. The following table summarizes the architecture:\\noutput map size\\n32×32\\n16×16\\n8×8\\n# layers\\n1+2n\\n2n\\n2n\\n# ﬁlters\\n16\\n32\\n64\\nWhen shortcut connections are used, they are connected\\nto the pairs of 3×3 layers (totally 3n shortcuts). On this\\ndataset we use identity shortcuts in all cases (i.e., option A),\\nmethod\\nerror (%)\\nMaxout [10]\\n9.38\\nNIN [25]\\n8.81\\nDSN [24]\\n8.22\\n# layers\\n# params\\nFitNet [35]\\n19\\n2.5M\\n8.39\\nHighway [42, 43]\\n19\\n2.3M\\n7.54 (7.72±0.16)\\nHighway [42, 43]\\n32\\n1.25M\\n8.80\\nResNet\\n20\\n0.27M\\n8.75\\nResNet\\n32\\n0.46M\\n7.51\\nResNet\\n44\\n0.66M\\n7.17\\nResNet\\n56\\n0.85M\\n6.97\\nResNet\\n110\\n1.7M\\n6.43 (6.61±0.16)\\nResNet\\n1202\\n19.4M\\n7.93\\nTable 6. Classiﬁcation error on the CIFAR-10 test set. All meth-\\nods are with data augmentation. For ResNet-110, we run it 5 times\\nand show “best (mean±std)” as in [43].\\nso our residual models have exactly the same depth, width,\\nand number of parameters as the plain counterparts.\\nWe use a weight decay of 0.0001 and momentum of 0.9,\\nand adopt the weight initialization in [13] and BN [16] but\\nwith no dropout. These models are trained with a mini-\\nbatch size of 128 on two GPUs. We start with a learning\\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\\nterminate training at 64k iterations, which is determined on\\na 45k/5k train/val split. We follow the simple data augmen-\\ntation in [24] for training: 4 pixels are padded on each side,\\nand a 32×32 crop is randomly sampled from the padded\\nimage or its horizontal ﬂip. For testing, we only evaluate\\nthe single view of the original 32×32 image.\\nWe compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and\\n56-layer networks. Fig. 6 (left) shows the behaviors of the\\nplain nets. The deep plain nets suffer from increased depth,\\nand exhibit higher training error when going deeper. This\\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\\non MNIST (see [42]), suggesting that such an optimization\\ndifﬁculty is a fundamental problem.\\nFig. 6 (middle) shows the behaviors of ResNets. Also\\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\\nmanage to overcome the optimization difﬁculty and demon-\\nstrate accuracy gains when the depth increases.\\nWe further explore n = 18 that leads to a 110-layer\\nResNet. In this case, we ﬁnd that the initial learning rate\\nof 0.1 is slightly too large to start converging5. So we use\\n0.01 to warm up the training until the training error is below\\n80% (about 400 iterations), and then go back to 0.1 and con-\\ntinue training. The rest of the learning schedule is as done\\npreviously. This 110-layer network converges well (Fig. 6,\\nmiddle). It has fewer parameters than other deep and thin\\n5With an initial learning rate of 0.1, it starts converging (<90% error)\\nafter several epochs, but still reaches similar accuracy.\\n7\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 7, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\n \\n \\nplain-20\\nplain-32\\nplain-44\\nplain-56\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n0\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\n \\n \\nResNet-20\\nResNet-32\\nResNet-44\\nResNet-56\\nResNet-110\\n56-layer\\n20-layer\\n110-layer\\n20-layer\\n4\\n5\\n6\\n0\\n1\\n5\\n10\\n20\\niter. (1e4)\\nerror (%)\\n \\n \\nresidual-110\\nresidual-1202\\nFigure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\\nof plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.\\n0\\n20\\n40\\n60\\n80\\n100\\n1\\n2\\n3\\nlayer index (sorted by magnitude)\\nstd\\n \\n \\nplain-20\\nplain-56\\nResNet-20\\nResNet-56\\nResNet-110\\n0\\n20\\n40\\n60\\n80\\n100\\n1\\n2\\n3\\nlayer index (original)\\nstd\\n \\n \\nplain-20\\nplain-56\\nResNet-20\\nResNet-56\\nResNet-110\\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\\n10. The responses are the outputs of each 3×3 layer, after BN and\\nbefore nonlinearity. Top: the layers are shown in their original\\norder. Bottom: the responses are ranked in descending order.\\nnetworks such as FitNet [35] and Highway [42] (Table 6),\\nyet is among the state-of-the-art results (6.43%, Table 6).\\nAnalysis of Layer Responses. Fig. 7 shows the standard\\ndeviations (std) of the layer responses. The responses are\\nthe outputs of each 3×3 layer, after BN and before other\\nnonlinearity (ReLU/addition).\\nFor ResNets, this analy-\\nsis reveals the response strength of the residual functions.\\nFig. 7 shows that ResNets have generally smaller responses\\nthan their plain counterparts. These results support our ba-\\nsic motivation (Sec.3.1) that the residual functions might\\nbe generally closer to zero than the non-residual functions.\\nWe also notice that the deeper ResNet has smaller magni-\\ntudes of responses, as evidenced by the comparisons among\\nResNet-20, 56, and 110 in Fig. 7. When there are more\\nlayers, an individual layer of ResNets tends to modify the\\nsignal less.\\nExploring Over 1000 layers. We explore an aggressively\\ndeep model of over 1000 layers.\\nWe set n = 200 that\\nleads to a 1202-layer network, which is trained as described\\nabove. Our method shows no optimization difﬁculty, and\\nthis 103-layer network is able to achieve training error\\n<0.1% (Fig. 6, right).\\nIts test error is still fairly good\\n(7.93%, Table 6).\\nBut there are still open problems on such aggressively\\ndeep models. The testing result of this 1202-layer network\\nis worse than that of our 110-layer network, although both\\ntraining data\\n07+12\\n07++12\\ntest data\\nVOC 07 test\\nVOC 12 test\\nVGG-16\\n73.2\\n70.4\\nResNet-101\\n76.4\\n73.8\\nTable 7. Object detection mAP (%) on the PASCAL VOC\\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\\nble 10 and 11 for better results.\\nmetric\\nmAP@.5\\nmAP@[.5, .95]\\nVGG-16\\n41.5\\n21.2\\nResNet-101\\n48.4\\n27.2\\nTable 8. Object detection mAP (%) on the COCO validation set\\nusing baseline Faster R-CNN. See also Table 9 for better results.\\nhave similar training error. We argue that this is because of\\noverﬁtting. The 1202-layer network may be unnecessarily\\nlarge (19.4M) for this small dataset. Strong regularization\\nsuch as maxout [10] or dropout [14] is applied to obtain the\\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\\nwe use no maxout/dropout and just simply impose regular-\\nization via deep and thin architectures by design, without\\ndistracting from the focus on the difﬁculties of optimiza-\\ntion. But combining with stronger regularization may im-\\nprove results, which we will study in the future.\\n4.3. Object Detection on PASCAL and MS COCO\\nOur method has good generalization performance on\\nother recognition tasks. Table 7 and 8 show the object de-\\ntection baseline results on PASCAL VOC 2007 and 2012\\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\\ntection method. Here we are interested in the improvements\\nof replacing VGG-16 [41] with ResNet-101. The detection\\nimplementation (see appendix) of using both models is the\\nsame, so the gains can only be attributed to better networks.\\nMost remarkably, on the challenging COCO dataset we ob-\\ntain a 6.0% increase in COCO’s standard metric (mAP@[.5,\\n.95]), which is a 28% relative improvement. This gain is\\nsolely due to the learned representations.\\nBased on deep residual nets, we won the 1st places in\\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\\nageNet detection, ImageNet localization, COCO detection,\\nand COCO segmentation. The details are in the appendix.\\n8\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 8, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='References\\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\\ncies with gradient descent is difﬁcult. IEEE Transactions on Neural\\nNetworks, 5(2):157–166, 1994.\\n[2] C. M. Bishop.\\nNeural networks for pattern recognition.\\nOxford\\nuniversity press, 1995.\\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\\n2000.\\n[4] K. Chatﬁeld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\\nis in the details: an evaluation of recent feature encoding methods.\\nIn BMVC, 2011.\\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\\npages 303–338, 2010.\\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\\nsemantic segmentation-aware cnn model. In ICCV, 2015.\\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015.\\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\\narchies for accurate object detection and semantic segmentation. In\\nCVPR, 2014.\\n[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training\\ndeep feedforward neural networks. In AISTATS, 2010.\\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.\\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\\ncost. In CVPR, 2015.\\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\\nconvolutional networks for visual recognition. In ECCV, 2014.\\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:\\nSurpassing human-level performance on imagenet classiﬁcation. In\\nICCV, 2015.\\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\\nR. R. Salakhutdinov. Improving neural networks by preventing co-\\nadaptation of feature detectors. arXiv:1207.0580, 2012.\\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\\ncomputation, 9(8):1735–1780, 1997.\\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. In ICML, 2015.\\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\\nneighbor search. TPAMI, 33, 2011.\\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into compact codes.\\nTPAMI, 2012.\\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\\nfast feature embedding. arXiv:1408.5093, 2014.\\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\\nages. Tech Report, 2009.\\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation\\nwith deep convolutional neural networks. In NIPS, 2012.\\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\\nwritten zip code recognition. Neural computation, 1989.\\n[23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop.\\nIn Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.\\n[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.\\nDeeply-\\nsupervised nets. arXiv:1409.5185, 2014.\\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400,\\n2013.\\n[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\\nP. Doll´ar, and C. L. Zitnick. Microsoft COCO: Common objects in\\ncontext. In ECCV. 2014.\\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\\nfor semantic segmentation. In CVPR, 2015.\\n[28] G. Mont´ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\\nlinear regions of deep neural networks. In NIPS, 2014.\\n[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\\nboltzmann machines. In ICML, 2010.\\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\\nimage categorization. In CVPR, 2007.\\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\\nlinear transformations in perceptrons. In AISTATS, 2012.\\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\\nreal-time object detection with region proposal networks. In NIPS,\\n2015.\\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\\nnetworks on convolutional feature maps. arXiv:1504.06066, 2015.\\n[34] B. D. Ripley. Pattern recognition and neural networks. Cambridge\\nuniversity press, 1996.\\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\\nY. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.\\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\\nlarge scale visual recognition challenge. arXiv:1409.0575, 2014.\\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\\nthe nonlinear dynamics of learning in deep linear neural networks.\\narXiv:1312.6120, 2013.\\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\\ndecomposition. Technical report, 1998.\\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\\nNeural Networks: Tricks of the Trade, pages 207–226. Springer,\\n1998.\\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\\nCun.\\nOverfeat: Integrated recognition, localization and detection\\nusing convolutional networks. In ICLR, 2014.\\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\\nfor large-scale image recognition. In ICLR, 2015.\\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\\narXiv:1505.00387, 2015.\\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\\nnetworks. 1507.06228, 2015.\\n[44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\\nhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu-\\ntions. In CVPR, 2015.\\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\\ntions. TPAMI, 1990.\\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\\nSIGGRAPH, 2006.\\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas-\\ntic gradient towards second-order methods–backpropagation learn-\\ning with transformations in nonlinearities.\\nIn Neural Information\\nProcessing, 2013.\\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\\nof computer vision algorithms, 2008.\\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\\n1999.\\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\\ntional neural networks. In ECCV, 2014.\\n9\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 9, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='A. Object Detection Baselines\\nIn this section we introduce our detection method based\\non the baseline Faster R-CNN [32] system. The models are\\ninitialized by the ImageNet classiﬁcation models, and then\\nﬁne-tuned on the object detection data. We have experi-\\nmented with ResNet-50/101 at the time of the ILSVRC &\\nCOCO 2015 detection competitions.\\nUnlike VGG-16 used in [32], our ResNet has no hidden\\nfc layers. We adopt the idea of “Networks on Conv fea-\\nture maps” (NoC) [33] to address this issue. We compute\\nthe full-image shared conv feature maps using those lay-\\ners whose strides on the image are no greater than 16 pixels\\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\\nlayers in ResNet-101; Table 1). We consider these layers as\\nanalogous to the 13 conv layers in VGG-16, and by doing\\nso, both ResNet and VGG-16 have conv feature maps of the\\nsame total stride (16 pixels). These layers are shared by a\\nregion proposal network (RPN, generating 300 proposals)\\n[32] and a Fast R-CNN detection network [7]. RoI pool-\\ning [7] is performed before conv5 1. On this RoI-pooled\\nfeature, all layers of conv5 x and up are adopted for each\\nregion, playing the roles of VGG-16’s fc layers. The ﬁnal\\nclassiﬁcation layer is replaced by two sibling layers (classi-\\nﬁcation and box regression [7]).\\nFor the usage of BN layers, after pre-training, we com-\\npute the BN statistics (means and variances) for each layer\\non the ImageNet training set. Then the BN layers are ﬁxed\\nduring ﬁne-tuning for object detection. As such, the BN\\nlayers become linear activations with constant offsets and\\nscales, and BN statistics are not updated by ﬁne-tuning. We\\nﬁx the BN layers mainly for reducing memory consumption\\nin Faster R-CNN training.\\nPASCAL VOC\\nFollowing [7, 32], for the PASCAL VOC 2007 test set,\\nwe use the 5k trainval images in VOC 2007 and 16k train-\\nval images in VOC 2012 for training (“07+12”). For the\\nPASCAL VOC 2012 test set, we use the 10k trainval+test\\nimages in VOC 2007 and 16k trainval images in VOC 2012\\nfor training (“07++12”). The hyper-parameters for train-\\ning Faster R-CNN are the same as in [32]. Table 7 shows\\nthe results. ResNet-101 improves the mAP by >3% over\\nVGG-16. This gain is solely because of the improved fea-\\ntures learned by ResNet.\\nMS COCO\\nThe MS COCO dataset [26] involves 80 object cate-\\ngories. We evaluate the PASCAL VOC metric (mAP @\\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\\n.5:.05:.95). We use the 80k images on the train set for train-\\ning and the 40k images on the val set for evaluation. Our\\ndetection system for COCO is similar to that for PASCAL\\nVOC. We train the COCO models with an 8-GPU imple-\\nmentation, and thus the RPN step has a mini-batch size of\\n8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\\nmini-batch size of 16 images. The RPN step and Fast R-\\nCNN step are both trained for 240k iterations with a learn-\\ning rate of 0.001 and then for 80k iterations with 0.0001.\\nTable 8 shows the results on the MS COCO validation\\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\\nVGG-16, which is a 28% relative improvement, solely con-\\ntributed by the features learned by the better network. Re-\\nmarkably, the mAP@[.5, .95]’s absolute increase (6.0%) is\\nnearly as big as mAP@.5’s (6.9%). This suggests that a\\ndeeper network can improve both recognition and localiza-\\ntion.\\nB. Object Detection Improvements\\nFor completeness, we report the improvements made for\\nthe competitions. These improvements are based on deep\\nfeatures and thus should beneﬁt from residual learning.\\nMS COCO\\nBox reﬁnement. Our box reﬁnement partially follows the it-\\nerative localization in [6]. In Faster R-CNN, the ﬁnal output\\nis a regressed box that is different from its proposal box. So\\nfor inference, we pool a new feature from the regressed box\\nand obtain a new classiﬁcation score and a new regressed\\nbox. We combine these 300 new predictions with the orig-\\ninal 300 predictions. Non-maximum suppression (NMS) is\\napplied on the union set of predicted boxes using an IoU\\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\\nﬁnement improves mAP by about 2 points (Table 9).\\nGlobal context.\\nWe combine global context in the Fast\\nR-CNN step. Given the full-image conv feature map, we\\npool a feature by global Spatial Pyramid Pooling [12] (with\\na “single-level” pyramid) which can be implemented as\\n“RoI” pooling using the entire image’s bounding box as the\\nRoI. This pooled feature is fed into the post-RoI layers to\\nobtain a global context feature. This global feature is con-\\ncatenated with the original per-region feature, followed by\\nthe sibling classiﬁcation and box regression layers. This\\nnew structure is trained end-to-end.\\nGlobal context im-\\nproves mAP@.5 by about 1 point (Table 9).\\nMulti-scale testing. In the above, all results are obtained by\\nsingle-scale training/testing as in [32], where the image’s\\nshorter side is s = 600 pixels. Multi-scale training/testing\\nhas been developed in [12, 7] by selecting a scale from a\\nfeature pyramid, and in [33] by using maxout layers. In\\nour current implementation, we have performed multi-scale\\ntesting following [33]; we have not performed multi-scale\\ntraining because of limited time. In addition, we have per-\\nformed multi-scale testing only for the Fast R-CNN step\\n(but not yet for the RPN step). With a trained model, we\\ncompute conv feature maps on an image pyramid, where the\\nimage’s shorter sides are s ∈{200, 400, 600, 800, 1000}.\\n10\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 10, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='training data\\nCOCO train\\nCOCO trainval\\ntest data\\nCOCO val\\nCOCO test-dev\\nmAP\\n@.5\\n@[.5, .95]\\n@.5\\n@[.5, .95]\\nbaseline Faster R-CNN (VGG-16)\\n41.5\\n21.2\\nbaseline Faster R-CNN (ResNet-101)\\n48.4\\n27.2\\n+box reﬁnement\\n49.9\\n29.9\\n+context\\n51.1\\n30.0\\n53.3\\n32.2\\n+multi-scale testing\\n53.8\\n32.5\\n55.7\\n34.9\\nensemble\\n59.0\\n37.4\\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\\nsystem\\nnet\\ndata\\nmAP\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike person\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nbaseline\\nVGG-16\\n07+12\\n73.2\\n76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\\nbaseline\\nResNet-101\\n07+12\\n76.4\\n79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\\nbaseline+++ ResNet-101\\nCOCO+07+12\\n85.6\\n90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system “baseline+++”\\ninclude box reﬁnement, context, and multi-scale testing in Table 9.\\nsystem\\nnet\\ndata\\nmAP\\nareo\\nbike\\nbird\\nboat\\nbottle\\nbus\\ncar\\ncat\\nchair\\ncow\\ntable\\ndog\\nhorse\\nmbike person\\nplant\\nsheep\\nsofa\\ntrain\\ntv\\nbaseline\\nVGG-16\\n07++12\\n70.4\\n84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\\nbaseline\\nResNet-101\\n07++12\\n73.8\\n86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\\nbaseline+++ ResNet-101 COCO+07++12\\n83.8\\n92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\\nTable 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/\\ndisplaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system “baseline+++” include\\nbox reﬁnement, context, and multi-scale testing in Table 9.\\nWe select two adjacent scales from the pyramid following\\n[33]. RoI pooling and subsequent layers are performed on\\nthe feature maps of these two scales [33], which are merged\\nby maxout as in [33]. Multi-scale testing improves the mAP\\nby over 2 points (Table 9).\\nUsing validation data. Next we use the 80k+40k trainval set\\nfor training and the 20k test-dev set for evaluation. The test-\\ndev set has no publicly available ground truth and the result\\nis reported by the evaluation server. Under this setting, the\\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\\n34.9% (Table 9). This is our single-model result.\\nEnsemble. In Faster R-CNN, the system is designed to learn\\nregion proposals and also object classiﬁers, so an ensemble\\ncan be used to boost both tasks. We use an ensemble for\\nproposing regions, and the union set of proposals are pro-\\ncessed by an ensemble of per-region classiﬁers. Table 9\\nshows our result based on an ensemble of 3 networks. The\\nmAP is 59.0% and 37.4% on the test-dev set. This result\\nwon the 1st place in the detection task in COCO 2015.\\nPASCAL VOC\\nWe revisit the PASCAL VOC dataset based on the above\\nmodel. With the single model on the COCO dataset (55.7%\\nmAP@.5 in Table 9), we ﬁne-tune this model on the PAS-\\nCAL VOC sets. The improvements of box reﬁnement, con-\\ntext, and multi-scale testing are also adopted. By doing so\\nval2\\ntest\\nGoogLeNet [44] (ILSVRC’14)\\n-\\n43.9\\nour single model (ILSVRC’15)\\n60.5\\n58.8\\nour ensemble (ILSVRC’15)\\n63.6\\n62.1\\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\\nOur detection system is Faster R-CNN [32] with the improvements\\nin Table 9, using ResNet-101.\\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\\non PASCAL VOC 2012 is 10 points higher than the previ-\\nous state-of-the-art result [6].\\nImageNet Detection\\nThe ImageNet Detection (DET) task involves 200 object\\ncategories. The accuracy is evaluated by mAP@.5. Our\\nobject detection algorithm for ImageNet DET is the same\\nas that for MS COCO in Table 9. The networks are pre-\\ntrained on the 1000-class ImageNet classiﬁcation set, and\\nare ﬁne-tuned on the DET data. We split the validation set\\ninto two parts (val1/val2) following [8]. We ﬁne-tune the\\ndetection models using the DET training set and the val1\\nset. The val2 set is used for validation. We do not use other\\nILSVRC 2015 data. Our single model with ResNet-101 has\\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html,\\nsubmitted on 2015-11-26.\\n11\\n'),\n",
       " Document(metadata={'source': 'docs/resnet_paper.pdf', 'file_path': 'docs/resnet_paper.pdf', 'page': 11, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151211011345Z', 'modDate': 'D:20151211011345Z', 'trapped': ''}, page_content='LOC\\nmethod\\nLOC\\nnetwork\\ntesting LOC error\\non GT CLS\\nclassiﬁcation\\nnetwork\\ntop-5 LOC error\\non predicted CLS\\nVGG’s [41]\\nVGG-16\\n1-crop\\n33.1 [41]\\nRPN\\nResNet-101 1-crop\\n13.3\\nRPN\\nResNet-101 dense\\n11.7\\nRPN\\nResNet-101 dense\\nResNet-101\\n14.4\\nRPN+RCNN ResNet-101 dense\\nResNet-101\\n10.6\\nRPN+RCNN\\nensemble\\ndense\\nensemble\\n8.9\\nTable 13. Localization error (%) on the ImageNet validation. In\\nthe column of “LOC error on GT class” ([41]), the ground truth\\nclass is used. In the “testing” column, “1-crop” denotes testing\\non a center crop of 224×224 pixels, “dense” denotes dense (fully\\nconvolutional) and multi-scale testing.\\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\\non the DET test set (Table 12). This result won the 1st place\\nin the ImageNet detection task in ILSVRC 2015, surpassing\\nthe second place by 8.5 points (absolute).\\nC. ImageNet Localization\\nThe ImageNet Localization (LOC) task [36] requires to\\nclassify and localize the objects. Following [40, 41], we\\nassume that the image-level classiﬁers are ﬁrst adopted for\\npredicting the class labels of an image, and the localiza-\\ntion algorithm only accounts for predicting bounding boxes\\nbased on the predicted classes. We adopt the “per-class re-\\ngression” (PCR) strategy [40, 41], learning a bounding box\\nregressor for each class. We pre-train the networks for Im-\\nageNet classiﬁcation and then ﬁne-tune them for localiza-\\ntion. We train networks on the provided 1000-class Ima-\\ngeNet training set.\\nOur localization algorithm is based on the RPN frame-\\nwork of [32] with a few modiﬁcations. Unlike the way in\\n[32] that is category-agnostic, our RPN for localization is\\ndesigned in a per-class form. This RPN ends with two sib-\\nling 1×1 convolutional layers for binary classiﬁcation (cls)\\nand box regression (reg), as in [32]. The cls and reg layers\\nare both in a per-class from, in contrast to [32]. Speciﬁ-\\ncally, the cls layer has a 1000-d output, and each dimension\\nis binary logistic regression for predicting being or not be-\\ning an object class; the reg layer has a 1000×4-d output\\nconsisting of box regressors for 1000 classes. As in [32],\\nour bounding box regression is with reference to multiple\\ntranslation-invariant “anchor” boxes at each position.\\nAs in our ImageNet classiﬁcation training (Sec. 3.4), we\\nrandomly sample 224×224 crops for data augmentation.\\nWe use a mini-batch size of 256 images for ﬁne-tuning. To\\navoid negative samples being dominate, 8 anchors are ran-\\ndomly sampled for each image, where the sampled positive\\nand negative anchors have a ratio of 1:1 [32]. For testing,\\nthe network is applied on the image fully-convolutionally.\\nTable 13 compares the localization results. Following\\n[41], we ﬁrst perform “oracle” testing using the ground truth\\nclass as the classiﬁcation prediction. VGG’s paper [41] re-\\nmethod\\ntop-5 localization err\\nval\\ntest\\nOverFeat [40] (ILSVRC’13)\\n30.0\\n29.9\\nGoogLeNet [44] (ILSVRC’14)\\n-\\n26.7\\nVGG [41] (ILSVRC’14)\\n26.9\\n25.3\\nours (ILSVRC’15)\\n8.9\\n9.0\\nTable 14. Comparisons of localization error (%) on the ImageNet\\ndataset with state-of-the-art methods.\\nports a center-crop error of 33.1% (Table 13) using ground\\ntruth classes. Under the same setting, our RPN method us-\\ning ResNet-101 net signiﬁcantly reduces the center-crop er-\\nror to 13.3%. This comparison demonstrates the excellent\\nperformance of our framework. With dense (fully convolu-\\ntional) and multi-scale testing, our ResNet-101 has an error\\nof 11.7% using ground truth classes. Using ResNet-101 for\\npredicting classes (4.6% top-5 classiﬁcation error, Table 4),\\nthe top-5 localization error is 14.4%.\\nThe above results are only based on the proposal network\\n(RPN) in Faster R-CNN [32]. One may use the detection\\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\\nresults. But we notice that on this dataset, one image usually\\ncontains a single dominate object, and the proposal regions\\nhighly overlap with each other and thus have very similar\\nRoI-pooled features. As a result, the image-centric training\\nof Fast R-CNN [7] generates samples of small variations,\\nwhich may not be desired for stochastic training. Motivated\\nby this, in our current experiment we use the original R-\\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\\nOur R-CNN implementation is as follows. We apply the\\nper-class RPN trained as above on the training images to\\npredict bounding boxes for the ground truth class. These\\npredicted boxes play a role of class-dependent proposals.\\nFor each training image, the highest scored 200 proposals\\nare extracted as training samples to train an R-CNN classi-\\nﬁer. The image region is cropped from a proposal, warped\\nto 224×224 pixels, and fed into the classiﬁcation network\\nas in R-CNN [8]. The outputs of this network consist of two\\nsibling fc layers for cls and reg, also in a per-class form.\\nThis R-CNN network is ﬁne-tuned on the training set us-\\ning a mini-batch size of 256 in the RoI-centric fashion. For\\ntesting, the RPN generates the highest scored 200 proposals\\nfor each predicted class, and the R-CNN network is used to\\nupdate these proposals’ scores and box positions.\\nThis method reduces the top-5 localization error to\\n10.6% (Table 13). This is our single-model result on the\\nvalidation set. Using an ensemble of networks for both clas-\\nsiﬁcation and localization, we achieve a top-5 localization\\nerror of 9.0% on the test set. This number signiﬁcantly out-\\nperforms the ILSVRC 14 results (Table 14), showing a 64%\\nrelative reduction of error. This result won the 1st place in\\nthe ImageNet localization task in ILSVRC 2015.\\n12\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 0, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nAN IMAGE IS WORTH 16X16 WORDS:\\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\\n∗equal technical contribution, †equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\nABSTRACT\\nWhile the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.1\\n1\\nINTRODUCTION\\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classiﬁcation in supervised fashion.\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 1, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufﬁcient amounts of data.\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefﬁciently on hardware accelerators.\\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019; Li et al., 2019).\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n2\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 2, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 3, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\nz0 = [xclass; x1\\npE; x2\\npE; · · · ; xN\\np E] + Epos,\\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\\n(1)\\nz′\\nℓ= MSA(LN(zℓ−1)) + zℓ−1,\\nℓ= 1 . . . L\\n(2)\\nzℓ= MLP(LN(z′\\nℓ)) + z′\\nℓ,\\nℓ= 1 . . . L\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\nHybrid Architecture.\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classiﬁcation input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n4.1\\nSETUP\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n4\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 4, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\\nimagery, and Structured – tasks that require geometric understanding like localization.\\nModel Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\nMetrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\\nwhere ﬁne-tuning would be too costly.\\n4.2\\nCOMPARISON TO STATE OF THE ART\\nWe ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n5\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 5, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfﬁcientNet-L2)\\nImageNet\\n88.55 ± 0.04\\n87.76 ± 0.03\\n85.30 ± 0.02\\n87.54 ± 0.02\\n88.4/88.5∗\\nImageNet ReaL\\n90.72 ± 0.05\\n90.54 ± 0.03\\n88.62 ± 0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50 ± 0.06\\n99.42 ± 0.03\\n99.15 ± 0.03\\n99.37 ± 0.06\\n−\\nCIFAR-100\\n94.55 ± 0.04\\n93.90 ± 0.05\\n93.25 ± 0.05\\n93.51 ± 0.08\\n−\\nOxford-IIIT Pets\\n97.56 ± 0.03\\n97.32 ± 0.11\\n94.67 ± 0.15\\n96.62 ± 0.23\\n−\\nOxford Flowers-102\\n99.68 ± 0.02\\n99.74 ± 0.00\\n99.61 ± 0.02\\n99.63 ± 0.03\\n−\\nVTAB (19 tasks)\\n77.63 ± 0.23\\n76.28 ± 0.46\\n72.72 ± 0.21\\n76.29 ± 1.70\\n−\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\\nin Touvron et al. (2020).\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy [%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\\nSpecialized the performance of the top two models is similar.\\n4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\\nexperiments.\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\\nresolution increase during ﬁne-tuning improves the performance.\\n6\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 6, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy [%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets. Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1 [%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size. ResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training. ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n102\\n103\\n90\\n95\\nTransfer accuracy [%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufﬁcient, even beneﬁcial.\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\\nis an exciting direction of future work.\\n7\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 7, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace. See Appendix D.7 for\\ndetails.\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations. The ﬁrst layer of\\nthe Vision Transformer linearly projects the ﬂattened patches into a\\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\\ncipal components of the the learned embedding ﬁlters. The com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ﬁne structure within each patch.\\nAfter the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Speciﬁcally, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model. Other attention heads\\nhave consistently small attention distances in the low layers. This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\\nregions that are semantically relevant for classiﬁcation (Figure 6).\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks. However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work.\\n5\\nCONCLUSION\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\\ndatasets, whilst being relatively cheap to pre-train.\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\nREFERENCES\\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\\nmutual information across views. In NeurIPS, 2019.\\n9\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 9, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\\nICLR, 2019.\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\\nIn ICCV, 2019.\\nLucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\\nwe done with imagenet? arXiv, 2020.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv, 2020.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\\npixels. In ICML, 2020a.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\\nfor contrastive learning of visual representations. In ICML, 2020b.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\ntransformers. arXiv, 2019.\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\\nattention and convolutional layers. In ICLR, 2020.\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR, 2009.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL, 2019.\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\\nlutional neural networks. arXiv, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition. In CVPR, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\\nMomentum contrast for\\nunsupervised visual representation learning. In CVPR, 2020.\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\\nmensional transformers. arXiv, 2019.\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In CVPR, 2018.\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\\nIn ICCV, 2019.\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\nOlivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\\nICML, 2020.\\n10\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 10, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\\ntion. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\\nAshwin Bharambe, and Laurens van der Maaten.\\nExploring the limits of weakly supervised\\npretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\\nICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\\n2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\\nDustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\\nJournal on Control and Optimization, 30(4):838–855, 1992.\\ndoi: 10.1137/0330046.\\nURL\\nhttps://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\\npreprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\\nStand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\\nfectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\\nmodel for video and language representation learning. In ICCV, 2019.\\n11\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 11, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy. In NeurIPS. 2019.\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020.\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\\n2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\\nChen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\\narXiv preprint\\narXiv:2003.07853, 2020b.\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\\nLearning deep transformer models for machine translation. In ACL, 2019.\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\\nCVPR, 2018.\\nDirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\\nICLR, 2019.\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\\nfor computer vision. arxiv, 2020.\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\\nimproves imagenet classiﬁcation. In CVPR, 2020.\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\\nSupervised Learning. In ICCV, 2019a.\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\\npreprint arXiv:1910.04867, 2019b.\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\\nCVPR, 2020.\\n12\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nModels\\nDataset\\nEpochs\\nBase LR\\nLR decay\\nWeight decay\\nDropout\\nViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/32\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-H/14\\nJFT-300M\\n14\\n3 · 10−4\\nlinear\\n0.1\\n0.0\\nR50x{1,2}\\nJFT-300M\\n7\\n10−3\\nlinear\\n0.1\\n0.0\\nR101x1\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR152x{1,2}\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/32\\nJFT-300M\\n7\\n2 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-B/{16,32}\\nImageNet-21k\\n90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-L/{16,32}\\nImageNet-21k\\n30/90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-∗\\nImageNet\\n300\\n3 · 10−3\\ncosine\\n0.3\\n0.1\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\\nclipping at global norm 1. Training resolution is 224.\\nAPPENDIX\\nA\\nMULTIHEAD SELF-ATTENTION\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\\ntwo elements of the sequence and their respective query qi and key kj representations.\\n[q, k, v] = zUqkv\\nUqkv ∈RD×3Dh,\\n(5)\\nA = softmax\\n\\x10\\nqk⊤/\\np\\nDh\\n\\x11\\nA ∈RN×N,\\n(6)\\nSA(z) = Av .\\n(7)\\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\\nMSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\\nUmsa ∈Rk·Dh×D\\n(8)\\nB\\nEXPERIMENT DETAILS\\nB.1\\nTRAINING\\nTable 3 summarizes our training setups for our different models. We found strong regularization\\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\\ntraining is done on resolution 224.\\nB.1.1\\nFINE-TUNING\\nWe ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n13\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nDataset\\nSteps\\nBase LR\\nImageNet\\n20 000\\n{0.003, 0.01, 0.03, 0.06}\\nCIFAR100\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nCIFAR10\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nOxford-IIIT Pets\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nOxford Flowers-102\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nVTAB (19 tasks)\\n2 500\\n0.01\\nTable 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\\nﬁne-tuning resolution is 384.\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\\net al., 2020)).\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks.\\nB.1.2\\nSELF-SUPERVISION\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\\npatch representations.\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\\n(2019) but results were also slightly worse on our few-shot metrics.\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\\nImageNet.\\nC\\nADDITIONAL RESULTS\\nWe report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n14\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 14, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nViT-B/16\\nViT-B/32\\nViT-L/16\\nViT-L/32\\nViT-H/14\\nImageNet\\nCIFAR-10\\n98.13\\n97.77\\n97.86\\n97.94\\n-\\nCIFAR-100\\n87.13\\n86.31\\n86.35\\n87.07\\n-\\nImageNet\\n77.91\\n73.38\\n76.53\\n71.16\\n-\\nImageNet ReaL\\n83.57\\n79.56\\n82.19\\n77.83\\n-\\nOxford Flowers-102\\n89.49\\n85.43\\n89.66\\n86.36\\n-\\nOxford-IIIT-Pets\\n93.81\\n92.04\\n93.64\\n91.35\\n-\\nImageNet-21k\\nCIFAR-10\\n98.95\\n98.79\\n99.16\\n99.13\\n99.27\\nCIFAR-100\\n91.67\\n91.97\\n93.44\\n93.04\\n93.82\\nImageNet\\n83.97\\n81.28\\n85.15\\n80.99\\n85.13\\nImageNet ReaL\\n88.35\\n86.63\\n88.40\\n85.65\\n88.70\\nOxford Flowers-102\\n99.38\\n99.11\\n99.61\\n99.19\\n99.51\\nOxford-IIIT-Pets\\n94.43\\n93.02\\n94.73\\n93.09\\n94.82\\nJFT-300M\\nCIFAR-10\\n99.00\\n98.61\\n99.38\\n99.19\\n99.50\\nCIFAR-100\\n91.87\\n90.49\\n94.04\\n92.52\\n94.55\\nImageNet\\n84.15\\n80.73\\n87.12\\n84.37\\n88.04\\nImageNet ReaL\\n88.85\\n86.27\\n89.99\\n88.28\\n90.33\\nOxford Flowers-102\\n99.56\\n99.27\\n99.56\\n99.45\\n99.68\\nOxford-IIIT-Pets\\n95.80\\n93.40\\n97.11\\n95.83\\n97.56\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\nEpochs\\nImageNet\\nImageNet ReaL\\nCIFAR-10\\nCIFAR-100\\nPets\\nFlowers\\nexaFLOPs\\nname\\nViT-B/32\\n7\\n80.73\\n86.27\\n98.61\\n90.49\\n93.40\\n99.27\\n55\\nViT-B/16\\n7\\n84.15\\n88.85\\n99.00\\n91.87\\n95.80\\n99.56\\n224\\nViT-L/32\\n7\\n84.37\\n88.28\\n99.19\\n92.52\\n95.83\\n99.45\\n196\\nViT-L/16\\n7\\n86.30\\n89.43\\n99.38\\n93.46\\n96.81\\n99.66\\n783\\nViT-L/16\\n14\\n87.12\\n89.99\\n99.38\\n94.04\\n97.11\\n99.56\\n1567\\nViT-H/14\\n14\\n88.08\\n90.36\\n99.50\\n94.71\\n97.11\\n99.71\\n4262\\nResNet50x1\\n7\\n77.54\\n84.56\\n97.67\\n86.07\\n91.11\\n94.26\\n50\\nResNet50x2\\n7\\n82.12\\n87.94\\n98.29\\n89.20\\n93.43\\n97.02\\n199\\nResNet101x1\\n7\\n80.67\\n87.07\\n98.48\\n89.17\\n94.08\\n95.95\\n96\\nResNet152x1\\n7\\n81.88\\n87.96\\n98.82\\n90.22\\n94.17\\n96.94\\n141\\nResNet152x2\\n7\\n84.97\\n89.69\\n99.06\\n92.05\\n95.37\\n98.62\\n563\\nResNet152x2\\n14\\n85.56\\n89.89\\n99.24\\n91.92\\n95.75\\n98.75\\n1126\\nResNet200x3\\n14\\n87.22\\n90.15\\n99.34\\n93.53\\n96.32\\n99.04\\n3306\\nR50x1+ViT-B/32\\n7\\n84.90\\n89.15\\n99.01\\n92.24\\n95.75\\n99.46\\n106\\nR50x1+ViT-B/16\\n7\\n85.58\\n89.65\\n99.14\\n92.63\\n96.65\\n99.40\\n274\\nR50x1+ViT-L/32\\n7\\n85.68\\n89.04\\n99.24\\n92.93\\n96.97\\n99.43\\n246\\nR50x1+ViT-L/16\\n7\\n86.60\\n89.72\\n99.18\\n93.64\\n97.03\\n99.40\\n859\\nR50x1+ViT-L/16\\n14\\n87.12\\n89.76\\n99.31\\n93.89\\n97.36\\n99.11\\n1668\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\\naFLOPs).\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\\nvarying size, as well as the estimated computational cost of their pre-training.\\nD\\nADDITIONAL ANALYSES\\nD.1\\nSGD VS. ADAM FOR RESNETS\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning\\n15\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 15, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nResNet50\\nResNet152x2\\nDataset\\nAdam\\nSGD\\nAdam\\nSGD\\nImageNet\\n77.54\\n78.24\\n84.97\\n84.37\\nCIFAR10\\n97.67\\n97.46\\n99.06\\n99.07\\nCIFAR100\\n86.07\\n85.17\\n92.05\\n91.06\\nOxford-IIIT Pets\\n91.11\\n91.00\\n95.37\\n94.79\\nOxford Flowers-102\\n94.26\\n92.06\\n98.62\\n99.32\\nAverage\\n89.33\\n88.79\\n94.01\\n93.72\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n100\\n101\\nRelative Compute\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nImageNet 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\n100\\n101\\nRelative Compute\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAverage 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\nperformance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\\nfor 7 epochs, not 30.\\nD.2\\nTRANSFORMER SHAPE\\nWe ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\\neffective sequence length shows surprisingly robust improvements without introducing parameters.\\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\\nall dimensions proportionally results in robust improvements.\\nD.3\\nHEAD TYPE AND CLASS TOKEN\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\nThis design is inherited from the Transformer model for text, and we use it throughout the main\\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n16\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 16, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nEpochs of training\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nImageNet linear 5-shot accuracy [%]\\nCLS-Token, lr=8e-4\\nGAP, lr=8e-4\\nGAP, lr=3e-4\\nFigure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\\nwell, but require different learning-rates.\\nPos. Emb.\\nDefault/Stem\\nEvery Layer\\nEvery Layer-Shared\\nNo Pos. Emb.\\n0.61382\\nN/A\\nN/A\\n1-D Pos. Emb.\\n0.64206\\n0.63964\\n0.64292\\n2-D Pos. Emb.\\n0.64001\\n0.64046\\n0.64022\\nRel. Pos. Emb.\\n0.64032\\nN/A\\nN/A\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\\nImageNet 5-shot linear.\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\\nFigure 9.\\nD.4\\nPOSITIONAL EMBEDDING\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\\ntried the following cases:\\n• Providing no positional information: Considering the inputs as a bag of patches.\\n• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\\nthe raster order (default across all other experiments in this paper).\\n• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\\nembedding for that patch.\\n• Relative positional embeddings: Considering the relative distance between patches to en-\\ncode the spatial information as instead of their absolute position. To do so, we use 1-\\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\\nembedding. Then, we simply run extra attention, where we use the original query (the\\ncontent of query), but use relative positional embeddings as keys. We then use the log-\\nits from the relative attention as a bias term and add it to the logits of the main attention\\n(content-based attention) before applying the softmax.\\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n17\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 17, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0002, WD=0.01\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0004, WD=0.1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n14 epochs, LR=0.0004, WD=0.1\\n1\\n1\\nCosine similarity\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\\neach layer (shared between layers).\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\\nthere is a large gap between the performances of the model with no positional embedding and mod-\\nels with positional embedding, there is little to no difference between different ways of encoding\\npositional information. We speculate that since our Transformer encoder operates on patch-level\\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\\nhyperparameters (Figure 10).\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nR50x1 + ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\\nheads at one layer. Image width is 224 pixels.\\nD.5\\nEMPIRICAL COMPUTATIONAL COSTS\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n18\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 18, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 19, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n102\\nTotal compute [exaFLOPs]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\n102\\n103\\nPeak inference speed [img/sec/core]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\\nattention and although the sequence length that self-attention operates on is smaller in axial case,\\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\\non TPUs (Figure 13, right).\\nD.7\\nATTENTION DISTANCE\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\nD.8\\nATTENTION MAPS\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\\nfor the mixing of attention across tokens through all layers.\\nD.9\\nOBJECTNET RESULTS\\nWe also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\\nD.10\\nVTAB BREAKDOWN\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n20\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 20, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\n88\\n89\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n21\\n'),\n",
       " Document(metadata={'source': 'docs/Vision Transformers.pdf', 'file_path': 'docs/Vision Transformers.pdf', 'page': 21, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\nCaltech101\\nCIFAR-100\\nDTD\\nFlowers102\\nPets\\nSun397\\nSVHN\\nCamelyon\\nEuroSAT\\nResisc45\\nRetinopathy\\nClevr-Count\\nClevr-Dist\\nDMLab\\ndSpr-Loc\\ndSpr-Ori\\nKITTI-Dist\\nsNORB-Azim\\nsNORB-Elev\\nMean\\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\\n22\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 1, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 2, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 3, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 4, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 5, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 6, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 8, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 9, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 10, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 11, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 12, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 13, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\n'),\n",
       " Document(metadata={'source': 'docs/attention_paper.pdf', 'file_path': 'docs/attention_paper.pdf', 'page': 14, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230803000729Z', 'modDate': 'D:20230803000729Z', 'trapped': ''}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1 (\\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\nKeywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n1\\nIntroduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [11,\\narXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 1, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='2\\nZ. Shen et al.\\n37], layout detection [38, 22], table detection [26], and scene text detection [4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual speciﬁcation of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and beneﬁt a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical diﬃculties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [1] or PyTorch [24], and the high-level parameters can\\nbe obfuscated by implementation details [8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would beneﬁt the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-ﬂedged infrastructure for easily curating the target document image datasets\\nand ﬁne-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the ﬁnal outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,\\nand these pipelines are not documented in any central location (and often not\\ndocumented at all). This makes it diﬃcult for research teams to learn about how\\nfull pipelines are implemented and leads them to invest signiﬁcant resources in\\nreinventing the DIA wheel.\\nLayoutParser provides a uniﬁed toolkit to support DL-based document image\\nanalysis and processing. To address the aforementioned challenges, LayoutParser\\nis built with the following components:\\n1. An oﬀ-the-shelf toolkit for applying DL models for layout detection, character\\nrecognition, and other DIA tasks (Section 3)\\n2. A rich repository of pre-trained neural network models (Model Zoo) that\\nunderlies the oﬀ-the-shelf usage\\n3. Comprehensive tools for eﬃcient document image data annotation and model\\ntuning to support diﬀerent levels of customization\\n4. A DL model hub and community platform for the easy sharing, distribu-\\ntion, and discussion of DIA models and pipelines, to promote reusability,\\nreproducibility, and extensibility (Section 4)\\nThe library implements simple and intuitive Python APIs without sacriﬁcing\\ngeneralizability and versatility, and can be easily installed via pip. Its convenient\\nfunctions for handling document image data can be seamlessly integrated with\\nexisting DIA pipelines. With detailed documentations and carefully curated\\ntutorials, we hope this tool will beneﬁt a variety of end-users, and will lead to\\nadvances in applications in both industry and academic research.\\nLayoutParser is well aligned with recent eﬀorts for improving DL model\\nreusability in other disciplines like natural language processing [8, 34] and com-\\nputer vision [35], but with a focus on unique challenges in DIA. We show\\nLayoutParser can be applied in sophisticated and large-scale digitization projects\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 2, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n3\\nthat require precision, eﬃciency, and robustness, as well as simple and light-\\nweight document processing tasks focusing on eﬃcacy and ﬂexibility (Section 5).\\nLayoutParser is being actively maintained, and support for more deep learning\\nmodels and novel methods in text-based layout analysis methods [37, 34] is\\nplanned.\\nThe rest of the paper is organized as follows. Section 2 provides an overview\\nof related work. The core LayoutParser library, DL Model Zoo, and customized\\nmodel training are described in Section 3, and the DL model hub and commu-\\nnity platform are detailed in Section 4. Section 5 shows two examples of how\\nLayoutParser can be used in practical DIA projects, and Section 6 concludes.\\n2\\nRelated Work\\nRecently, various DL models and datasets have been developed for layout analysis\\ntasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\\ntation tasks on historical documents. Object detection-based methods like Faster\\nR-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\\nand detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\\nbeen used in table detection [27]. However, these models are usually implemented\\nindividually and there is no uniﬁed framework to load and use such models.\\nThere has been a surge of interest in creating open-source tools for document\\nimage processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces 6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\nand Detectron2-PubLayNet10 are individual deep learning models trained on\\nlayout analysis datasets without support for the full DIA pipeline. The Document\\nAnalysis and Exploitation (DAE) platform [15] and the DeepDIVA project [2]\\naim to improve the reproducibility of DIA methods (or DL models), yet they\\nare not actively maintained. OCR engines like Tesseract [14], easyOCR11 and\\npaddleOCR12 usually do not come with comprehensive functionalities for other\\nDIA tasks like layout analysis.\\nRecent years have also seen numerous eﬀorts to create libraries for promoting\\nreproducibility and reusability in the ﬁeld of DL. Libraries like Dectectron2 [35],\\n6 The number shown is obtained by specifying the search type as ‘code’.\\n7 https://ocr-d.de/en/about\\n8 https://github.com/BobLd/DocumentLayoutAnalysis\\n9 https://github.com/leonlulu/DeepLayout\\n10 https://github.com/hpanwar08/detectron2\\n11 https://github.com/JaidedAI/EasyOCR\\n12 https://github.com/PaddlePaddle/PaddleOCR\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 3, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='4\\nZ. Shen et al.\\nEfficient Data Annotation\\nCustomized Model Training\\nModel Customization\\nDIA Model Hub\\nDIA Pipeline Sharing\\nCommunity Platform\\nLayout Detection Models\\nDocument Images \\nThe Core LayoutParser Library\\nOCR Module\\nStorage & Visualization\\nLayout Data Structure\\nFig. 1: The overall architecture of LayoutParser. For an input document image,\\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\\ndata structure. LayoutParser also supports high level customization via eﬃcient\\nlayout annotation and model training functions. These improve model accuracy\\non the target samples. The community platform enables the easy sharing of DIA\\nmodels and whole digitization pipelines to promote reusability and reproducibility.\\nA collection of detailed documentation, tutorials and exemplar projects make\\nLayoutParser easy to learn and use.\\nAllenNLP [8] and transformers [34] have provided the community with complete\\nDL-based support for developing and deploying models for general computer\\nvision and natural language processing problems. LayoutParser, on the other\\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\\ncommunity platform inspired by established model hubs such as Torch Hub [23]\\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\\nfull document processing pipelines that are unique to DIA tasks.\\nThere have been a variety of document data collections to facilitate the\\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\\nPubLayNet [38](academic paper layouts), Table Bank [18](tables in academic\\npapers), Newspaper Navigator Dataset [16, 17](newspaper ﬁgure layouts) and\\nHJDataset [31](historical Japanese document layouts). A spectrum of models\\ntrained on these datasets are currently available in the LayoutParser model zoo\\nto support diﬀerent use cases.\\n3\\nThe Core LayoutParser Library\\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\\nbased document image analysis. Five components support a simple interface\\nwith comprehensive functionalities: 1) The layout detection models enable using\\npre-trained or self-trained DL models for layout detection with just four lines\\nof code. 2) The detected layout information is stored in carefully engineered\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 4, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n5\\nTable 1: Current layout detection models in the LayoutParser model zoo\\nDataset\\nBase Model1 Large Model\\nNotes\\nPubLayNet [38]\\nF / M\\nM\\nLayouts of modern scientiﬁc documents\\nPRImA [3]\\nM\\n-\\nLayouts of scanned modern magazines and scientiﬁc reports\\nNewspaper [17]\\nF\\n-\\nLayouts of scanned US newspapers from the 20th century\\nTableBank [18]\\nF\\nF\\nTable region on modern scientiﬁc and business document\\nHJDataset [31]\\nF / M\\n-\\nLayouts of history Japanese documents\\n1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\\nvs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\\nbackbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\\nR-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\\nzoo in coming months.\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When\\nnecessary, users can employ existing or customized OCR models via the uniﬁed\\nAPI provided in the OCR module. 4) LayoutParser comes with a set of utility\\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\\nis also highly customizable, via its integration with functions for layout data\\nannotation and model training. We now provide detailed descriptions for each\\ncomponent.\\n3.1\\nLayout Detection Models\\nIn LayoutParser, a layout model takes a document image as an input and\\ngenerates a list of rectangular boxes for the target content regions. Diﬀerent\\nfrom traditional methods, it relies on deep convolutional neural networks rather\\nthan manually curated rules to identify content regions. It is formulated as an\\nobject detection problem and state-of-the-art models like Faster R-CNN [28] and\\nMask R-CNN [12] are used. This yields prediction results of high accuracy and\\nmakes it possible to build a concise, generalized interface for layout detection.\\nLayoutParser, built upon Detectron2 [35], provides a minimal API that can\\nperform layout detection with only four lines of code in Python:\\n1 import\\nlayoutparser as lp\\n2 image = cv2.imread(\"image_file\") # load\\nimages\\n3 model = lp. Detectron2LayoutModel (\\n4\\n\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\\n5 layout = model.detect(image)\\nLayoutParser provides a wealth of pre-trained model weights using various\\ndatasets covering diﬀerent languages, time periods, and document types. Due to\\ndomain shift [7], the prediction performance can notably drop when models are ap-\\nplied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\\ndocument structures and layouts vary greatly in diﬀerent domains, it is important\\nto select models trained on a dataset similar to the test samples. A semantic syntax\\nis used for initializing the model weights in LayoutParser, using both the dataset\\nname and model name lp://<dataset-name>/<model-architecture-name>.\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 5, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='6\\nZ. Shen et al.\\nFig. 2: The relationship between the three types of layout data structures.\\nCoordinate supports three kinds of variation; TextBlock consists of the co-\\nordinate information and extra features like block text, types, and reading orders;\\na Layout object is a list of all possible layout elements, including other Layout\\nobjects. They all support the same set of transformation and operation APIs for\\nmaximum ﬂexibility.\\nShown in Table 1, LayoutParser currently hosts 9 pre-trained models trained\\non 5 diﬀerent datasets. Description of the training dataset is provided alongside\\nwith the trained models such that users can quickly identify the most suitable\\nmodels for their tasks. Additionally, when such a model is not readily available,\\nLayoutParser also supports training customized layout models and community\\nsharing of the models (detailed in Section 3.5).\\n3.2\\nLayout Data Structures\\nA critical feature of LayoutParser is the implementation of a series of data\\nstructures and operations that can be used to eﬃciently process and manipulate\\nthe layout elements. In document image analysis pipelines, various post-processing\\non the layout analysis model outputs is usually required to obtain the ﬁnal\\noutputs. Traditionally, this requires exporting DL model outputs and then loading\\nthe results into other pipelines. All model outputs from LayoutParser will be\\nstored in carefully engineered data types optimized for further processing, which\\nmakes it possible to build an end-to-end document digitization pipeline within\\nLayoutParser. There are three key components in the data structure, namely\\nthe Coordinate system, the TextBlock, and the Layout. They provide diﬀerent\\nlevels of abstraction for the layout data, and a set of APIs are supported for\\ntransformations or operations on these classes.\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 6, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n7\\nCoordinates are the cornerstones for storing layout information. Currently,\\nthree types of Coordinate data structures are provided in LayoutParser, shown\\nin Figure 2. Interval and Rectangle are the most common data types and\\nsupport specifying 1D or 2D regions within a document. They are parameterized\\nwith 2 and 4 parameters. A Quadrilateral class is also implemented to support\\na more generalized representation of rectangular regions when the document\\nis skewed or distorted, where the 4 corner points can be speciﬁed and a total\\nof 8 degrees of freedom are supported. A wide collection of transformations\\nlike shift, pad, and scale, and operations like intersect, union, and is_in,\\nare supported for these classes. Notably, it is common to separate a segment\\nof the image and analyze it individually. LayoutParser provides full support\\nfor this scenario via image cropping operations crop_image and coordinate\\ntransformations like relative_to and condition_on that transform coordinates\\nto and from their relative representations. We refer readers to Table 2 for a more\\ndetailed description of these operations13.\\nBased on Coordinates, we implement the TextBlock class that stores both\\nthe positional and extra features of individual layout elements. It also supports\\nspecifying the reading orders via setting the parent ﬁeld to the index of the parent\\nobject. A Layout class is built that takes in a list of TextBlocks and supports\\nprocessing the elements in batch. Layout can also be nested to support hierarchical\\nlayout structures. They support the same operations and transformations as the\\nCoordinate classes, minimizing both learning and deployment eﬀort.\\n3.3\\nOCR\\nLayoutParser provides a uniﬁed interface for existing OCR tools. Though there\\nare many OCR tools available, they are usually conﬁgured diﬀerently with distinct\\nAPIs or protocols for using them. It can be ineﬃcient to add new OCR tools into\\nan existing pipeline, and diﬃcult to make direct comparisons among the available\\ntools to ﬁnd the best option for a particular project. To this end, LayoutParser\\nbuilds a series of wrappers among existing OCR engines, and provides nearly\\nthe same syntax for using them. It supports a plug-and-play style of using OCR\\nengines, making it eﬀortless to switch, evaluate, and compare diﬀerent OCR\\nmodules:\\n1 ocr_agent = lp.TesseractAgent ()\\n2 # Can be easily\\nswitched to other OCR\\nsoftware\\n3 tokens = ocr_agent.detect(image)\\nThe OCR outputs will also be stored in the aforementioned layout data\\nstructures and can be seamlessly incorporated into the digitization pipeline.\\nCurrently LayoutParser supports the Tesseract and Google Cloud Vision OCR\\nengines.\\nLayoutParser also comes with a DL-based CNN-RNN OCR model [6] trained\\nwith the Connectionist Temporal Classiﬁcation (CTC) loss [10]. It can be used\\nlike the other OCR modules, and can be easily trained on customized datasets.\\n13 This is also available in the LayoutParser documentation pages.\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 7, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='8\\nZ. Shen et al.\\nTable 2: All operations supported by the layout elements. The same APIs are\\nsupported across diﬀerent layout element classes including Coordinate types,\\nTextBlock and Layout.\\nOperation Name\\nDescription\\nblock.pad(top, bottom, right, left)\\nEnlarge the current block according to the input\\nblock.scale(fx, fy)\\nScale the current block given the ratio\\nin x and y direction\\nblock.shift(dx, dy)\\nMove the current block with the shift\\ndistances in x and y direction\\nblock1.is in(block2)\\nWhether block1 is inside of block2\\nblock1.intersect(block2)\\nReturn the intersection region of block1 and block2.\\nCoordinate type to be determined based on the inputs.\\nblock1.union(block2)\\nReturn the union region of block1 and block2.\\nCoordinate type to be determined based on the inputs.\\nblock1.relative to(block2)\\nConvert the absolute coordinates of block1 to\\nrelative coordinates to block2\\nblock1.condition on(block2)\\nCalculate the absolute coordinates of block1 given\\nthe canvas block2’s absolute coordinates\\nblock.crop image(image)\\nObtain the image segments in the block region\\n3.4\\nStorage and visualization\\nThe end goal of DIA is to transform the image-based document data into a\\nstructured database. LayoutParser supports exporting layout data into diﬀerent\\nformats like JSON, csv, and will add the support for the METS/ALTO XML\\nformat 14 . It can also load datasets from layout analysis-speciﬁc formats like\\nCOCO [38] and the Page Format [25] for training layout models (Section 3.5).\\nVisualization of the layout detection results is critical for both presentation\\nand debugging. LayoutParser is built with an integrated API for displaying the\\nlayout information along with the original document image. Shown in Figure 3, it\\nenables presenting layout data with rich meta information and features in diﬀerent\\nmodes. More detailed information can be found in the online LayoutParser\\ndocumentation page.\\n3.5\\nCustomized Model Training\\nBesides the oﬀ-the-shelf library, LayoutParser is also highly customizable with\\nsupports for highly unique and challenging document analysis tasks. Target\\ndocument images can be vastly diﬀerent from the existing datasets for train-\\ning layout models, which leads to low layout detection accuracy. Training data\\n14 https://altoxml.github.io\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 8, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n9\\nFig. 3: Layout detection and OCR results visualization generated by the\\nLayoutParser APIs. Mode I directly overlays the layout region bounding boxes\\nand categories over the original image. Mode II recreates the original document\\nvia drawing the OCR’d texts at their corresponding positions on the image\\ncanvas. In this ﬁgure, tokens in textual regions are ﬁltered using the API and\\nthen displayed.\\ncan also be highly sensitive and not sharable publicly. To overcome these chal-\\nlenges, LayoutParser is built with rich features for eﬃcient data annotation and\\ncustomized model training.\\nLayoutParser incorporates a toolkit optimized for annotating document lay-\\nouts using object-level active learning [32]. With the help from a layout detection\\nmodel trained along with labeling, only the most important layout objects within\\neach image, rather than the whole image, are required for labeling. The rest of\\nthe regions are automatically annotated with high conﬁdence predictions from\\nthe layout detection model. This allows a layout dataset to be created more\\neﬃciently with only around 60% of the labeling budget.\\nAfter the training dataset is curated, LayoutParser supports diﬀerent modes\\nfor training the layout models. Fine-tuning can be used for training models on a\\nsmall newly-labeled dataset by initializing the model with existing pre-trained\\nweights. Training from scratch can be helpful when the source dataset and\\ntarget are signiﬁcantly diﬀerent and a large training set is available. However, as\\nsuggested in Studer et al.’s work[33], loading pre-trained weights on large-scale\\ndatasets like ImageNet [5], even from totally diﬀerent domains, can still boost\\nmodel performance. Through the integrated API provided by LayoutParser,\\nusers can easily compare model performances on the benchmark datasets.\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 9, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='10\\nZ. Shen et al.\\nFig. 4: Illustration of (a) the original historical Japanese document with layout\\ndetection results and (b) a recreated version of the document image that achieves\\nmuch better character recognition recall. The reorganization algorithm rearranges\\nthe tokens based on the their detected bounding boxes given a maximum allowed\\nheight.\\n4\\nLayoutParser Community Platform\\nAnother focus of LayoutParser is promoting the reusability of layout detection\\nmodels and full digitization pipelines. Similar to many existing deep learning\\nlibraries, LayoutParser comes with a community model hub for distributing\\nlayout models. End-users can upload their self-trained models to the model hub,\\nand these models can be loaded into a similar interface as the currently available\\nLayoutParser pre-trained models. For example, the model trained on the News\\nNavigator dataset [17] has been incorporated in the model hub.\\nBeyond DL models, LayoutParser also promotes the sharing of entire doc-\\nument digitization pipelines. For example, sometimes the pipeline requires the\\ncombination of multiple DL models to achieve better accuracy. Currently, pipelines\\nare mainly described in academic papers and implementations are often not pub-\\nlicly available. To this end, the LayoutParser community platform also enables\\nthe sharing of layout pipelines to promote the discussion and reuse of techniques.\\nFor each shared pipeline, it has a dedicated project page, with links to the source\\ncode, documentation, and an outline of the approaches. A discussion panel is\\nprovided for exchanging ideas. Combined with the core LayoutParser library,\\nusers can easily build reusable components based on the shared pipelines and\\napply them to solve their unique problems.\\n5\\nUse Cases\\nThe core objective of LayoutParser is to make it easier to create both large-scale\\nand light-weight document digitization pipelines. Large-scale document processing\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 10, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n11\\nfocuses on precision, eﬃciency, and robustness. The target documents may have\\ncomplicated structures, and may require training multiple layout detection models\\nto achieve the optimal accuracy. Light-weight pipelines are built for relatively\\nsimple documents, with an emphasis on development ease, speed and ﬂexibility.\\nIdeally one only needs to use existing resources, and model training should be\\navoided. Through two exemplar projects, we show how practitioners in both\\nacademia and industry can easily build such pipelines using LayoutParser and\\nextract high-quality structured document data for their downstream tasks. The\\nsource code for these projects will be publicly available in the LayoutParser\\ncommunity hub.\\n5.1\\nA Comprehensive Historical Document Digitization Pipeline\\nThe digitization of historical documents can unlock valuable data that can shed\\nlight on many important social, economic, and historical questions. Yet due to\\nscan noises, page wearing, and the prevalence of complicated layout structures, ob-\\ntaining a structured representation of historical document scans is often extremely\\ncomplicated.\\nFig. 5: Illustration of how LayoutParser\\nhelps with the historical document digi-\\ntization pipeline.\\nIn this example, LayoutParser was\\nused to develop a comprehensive\\npipeline, shown in Figure 5, to gener-\\nate high-quality structured data from\\nhistorical Japanese ﬁrm ﬁnancial ta-\\nbles with complicated layouts. The\\npipeline applies two layout models to\\nidentify diﬀerent levels of document\\nstructures and two customized OCR\\nengines for optimized character recog-\\nnition accuracy.\\nAs shown in Figure 4 (a), the\\ndocument contains columns of text\\nwritten vertically 15, a common style\\nin Japanese. Due to scanning noise\\nand archaic printing technology, the\\ncolumns can be skewed or have vari-\\nable widths, and hence cannot be eas-\\nily identiﬁed via rule-based methods.\\nWithin each column, words are sepa-\\nrated by white spaces of variable size,\\nand the vertical positions of objects\\ncan be an indicator of their layout\\ntype.\\n15 A document page consists of eight rows like this. For simplicity we skip the row\\nsegmentation discussion and refer readers to the source code when available.\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 11, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='12\\nZ. Shen et al.\\nTo decipher the complicated layout\\nstructure, two object detection models have been trained to recognize individual\\ncolumns and tokens, respectively. A small training set (400 images with approxi-\\nmately 100 annotations each) is curated via the active learning based annotation\\ntool [32] in LayoutParser. The models learn to identify both the categories and\\nregions for each token or column via their distinct visual features. The layout\\ndata structure enables easy grouping of the tokens within each column, and\\nrearranging columns to achieve the correct reading orders based on the horizontal\\nposition. Errors are identiﬁed and rectiﬁed via checking the consistency of the\\nmodel predictions. Therefore, though trained on a small dataset, the pipeline\\nachieves a high level of layout detection accuracy: it achieves a 96.97 AP [19]\\nscore across 5 categories for the column detection model, and a 89.23 AP across\\n4 categories for the token detection model.\\nA combination of character recognition methods is developed to tackle the\\nunique challenges in this document. In our experiments, we found that irregular\\nspacing between the tokens led to a low character recognition recall rate, whereas\\nexisting OCR models tend to perform better on densely-arranged texts. To\\novercome this challenge, we create a document reorganization algorithm that\\nrearranges the text based on the token bounding boxes detected in the layout\\nanalysis step. Figure 4 (b) illustrates the generated image of dense text, which is\\nsent to the OCR APIs as a whole to reduce the transaction costs. The ﬂexible\\ncoordinate system in LayoutParser is used to transform the OCR results relative\\nto their original positions on the page.\\nAdditionally, it is common for historical documents to use unique fonts\\nwith diﬀerent glyphs, which signiﬁcantly degrades the accuracy of OCR models\\ntrained on modern texts. In this document, a special ﬂat font is used for printing\\nnumbers and could not be detected by oﬀ-the-shelf OCR engines. Using the highly\\nﬂexible functionalities from LayoutParser, a pipeline approach is constructed\\nthat achieves a high recognition accuracy with minimal eﬀort. As the characters\\nhave unique visual structures and are usually clustered together, we train the\\nlayout model to identify number regions with a dedicated category. Subsequently,\\nLayoutParser crops images within these regions, and identiﬁes characters within\\nthem using a self-trained OCR model based on a CNN-RNN [6]. The model\\ndetects a total of 15 possible categories, and achieves a 0.98 Jaccard score16 and\\na 0.17 average Levinstein distances17 for token prediction on the test set.\\nOverall, it is possible to create an intricate and highly accurate digitization\\npipeline for large-scale digitization using LayoutParser. The pipeline avoids\\nspecifying the complicated rules used in traditional methods, is straightforward\\nto develop, and is robust to outliers. The DL models also generate ﬁne-grained\\nresults that enable creative approaches like page reorganization for OCR.\\n16 This measures the overlap between the detected and ground-truth characters, and\\nthe maximum is 1.\\n17 This measures the number of edits from the ground-truth text to the predicted text,\\nand lower is better.\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 12, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n13\\nFig. 6: This lightweight table detector can identify tables (outlined in red) and\\ncells (shaded in blue) in diﬀerent locations on a page. In very few cases (d), it\\nmight generate minor error predictions, e.g, failing to capture the top text line of\\na table.\\n5.2\\nA light-weight Visual Table Extractor\\nDetecting tables and parsing their structures (table extraction) are of central im-\\nportance for many document digitization tasks. Many previous works [26, 30, 27]\\nand tools 18 have been developed to identify and parse table structures. Yet they\\nmight require training complicated models from scratch, or are only applicable\\nfor born-digital PDF documents. In this section, we show how LayoutParser can\\nhelp build a light-weight accurate visual table extractor for legal docket tables\\nusing the existing resources with minimal eﬀort.\\nThe extractor uses a pre-trained layout detection model for identifying the\\ntable regions and some simple rules for pairing the rows and the columns in the\\nPDF image. Mask R-CNN [12] trained on the PubLayNet dataset [38] from the\\nLayoutParser Model Zoo can be used for detecting table regions. By ﬁltering\\nout model predictions of low conﬁdence and removing overlapping predictions,\\nLayoutParser can identify the tabular regions on each page, which signiﬁcantly\\nsimpliﬁes the subsequent steps. By applying the line detection functions within\\nthe tabular segments, provided in the utility module from LayoutParser, the\\npipeline can identify the three distinct columns in the tables. A row clustering\\nmethod is then applied via analyzing the y coordinates of token bounding boxes in\\nthe left-most column, which are obtained from the OCR engines. A non-maximal\\nsuppression algorithm is used to remove duplicated rows with extremely small\\ngaps. Shown in Figure 6, the built pipeline can detect tables at diﬀerent positions\\non a page accurately. Continued tables from diﬀerent pages are concatenated,\\nand a structured table representation has been easily created.\\n18 https://github.com/atlanhq/camelot, https://github.com/tabulapdf/tabula\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 13, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='14\\nZ. Shen et al.\\n6\\nConclusion\\nLayoutParser provides a comprehensive toolkit for deep learning-based document\\nimage analysis. The oﬀ-the-shelf library is easy to install, and can be used to\\nbuild ﬂexible and accurate pipelines for processing documents with complicated\\nstructures. It also supports high-level customization and enables easy labeling and\\ntraining of DL models on unique document image datasets. The LayoutParser\\ncommunity platform facilitates sharing DL models and DIA pipelines, inviting\\ndiscussion and promoting code reproducibility and reusability. The LayoutParser\\nteam is committed to keeping the library updated continuously and bringing\\nthe most recent advances in DL-based DIA, such as multi-modal document\\nmodeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.\\nAcknowledgements We thank the anonymous reviewers for their comments\\nand suggestions. This project is supported in part by NSF Grant OIA-2033558\\nand funding from the Harvard Data Science Initiative and Harvard Catalyst.\\nZejiang Shen thanks Doug Downey for suggestions.\\nReferences\\n[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,\\nG.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,\\nIrving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg,\\nJ., Man´e, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J.,\\nSteiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V.,\\nVi´egas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng,\\nX.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015),\\nhttps://www.tensorflow.org/, software available from tensorﬂow.org\\n[2] Alberti, M., Pondenkandath, V., W¨ursch, M., Ingold, R., Liwicki, M.: Deepdiva: a\\nhighly-functional python framework for reproducible experiments. In: 2018 16th\\nInternational Conference on Frontiers in Handwriting Recognition (ICFHR). pp.\\n423–428. IEEE (2018)\\n[3] Antonacopoulos, A., Bridson, D., Papadopoulos, C., Pletschacher, S.: A realistic\\ndataset for performance evaluation of document layout analysis. In: 2009 10th\\nInternational Conference on Document Analysis and Recognition. pp. 296–300.\\nIEEE (2009)\\n[4] Baek, Y., Lee, B., Han, D., Yun, S., Lee, H.: Character region awareness for text\\ndetection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. pp. 9365–9374 (2019)\\n[5] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale\\nHierarchical Image Database. In: CVPR09 (2009)\\n[6] Deng, Y., Kanervisto, A., Ling, J., Rush, A.M.: Image-to-markup generation with\\ncoarse-to-ﬁne attention. In: International Conference on Machine Learning. pp.\\n980–989. PMLR (2017)\\n[7] Ganin, Y., Lempitsky, V.: Unsupervised domain adaptation by backpropagation.\\nIn: International conference on machine learning. pp. 1180–1189. PMLR (2015)\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 14, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\\n15\\n[8] Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N., Peters,\\nM., Schmitz, M., Zettlemoyer, L.: Allennlp: A deep semantic natural language\\nprocessing platform. arXiv preprint arXiv:1803.07640 (2018)\\n[9]  Lukasz Garncarek, Powalski, R., Stanis lawek, T., Topolski, B., Halama, P.,\\nGrali´nski, F.: Lambert: Layout-aware (language) modeling using bert for in-\\nformation extraction (2020)\\n[10] Graves, A., Fern´andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal\\nclassiﬁcation: labelling unsegmented sequence data with recurrent neural networks.\\nIn: Proceedings of the 23rd international conference on Machine learning. pp.\\n369–376 (2006)\\n[11] Harley, A.W., Ufkes, A., Derpanis, K.G.: Evaluation of deep convolutional nets for\\ndocument image classiﬁcation and retrieval. In: 2015 13th International Conference\\non Document Analysis and Recognition (ICDAR). pp. 991–995. IEEE (2015)\\n[12] He, K., Gkioxari, G., Doll´ar, P., Girshick, R.: Mask r-cnn. In: Proceedings of the\\nIEEE international conference on computer vision. pp. 2961–2969 (2017)\\n[13] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition.\\npp. 770–778 (2016)\\n[14] Kay, A.: Tesseract: An open-source optical character recognition engine. Linux J.\\n2007(159), 2 (Jul 2007)\\n[15] Lamiroy, B., Lopresti, D.: An open architecture for end-to-end document analysis\\nbenchmarking. In: 2011 International Conference on Document Analysis and\\nRecognition. pp. 42–47. IEEE (2011)\\n[16] Lee, B.C., Weld, D.S.: Newspaper navigator: Open faceted search for 1.5\\nmillion images. In: Adjunct Publication of the 33rd Annual ACM Sym-\\nposium\\non\\nUser\\nInterface\\nSoftware\\nand\\nTechnology.\\np.\\n120–122.\\nUIST\\n’20 Adjunct, Association for Computing Machinery, New York, NY, USA\\n(2020). https://doi.org/10.1145/3379350.3416143, https://doi-org.offcampus.\\nlib.washington.edu/10.1145/3379350.3416143\\n[17] Lee, B.C.G., Mears, J., Jakeway, E., Ferriter, M., Adams, C., Yarasavage, N.,\\nThomas, D., Zwaard, K., Weld, D.S.: The Newspaper Navigator Dataset: Extracting\\nHeadlines and Visual Content from 16 Million Historic Newspaper Pages in\\nChronicling America, p. 3055–3062. Association for Computing Machinery, New\\nYork, NY, USA (2020), https://doi.org/10.1145/3340531.3412767\\n[18] Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: Table benchmark\\nfor image-based table detection and recognition. arXiv preprint arXiv:1903.01949\\n(2019)\\n[19] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,\\nZitnick, C.L.: Microsoft coco: Common objects in context. In: European conference\\non computer vision. pp. 740–755. Springer (2014)\\n[20] Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\\npattern recognition. pp. 3431–3440 (2015)\\n[21] Neudecker, C., Schlarb, S., Dogan, Z.M., Missier, P., Suﬁ, S., Williams, A., Wolsten-\\ncroft, K.: An experimental workﬂow development platform for historical document\\ndigitisation and analysis. In: Proceedings of the 2011 workshop on historical\\ndocument imaging and processing. pp. 161–168 (2011)\\n[22] Oliveira, S.A., Seguin, B., Kaplan, F.: dhsegment: A generic deep-learning approach\\nfor document segmentation. In: 2018 16th International Conference on Frontiers\\nin Handwriting Recognition (ICFHR). pp. 7–12. IEEE (2018)\\n'),\n",
       " Document(metadata={'source': 'docs/layoutparser_paper.pdf', 'file_path': 'docs/layoutparser_paper.pdf', 'page': 15, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}, page_content='16\\nZ. Shen et al.\\n[23] Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\\nDesmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch (2017)\\n[24] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,\\nT., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style,\\nhigh-performance deep learning library. arXiv preprint arXiv:1912.01703 (2019)\\n[25] Pletschacher, S., Antonacopoulos, A.: The page (page analysis and ground-truth\\nelements) format framework. In: 2010 20th International Conference on Pattern\\nRecognition. pp. 257–260. IEEE (2010)\\n[26] Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet:\\nAn approach for end to end table detection and structure recognition from image-\\nbased documents. In: Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops. pp. 572–573 (2020)\\n[27] Qasim, S.R., Mahmood, H., Shafait, F.: Rethinking table recognition using graph\\nneural networks. In: 2019 International Conference on Document Analysis and\\nRecognition (ICDAR). pp. 142–147. IEEE (2019)\\n[28] Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object\\ndetection with region proposal networks. In: Advances in neural information\\nprocessing systems. pp. 91–99 (2015)\\n[29] Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G.: The graph\\nneural network model. IEEE transactions on neural networks 20(1), 61–80 (2008)\\n[30] Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning\\nfor detection and structure recognition of tables in document images. In: 2017 14th\\nIAPR international conference on document analysis and recognition (ICDAR).\\nvol. 1, pp. 1162–1167. IEEE (2017)\\n[31] Shen, Z., Zhang, K., Dell, M.: A large dataset of historical japanese documents\\nwith complex layouts. In: Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition Workshops. pp. 548–549 (2020)\\n[32] Shen, Z., Zhao, J., Dell, M., Yu, Y., Li, W.: Olala: Object-level active learning\\nbased layout annotation. arXiv preprint arXiv:2010.01762 (2020)\\n[33] Studer, L., Alberti, M., Pondenkandath, V., Goktepe, P., Kolonko, T., Fischer,\\nA., Liwicki, M., Ingold, R.: A comprehensive study of imagenet pre-training for\\nhistorical document image analysis. In: 2019 International Conference on Document\\nAnalysis and Recognition (ICDAR). pp. 720–725. IEEE (2019)\\n[34] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\\nRault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s transformers: State-of-\\nthe-art natural language processing. arXiv preprint arXiv:1910.03771 (2019)\\n[35] Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https://\\ngithub.com/facebookresearch/detectron2 (2019)\\n[36] Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio, D., Zhang, C.,\\nChe, W., et al.: Layoutlmv2: Multi-modal pre-training for visually-rich document\\nunderstanding. arXiv preprint arXiv:2012.14740 (2020)\\n[37] Xu, Y., Li, M., Cui, L., Huang, S., Wei, F., Zhou, M.: Layoutlm: Pre-training of\\ntext and layout for document image understanding (2019)\\n[38] Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for doc-\\nument\\nlayout\\nanalysis.\\nIn:\\n2019\\nInternational\\nConference\\non\\nDocument\\nAnalysis\\nand\\nRecognition\\n(ICDAR).\\npp.\\n1015–1022.\\nIEEE\\n(Sep\\n2019).\\nhttps://doi.org/10.1109/ICDAR.2019.00166\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 0, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nAN IMAGE IS WORTH 16X16 WORDS:\\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\\nAlexey Dosovitskiy∗,†, Lucas Beyer∗, Alexander Kolesnikov∗, Dirk Weissenborn∗,\\nXiaohua Zhai∗, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby∗,†\\n∗equal technical contribution, †equal advising\\nGoogle Research, Brain Team\\n{adosovitskiy, neilhoulsby}@google.com\\nABSTRACT\\nWhile the Transformer architecture has become the de-facto standard for natural\\nlanguage processing tasks, its applications to computer vision remain limited. In\\nvision, attention is either applied in conjunction with convolutional networks, or\\nused to replace certain components of convolutional networks while keeping their\\noverall structure in place. We show that this reliance on CNNs is not necessary\\nand a pure transformer applied directly to sequences of image patches can perform\\nvery well on image classiﬁcation tasks. When pre-trained on large amounts of\\ndata and transferred to multiple mid-sized or small image recognition benchmarks\\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\\nresults compared to state-of-the-art convolutional networks while requiring sub-\\nstantially fewer computational resources to train.1\\n1\\nINTRODUCTION\\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\\na large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019). Thanks\\nto Transformers’ computational efﬁciency and scalability, it has become possible to train models of\\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\\nmodels and datasets growing, there is still no sign of saturating performance.\\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\\ntheoretically efﬁcient, have not yet been scaled effectively on modern hardware accelerators due to\\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\\n2020).\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modiﬁcations. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classiﬁcation in supervised fashion.\\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\\n1Fine-tuning\\ncode\\nand\\npre-trained\\nmodels\\nare\\navailable\\nat\\nhttps://github.com/\\ngoogle-research/vision_transformer\\n1\\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 1, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\\nwhen trained on insufﬁcient amounts of data.\\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\\nﬁnd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\\nresults when pre-trained at sufﬁcient scale and transferred to tasks with fewer datapoints. When\\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\\nand 77.63% on the VTAB suite of 19 tasks.\\n2\\nRELATED WORK\\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\\npre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019)\\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\\nNaive application of self-attention to images would require that each pixel attends to every other\\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\\nto apply Transformers in the context of image processing, several approximations have been tried in\\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\\npromising results on computer vision tasks, but require complex engineering to be implemented\\nefﬁciently on hardware accelerators.\\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 × 2\\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\\nuse a small patch size of 2 × 2 pixels, which makes the model applicable only to small-resolution\\nimages, while we handle medium-resolution images as well.\\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\\nof self-attention, e.g. by augmenting feature maps for image classiﬁcation (Bello et al., 2019) or by\\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu\\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen\\net al., 2020c; Lu et al., 2019; Li et al., 2019).\\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\\npervised fashion as a generative model, and the resulting representation can then be ﬁne-tuned or\\nprobed linearly for classiﬁcation performance, achieving a maximal accuracy of 72% on ImageNet.\\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\\nwell, but train Transformers instead of ResNet-based models used in prior works.\\n2\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 2, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nTransformer Encoder\\nMLP \\nHead\\nVision Transformer (ViT)\\n*\\nLinear Projection of Flattened Patches\\n* Extra learnable\\n     [ cl ass]  embedding\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n0\\nPatch + Position \\nEmbedding\\nClass\\nBird\\nBall\\nCar\\n...\\nEmbedded \\nPatches\\nMulti-Head \\nAttention\\nNorm\\nMLP\\nNorm\\n+\\nL x\\n+\\nTransformer Encoder\\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\\nVaswani et al. (2017).\\n3\\nMETHOD\\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and\\ntheir efﬁcient implementations – can be used almost out of the box.\\n3.1\\nVISION TRANSFORMER (VIT)\\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W ×C into a\\nsequence of ﬂattened 2D patches xp ∈RN×(P 2·C), where (H, W) is the resolution of the original\\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\\nis the resulting number of patches, which also serves as the effective input sequence length for the\\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\\nﬂatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\\nthe output of this projection as the patch embeddings.\\nSimilar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embed-\\nded patches (z0\\n0 = xclass), whose state at the output of the Transformer encoder (z0\\nL) serves as the\\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\\ntached to z0\\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\\ntime and by a single linear layer at ﬁne-tuning time.\\nPosition embeddings are added to the patch embeddings to retain positional information. We use\\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\\nsequence of embedding vectors serves as input to the encoder.\\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\\n3\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 3, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nThe MLP contains two layers with a GELU non-linearity.\\nz0 = [xclass; x1\\npE; x2\\npE; · · · ; xN\\np E] + Epos,\\nE ∈R(P 2·C)×D, Epos ∈R(N+1)×D\\n(1)\\nz′\\nℓ= MSA(LN(zℓ−1)) + zℓ−1,\\nℓ= 1 . . . L\\n(2)\\nzℓ= MLP(LN(z′\\nℓ)) + z′\\nℓ,\\nℓ= 1 . . . L\\n(3)\\ny = LN(z0\\nL)\\n(4)\\nInductive bias.\\nWe note that Vision Transformer has much less image-speciﬁc inductive bias than\\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\\nat ﬁne-tuning time for adjusting the position embeddings for images of different resolution (as de-\\nscribed below). Other than that, the position embeddings at initialization time carry no information\\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\\nfrom scratch.\\nHybrid Architecture.\\nAs an alternative to raw image patches, the input sequence can be formed\\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\\nﬂattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\\nThe classiﬁcation input embedding and position embeddings are added as described above.\\n3.2\\nFINE-TUNING AND HIGHER RESOLUTION\\nTypically, we pre-train ViT on large datasets, and ﬁne-tune to (smaller) downstream tasks. For\\nthis, we remove the pre-trained prediction head and attach a zero-initialized D × K feedforward\\nlayer, where K is the number of downstream classes. It is often beneﬁcial to ﬁne-tune at higher\\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\\n2D interpolation of the pre-trained position embeddings, according to their location in the original\\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\\n4\\nEXPERIMENTS\\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\\nthat self-supervised ViT holds promise for the future.\\n4.1\\nSETUP\\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\\nfollows Kolesnikov et al. (2020).\\n4\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 4, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nModel\\nLayers\\nHidden size D\\nMLP size\\nHeads\\nParams\\nViT-Base\\n12\\n768\\n3072\\n12\\n86M\\nViT-Large\\n24\\n1024\\n4096\\n16\\n307M\\nViT-Huge\\n32\\n1280\\n5120\\n16\\n632M\\nTable 1: Details of Vision Transformer model variants.\\nWe also evaluate on the 19-task VTAB classiﬁcation suite (Zhai et al., 2019b). VTAB evaluates\\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\\nthree groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite\\nimagery, and Structured – tasks that require geometric understanding like localization.\\nModel Variants. We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as\\nsummarized in Table 1. The “Base” and “Large” models are directly adopted from BERT and we\\nadd the larger “Huge” model. In what follows we use brief notation to indicate the model size and\\nthe input patch size: for instance, ViT-L/16 means the “Large” variant with 16×16 input patch size.\\nNote that the Transformer’s sequence length is inversely proportional to the square of the patch size,\\nthus models with smaller patch size are computationally more expensive.\\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\\nconvolutions (Qiao et al., 2019). These modiﬁcations improve transfer (Kolesnikov et al., 2020),\\nand we denote the modiﬁed model “ResNet (BiT)”. For the hybrids, we feed the intermediate fea-\\nture maps into ViT with patch size of one “pixel”. To experiment with different sequence lengths,\\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\\n2015) with β1 = 0.9, β2 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\\nrate warmup and decay, see Appendix B.1 for details. For ﬁne-tuning we use SGD with momentum,\\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we ﬁne-tuned at\\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\\nMetrics. We report results on downstream datasets either through few-shot or ﬁne-tuning accuracy.\\nFine-tuning accuracies capture the performance of each model after ﬁne-tuning it on the respective\\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\\nthat maps the (frozen) representation of a subset of training images to {−1, 1}K target vectors. This\\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\\nﬁne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-ﬂy evaluation\\nwhere ﬁne-tuning would be too costly.\\n4.2\\nCOMPARISON TO STATE OF THE ART\\nWe ﬁrst compare our largest models – ViT-H/14 and ViT-L/16 – to state-of-the-art CNNs from\\nthe literature. The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\\n2020), which is a large EfﬁcientNet trained using semi-supervised learning on ImageNet and JFT-\\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\\nv3 cores (2 per chip) used for training multiplied by the training time in days.\\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\\non the more challenging datasets – ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\\n5\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 5, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nOurs-JFT\\nOurs-JFT\\nOurs-I21k\\nBiT-L\\nNoisy Student\\n(ViT-H/14)\\n(ViT-L/16)\\n(ViT-L/16)\\n(ResNet152x4)\\n(EfﬁcientNet-L2)\\nImageNet\\n88.55 ± 0.04\\n87.76 ± 0.03\\n85.30 ± 0.02\\n87.54 ± 0.02\\n88.4/88.5∗\\nImageNet ReaL\\n90.72 ± 0.05\\n90.54 ± 0.03\\n88.62 ± 0.05\\n90.54\\n90.55\\nCIFAR-10\\n99.50 ± 0.06\\n99.42 ± 0.03\\n99.15 ± 0.03\\n99.37 ± 0.06\\n−\\nCIFAR-100\\n94.55 ± 0.04\\n93.90 ± 0.05\\n93.25 ± 0.05\\n93.51 ± 0.08\\n−\\nOxford-IIIT Pets\\n97.56 ± 0.03\\n97.32 ± 0.11\\n94.67 ± 0.15\\n96.62 ± 0.23\\n−\\nOxford Flowers-102\\n99.68 ± 0.02\\n99.74 ± 0.00\\n99.61 ± 0.02\\n99.63 ± 0.03\\n−\\nVTAB (19 tasks)\\n77.63 ± 0.23\\n76.28 ± 0.46\\n72.72 ± 0.21\\n76.29 ± 1.70\\n−\\nTPUv3-core-days\\n2.5k\\n0.68k\\n0.23k\\n9.9k\\n12.3k\\nTable 2:\\nComparison with state of the art on popular image classiﬁcation benchmarks. We re-\\nport mean and standard deviation of the accuracies, averaged over three ﬁne-tuning runs. Vision\\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\\nsmaller public ImageNet-21k dataset performs well too. ∗Slightly improved 88.5% result reported\\nin Touvron et al. (2020).\\nVTAB (19 tasks)\\n65\\n70\\n75\\n80\\nAccuracy [%]\\nNatural (7 tasks)\\n70\\n80\\n90\\nSpecialized (4 tasks)\\n80\\n82\\n85\\n88\\n90\\nStructured (8 tasks)\\n50\\n60\\n70\\nViT-H/14\\nBiT-L (R152x4)\\nVIVI-Ex-100% (R50x3)\\nS4L (R50x1)\\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\\nthat pre-training efﬁciency may be affected not only by the architecture choice, but also other pa-\\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\\nproximately 30 days.\\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\\nmethods on this benchmark: BiT, VIVI – a ResNet co-trained on ImageNet and Youtube (Tschannen\\net al., 2020), and S4L – supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\\nSpecialized the performance of the top two models is similar.\\n4.3\\nPRE-TRAINING DATA REQUIREMENTS\\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\\nexperiments.\\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\\nparameters – weight decay, dropout, and label smoothing. Figure 3 shows the results after ﬁne-\\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\\nwith JFT-300M, do we see the full beneﬁt of larger models. Figure 3 also shows the performance\\n2Note that the ImageNet pre-trained models are also ﬁne-tuned, but again on ImageNet. This is because the\\nresolution increase during ﬁne-tuning improves the performance.\\n6\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 6, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nImageNet\\nImageNet-21k\\nJFT-300M\\nPre-training dataset\\n70\\n75\\n80\\n85\\n90\\nImageNet Top1 Accuracy [%]\\nBiT\\nViT-B/32\\nViT-B/16\\nViT-L/32\\nViT-L/16\\nViT-H/14\\nFigure 3:\\nTransfer to ImageNet.\\nWhile\\nlarge ViT models perform worse than BiT\\nResNets (shaded area) when pre-trained on\\nsmall datasets, they shine when pre-trained on\\nlarger datasets. Similarly, larger ViT variants\\novertake smaller ones as the dataset grows.\\n10 M\\n30 M\\n100 M\\n300 M\\nNumber of JFT pre-training samples\\n30\\n40\\n50\\n60\\n70\\nLinear 5-shot ImageNet Top1 [%]\\nViT-L/16\\nViT-L/32\\nViT-B/32\\nViT-b/32\\nResNet50x1 (BiT)\\nResNet152x2 (BiT)\\nFigure 4: Linear few-shot evaluation on Ima-\\ngeNet versus pre-training size. ResNets per-\\nform better with smaller pre-training datasets\\nbut plateau sooner than ViT, which performs\\nbetter with larger pre-training. ViT-b is ViT-B\\nwith all hidden dimensions halved.\\n102\\n103\\n90\\n95\\nTransfer accuracy [%]\\nAverage-5\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\n102\\n103\\n75\\n80\\n85\\n90\\nImageNet\\nTransformer (ViT)\\nResNet (BiT)\\nHybrid\\nTotal pre-training compute [exaFLOPs]\\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\\nvanishes for larger models.\\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\\nwith the larger datasets, ViT overtakes.\\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\\nachieved during training. To save compute, we report few-shot linear accuracy instead of full ﬁne-\\ntuning accuracy. Figure 4 contains the results. Vision Transformers overﬁt more than ResNets with\\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\\ndata is sufﬁcient, even beneﬁcial.\\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\\nis an exciting direction of future work.\\n7\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 7, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n4.4\\nSCALING STUDY\\nWe perform a controlled scaling study of different models by evaluating transfer performance from\\nJFT-300M. In this setting data size does not bottleneck the models’ performances, and we assess\\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\\nbackbone).\\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\\nperformance/compute trade-off. ViT uses approximately 2 −4× less compute to attain the same\\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\\n4.5\\nINSPECTING VISION TRANSFORMER\\nInput\\nAttention\\nFigure 6: Representative ex-\\namples of attention from the\\noutput token to the input\\nspace. See Appendix D.7 for\\ndetails.\\nTo begin to understand how the Vision Transformer processes im-\\nage data, we analyze its internal representations. The ﬁrst layer of\\nthe Vision Transformer linearly projects the ﬂattened patches into a\\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\\ncipal components of the the learned embedding ﬁlters. The com-\\nponents resemble plausible basis functions for a low-dimensional\\nrepresentation of the ﬁne structure within each patch.\\nAfter the projection, a learned position embedding is added to the\\npatch representations. Figure 7 (center) shows that the model learns\\nto encode distance within the image in the similarity of position em-\\nbeddings, i.e. closer patches tend to have more similar position em-\\nbeddings. Further, the row-column structure appears; patches in the\\nsame row/column have similar embeddings. Finally, a sinusoidal\\nstructure is sometimes apparent for larger grids (Appendix D). That\\nthe position embeddings learn to represent 2D image topology ex-\\nplains why hand-crafted 2D-aware embedding variants do not yield\\nimprovements (Appendix D.4).\\nSelf-attention allows ViT to integrate information across the entire\\nimage even in the lowest layers. We investigate to what degree\\nthe network makes use of this capability. Speciﬁcally, we compute\\nthe average distance in image space across which information is\\nintegrated, based on the attention weights (Figure 7, right). This\\n“attention distance” is analogous to receptive ﬁeld size in CNNs.\\nWe ﬁnd that some heads attend to most of the image already in the lowest layers, showing that\\nthe ability to integrate information globally is indeed used by the model. Other attention heads\\nhave consistently small attention distances in the low layers. This highly localized attention is\\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\\nattention distance increases with network depth. Globally, we ﬁnd that the model attends to image\\nregions that are semantically relevant for classiﬁcation (Figure 6).\\n4.6\\nSELF-SUPERVISION\\nTransformers show impressive performance on NLP tasks. However, much of their success stems\\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\\n8\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nRGB embedding filters\\n(first 28 principal components)\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nInput patch row\\nPosition embedding similarity\\n1\\n1\\nCosine similarity\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\\nembedding of the patch with the indicated row and column and the position embeddings of all other\\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\\nsigniﬁcant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\\net al., 2020b; He et al., 2020; Bachman et al., 2019; H´enaff et al., 2020) to future work.\\n5\\nCONCLUSION\\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\\nusing self-attention in computer vision, we do not introduce image-speciﬁc inductive biases into\\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\\nThus, Vision Transformer matches or exceeds the state of the art on many image classiﬁcation\\ndatasets, whilst being relatively cheap to pre-train.\\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\\nACKNOWLEDGEMENTS\\nThe work was performed in Berlin, Z¨urich, and Amsterdam. We thank many colleagues at Google\\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Luˇci´c, Noam\\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\\nREFERENCES\\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In ACL, 2020.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\\nmutual information across views. In NeurIPS, 2019.\\n9\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 9, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\\nICLR, 2019.\\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\\nIn ICCV, 2019.\\nLucas Beyer, Olivier J. H´enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨aron van den Oord. Are\\nwe done with imagenet? arXiv, 2020.\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. arXiv, 2020.\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\\npixels. In ICML, 2020a.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\\nfor contrastive learning of visual representations. In ICML, 2020b.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\\ntransformers. arXiv, 2019.\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\\nattention and convolutional layers. In ICLR, 2020.\\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\\nimage database. In CVPR, 2009.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In NAACL, 2019.\\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D’Amour, Dan Moldovan, Sylvan\\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\\nlutional neural networks. arXiv, 2020.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\\nnition. In CVPR, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\\nMomentum contrast for\\nunsupervised visual representation learning. In CVPR, 2020.\\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\\nmensional transformers. arXiv, 2019.\\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In CVPR, 2018.\\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\\nIn ICCV, 2019.\\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\\nOlivier J. H´enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\\nand Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. In\\nICML, 2020.\\n10\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 10, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. 2015.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convo-\\nlutional neural networks. In NIPS, 2012.\\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\\ngation applied to handwritten zip code recognition. Neural Computation, 1:541–551, 1989.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding. arXiv, 2020.\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\\ntion. arXiv, 2020.\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\\nAshwin Bharambe, and Laurens van der Maaten.\\nExploring the limits of weakly supervised\\npretraining. In ECCV, 2018.\\nM. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\\nICVGIP, 2008.\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\\n2012.\\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\\nDustin Tran. Image transformer. In ICML, 2018.\\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\\nJournal on Control and Optimization, 30(4):838–855, 1992.\\ndoi: 10.1137/0330046.\\nURL\\nhttps://doi.org/10.1137/0330046.\\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\\npreprint arXiv:1903.10520, 2019.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\\nstanding with unsupervised learning. Technical Report, 2018.\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nmodels are unsupervised multitask learners. Technical Report, 2019.\\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\\nStand-alone self-attention in vision models. In NeurIPS, 2019.\\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\\nfectiveness of data in deep learning era. In ICCV, 2017.\\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\\nmodel for video and language representation learning. In ICCV, 2019.\\n11\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 11, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy. In NeurIPS. 2019.\\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\\ndiscrepancy: Fixefﬁcientnet. arXiv preprint arXiv:2003.08237, 2020.\\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\\n2020.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\\nChen.\\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\\narXiv preprint\\narXiv:2003.07853, 2020b.\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\\nLearning deep transformer models for machine translation. In ACL, 2019.\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\\nCVPR, 2018.\\nDirk Weissenborn, Oscar T¨ackstr¨om, and Jakob Uszkoreit. Scaling autoregressive video models. In\\nICLR, 2019.\\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\\nfor computer vision. arxiv, 2020.\\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\\nimproves imagenet classiﬁcation. In CVPR, 2020.\\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\\nSupervised Learning. In ICCV, 2019a.\\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\\npreprint arXiv:1910.04867, 2019b.\\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\\nCVPR, 2020.\\n12\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nModels\\nDataset\\nEpochs\\nBase LR\\nLR decay\\nWeight decay\\nDropout\\nViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/32\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-H/14\\nJFT-300M\\n14\\n3 · 10−4\\nlinear\\n0.1\\n0.0\\nR50x{1,2}\\nJFT-300M\\n7\\n10−3\\nlinear\\n0.1\\n0.0\\nR101x1\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR152x{1,2}\\nJFT-300M\\n7\\n6 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-B/{16,32}\\nJFT-300M\\n7\\n8 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/32\\nJFT-300M\\n7\\n2 · 10−4\\nlinear\\n0.1\\n0.0\\nR50+ViT-L/16\\nJFT-300M\\n7/14\\n4 · 10−4\\nlinear\\n0.1\\n0.0\\nViT-B/{16,32}\\nImageNet-21k\\n90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-L/{16,32}\\nImageNet-21k\\n30/90\\n10−3\\nlinear\\n0.03\\n0.1\\nViT-∗\\nImageNet\\n300\\n3 · 10−3\\ncosine\\n0.3\\n0.1\\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\\ning rate warmup of 10k steps. For ImageNet we found it beneﬁcial to additionally apply gradient\\nclipping at global norm 1. Training resolution is 224.\\nAPPENDIX\\nA\\nMULTIHEAD SELF-ATTENTION\\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\\ntectures. For each element in an input sequence z ∈RN×D, we compute a weighted sum over all\\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\\ntwo elements of the sequence and their respective query qi and key kj representations.\\n[q, k, v] = zUqkv\\nUqkv ∈RD×3Dh,\\n(5)\\nA = softmax\\n\\x10\\nqk⊤/\\np\\nDh\\n\\x11\\nA ∈RN×N,\\n(6)\\nSA(z) = Av .\\n(7)\\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\\ncalled “heads”, in parallel, and project their concatenated outputs. To keep compute and number of\\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\\nMSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa\\nUmsa ∈Rk·Dh×D\\n(8)\\nB\\nEXPERIMENT DETAILS\\nB.1\\nTRAINING\\nTable 3 summarizes our training setups for our different models. We found strong regularization\\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\\ntraining is done on resolution 224.\\nB.1.1\\nFINE-TUNING\\nWe ﬁne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\\nremaining data. For ﬁnal results we train on the entire training set and evaluate on the respective\\ntest data. For ﬁne-tuning ResNets and hybrid models we use the exact same setup, with the only\\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\\n13\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nDataset\\nSteps\\nBase LR\\nImageNet\\n20 000\\n{0.003, 0.01, 0.03, 0.06}\\nCIFAR100\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nCIFAR10\\n10 000\\n{0.001, 0.003, 0.01, 0.03}\\nOxford-IIIT Pets\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nOxford Flowers-102\\n500\\n{0.001, 0.003, 0.01, 0.03}\\nVTAB (19 tasks)\\n2 500\\n0.01\\nTable 4: Hyperparameters for ﬁne-tuning. All models are ﬁne-tuned with cosine learning rate decay,\\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\\nﬁne-tuning resolution is 384.\\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\\nthis run and our sweep. Finally, if not mentioned otherwise, all ﬁne-tuning experiments run at 384\\nresolution (running ﬁne-tuning at different resolution than training is common practice (Kolesnikov\\net al., 2020)).\\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\\nin Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions. Instead we ﬁnd\\nthat Vision Transformer beneﬁts most from a high resolution (384 × 384) for all tasks.\\nB.1.2\\nSELF-SUPERVISION\\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\\npatch representations.\\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\\nuse Adam, with a base learning rate of 2·10−4, warmup of 10k steps and cosine learning rate decay.\\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 × 4 downsized version of the 16 × 16\\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\\nwell, though L2 was slightly worse. We report ﬁnal results only for option 1) because it has shown\\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\\n(2019) but results were also slightly worse on our few-shot metrics.\\nLastly, we would like to remark that our instantiation of masked patch prediction doesn’t require\\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\\nilar performance gains on ImageNet classiﬁcation. That is, we observed diminishing returns on\\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\\nImageNet.\\nC\\nADDITIONAL RESULTS\\nWe report detailed results corresponding to the ﬁgures presented in the paper. Table 5 corresponds\\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\\n14\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 14, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nViT-B/16\\nViT-B/32\\nViT-L/16\\nViT-L/32\\nViT-H/14\\nImageNet\\nCIFAR-10\\n98.13\\n97.77\\n97.86\\n97.94\\n-\\nCIFAR-100\\n87.13\\n86.31\\n86.35\\n87.07\\n-\\nImageNet\\n77.91\\n73.38\\n76.53\\n71.16\\n-\\nImageNet ReaL\\n83.57\\n79.56\\n82.19\\n77.83\\n-\\nOxford Flowers-102\\n89.49\\n85.43\\n89.66\\n86.36\\n-\\nOxford-IIIT-Pets\\n93.81\\n92.04\\n93.64\\n91.35\\n-\\nImageNet-21k\\nCIFAR-10\\n98.95\\n98.79\\n99.16\\n99.13\\n99.27\\nCIFAR-100\\n91.67\\n91.97\\n93.44\\n93.04\\n93.82\\nImageNet\\n83.97\\n81.28\\n85.15\\n80.99\\n85.13\\nImageNet ReaL\\n88.35\\n86.63\\n88.40\\n85.65\\n88.70\\nOxford Flowers-102\\n99.38\\n99.11\\n99.61\\n99.19\\n99.51\\nOxford-IIIT-Pets\\n94.43\\n93.02\\n94.73\\n93.09\\n94.82\\nJFT-300M\\nCIFAR-10\\n99.00\\n98.61\\n99.38\\n99.19\\n99.50\\nCIFAR-100\\n91.87\\n90.49\\n94.04\\n92.52\\n94.55\\nImageNet\\n84.15\\n80.73\\n87.12\\n84.37\\n88.04\\nImageNet ReaL\\n88.85\\n86.27\\n89.99\\n88.28\\n90.33\\nOxford Flowers-102\\n99.56\\n99.27\\n99.56\\n99.45\\n99.68\\nOxford-IIIT-Pets\\n95.80\\n93.40\\n97.11\\n95.83\\n97.56\\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\\nare ﬁne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\\nEpochs\\nImageNet\\nImageNet ReaL\\nCIFAR-10\\nCIFAR-100\\nPets\\nFlowers\\nexaFLOPs\\nname\\nViT-B/32\\n7\\n80.73\\n86.27\\n98.61\\n90.49\\n93.40\\n99.27\\n55\\nViT-B/16\\n7\\n84.15\\n88.85\\n99.00\\n91.87\\n95.80\\n99.56\\n224\\nViT-L/32\\n7\\n84.37\\n88.28\\n99.19\\n92.52\\n95.83\\n99.45\\n196\\nViT-L/16\\n7\\n86.30\\n89.43\\n99.38\\n93.46\\n96.81\\n99.66\\n783\\nViT-L/16\\n14\\n87.12\\n89.99\\n99.38\\n94.04\\n97.11\\n99.56\\n1567\\nViT-H/14\\n14\\n88.08\\n90.36\\n99.50\\n94.71\\n97.11\\n99.71\\n4262\\nResNet50x1\\n7\\n77.54\\n84.56\\n97.67\\n86.07\\n91.11\\n94.26\\n50\\nResNet50x2\\n7\\n82.12\\n87.94\\n98.29\\n89.20\\n93.43\\n97.02\\n199\\nResNet101x1\\n7\\n80.67\\n87.07\\n98.48\\n89.17\\n94.08\\n95.95\\n96\\nResNet152x1\\n7\\n81.88\\n87.96\\n98.82\\n90.22\\n94.17\\n96.94\\n141\\nResNet152x2\\n7\\n84.97\\n89.69\\n99.06\\n92.05\\n95.37\\n98.62\\n563\\nResNet152x2\\n14\\n85.56\\n89.89\\n99.24\\n91.92\\n95.75\\n98.75\\n1126\\nResNet200x3\\n14\\n87.22\\n90.15\\n99.34\\n93.53\\n96.32\\n99.04\\n3306\\nR50x1+ViT-B/32\\n7\\n84.90\\n89.15\\n99.01\\n92.24\\n95.75\\n99.46\\n106\\nR50x1+ViT-B/16\\n7\\n85.58\\n89.65\\n99.14\\n92.63\\n96.65\\n99.40\\n274\\nR50x1+ViT-L/32\\n7\\n85.68\\n89.04\\n99.24\\n92.93\\n96.97\\n99.43\\n246\\nR50x1+ViT-L/16\\n7\\n86.60\\n89.72\\n99.18\\n93.64\\n97.03\\n99.40\\n859\\nR50x1+ViT-L/16\\n14\\n87.12\\n89.76\\n99.31\\n93.89\\n97.36\\n99.11\\n1668\\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\\naFLOPs).\\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\\nvarying size, as well as the estimated computational cost of their pre-training.\\nD\\nADDITIONAL ANALYSES\\nD.1\\nSGD VS. ADAM FOR RESNETS\\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\\nHere we show the experiments that motivated this choice. Namely, we compare the ﬁne-tuning\\n15\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 15, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nResNet50\\nResNet152x2\\nDataset\\nAdam\\nSGD\\nAdam\\nSGD\\nImageNet\\n77.54\\n78.24\\n84.97\\n84.37\\nCIFAR10\\n97.67\\n97.46\\n99.06\\n99.07\\nCIFAR100\\n86.07\\n85.17\\n92.05\\n91.06\\nOxford-IIIT Pets\\n91.11\\n91.00\\n95.37\\n94.79\\nOxford Flowers-102\\n94.26\\n92.06\\n98.62\\n99.32\\nAverage\\n89.33\\n88.79\\n94.01\\n93.72\\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\\n100\\n101\\nRelative Compute\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nImageNet 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\n100\\n101\\nRelative Compute\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\nAverage 5shot\\nModels\\nAll\\nDepth\\nPatch size\\nWidth MLP\\nWidth\\nFigure 8: Scaling different model dimensions of the Vision Transformer.\\nperformance of two ResNets – 50x1 and 152x2 – pre-trained on JFT with SGD and Adam. For\\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\\nThis justiﬁes the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\\nfor 7 epochs, not 30.\\nD.2\\nTRANSFORMER SHAPE\\nWe ran ablations on scaling different dimensions of the Transformer architecture to ﬁnd out which\\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\\nfor different conﬁgurations. All conﬁgurations are based on a ViT model with 8 layers, D = 1024,\\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\\neffective sequence length shows surprisingly robust improvements without introducing parameters.\\nThese ﬁndings suggest that compute might be a better predictor of performance than the number of\\nparameters, and that scaling should emphasize depth over width if any. Overall, we ﬁnd that scaling\\nall dimensions proportionally results in robust improvements.\\nD.3\\nHEAD TYPE AND CLASS TOKEN\\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\\n[class] token, which is taken as image representation. The output of this token is then trans-\\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\\nin the single hidden layer.\\nThis design is inherited from the Transformer model for text, and we use it throughout the main\\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\\nthem, followed by a linear classiﬁer—just like ResNet’s ﬁnal feature map—performed very poorly.\\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\\n16\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 16, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nEpochs of training\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\nImageNet linear 5-shot accuracy [%]\\nCLS-Token, lr=8e-4\\nGAP, lr=8e-4\\nGAP, lr=3e-4\\nFigure 9: Comparison of class-token and global average pooling classiﬁers. Both work similarly\\nwell, but require different learning-rates.\\nPos. Emb.\\nDefault/Stem\\nEvery Layer\\nEvery Layer-Shared\\nNo Pos. Emb.\\n0.61382\\nN/A\\nN/A\\n1-D Pos. Emb.\\n0.64206\\n0.63964\\n0.64292\\n2-D Pos. Emb.\\n0.64001\\n0.64046\\n0.64022\\nRel. Pos. Emb.\\n0.64032\\nN/A\\nN/A\\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\\nImageNet 5-shot linear.\\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\\nFigure 9.\\nD.4\\nPOSITIONAL EMBEDDING\\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\\ntried the following cases:\\n• Providing no positional information: Considering the inputs as a bag of patches.\\n• 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\\nthe raster order (default across all other experiments in this paper).\\n• 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\\nthe path in the input, we concatenate the X and Y embedding to get the ﬁnal positional\\nembedding for that patch.\\n• Relative positional embeddings: Considering the relative distance between patches to en-\\ncode the spatial information as instead of their absolute position. To do so, we use 1-\\ndimensional Relative Attention, in which we deﬁne the relative distance all possible pairs\\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\\ntention mechanism), we have an offset pq −pk, where each offset is associated with an\\nembedding. Then, we simply run extra attention, where we use the original query (the\\ncontent of query), but use relative positional embeddings as keys. We then use the log-\\nits from the relative attention as a bias term and add it to the logits of the main attention\\n(content-based attention) before applying the softmax.\\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\\n17\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 17, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0002, WD=0.01\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n7 epochs, LR=0.0004, WD=0.1\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10 11 12 13 14\\nInput patch column\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\nInput patch row\\nViT-L16\\n14 epochs, LR=0.0004, WD=0.1\\n1\\n1\\nCosine similarity\\nFigure 10: Position embeddings of models trained with different hyperparameters.\\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\\neach layer (shared between layers).\\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\\nthere is a large gap between the performances of the model with no positional embedding and mod-\\nels with positional embedding, there is little to no difference between different ways of encoding\\npositional information. We speculate that since our Transformer encoder operates on patch-level\\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\\npixel-level inputs, e.g., 14 × 14 as opposed to 224 × 224, and learning to represent the spatial re-\\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\\nthe speciﬁc pattern of position embedding similarity learned by the network depends on the training\\nhyperparameters (Figure 10).\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nMean attention distance (pixels)\\nViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\n0\\n5\\n10\\n15\\n20\\nNetwork depth (layer)\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nR50x1 + ViT-L/16\\nHead 1\\nHead 2\\nHead 3\\n...\\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\\nheads at one layer. Image width is 224 pixels.\\nD.5\\nEMPIRICAL COMPUTATIONAL COSTS\\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\\n18\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 18, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\\ndifference between inference and backprop speed is a constant model-independent factor.\\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\\nfor the largest models at the largest resolutions.\\nAnother quantity of interest is the largest batch-size each model can ﬁt onto a core, larger being\\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\\nThis shows that large ViT models have a clear advantage in terms of memory-efﬁciency over ResNet\\nmodels.\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\n104\\nPeak inference speed [img/sec/core]\\n64\\n128\\n224\\n384\\n512\\nInput size [px]\\n102\\n103\\nLargest per-core batch-size\\nR50x1\\nR50x2\\nViT-B/32\\nViT-L/32\\nViT-B/16\\nViT-L/16\\nViT-H/14\\nR152x4\\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size ﬁtting on device with\\nvarious architectures across input sizes. ViT models are clearly more memory-efﬁcient.\\nD.6\\nAXIAL ATTENTION\\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\\ninstead of applying 1-dimensional attention to the ﬂattened version of the input. In axial attention,\\neach attention mixes information along a particular axis, while keeping information along the other\\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\\nall the convolutions with kernel size 3 × 3 in a ResNet50 are replaced by axial self-attention, i.e.\\na row and column attention, augmented by relative positional encoding. We have implemented\\nAxialResNet as a baseline model.3.\\nMoreover, we have modiﬁed ViT to process inputs in the 2-dimensional shape, instead of a 1-\\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\\ncolumn-self-attention plus an MLP.\\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\\nunlocked by a carefully optimized implementation.\\n19\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 19, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n102\\nTotal compute [exaFLOPs]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\n102\\n103\\nPeak inference speed [img/sec/core]\\n0.500\\n0.525\\n0.550\\n0.575\\n0.600\\n0.625\\n0.650\\nImageNet 5-shot linear top-1 accuracy\\nAxialViT-B/16\\nAxialViT-B/32\\nViT-B/16\\nViT-B/32\\nResNet50\\nAxialResNet50\\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\\nattention and although the sequence length that self-attention operates on is smaller in axial case,\\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\\non TPUs (Figure 13, right).\\nD.7\\nATTENTION DISTANCE\\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\\nthe average distance spanned by attention weights at different layers (Figure 11). This “attention\\ndistance” is analogous to receptive ﬁeld size in CNNs. Average attention distance is highly variable\\nacross heads in lower layers, with some heads attending to much of the image, while others attend\\nto small regions at or near the query location. As depth increases, attention distance increases for all\\nheads. In the second half of the network, most heads attend widely across tokens.\\nD.8\\nATTENTION MAPS\\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\\nused Attention Rollout (Abnar & Zuidema, 2020). Brieﬂy, we averaged attention weights of ViT-\\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\\nfor the mixing of attention across tokens through all layers.\\nD.9\\nOBJECTNET RESULTS\\nWe also evaluate our ﬂagship ViT-H/14 model on the ObjectNet benchmark following the evaluation\\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\\nD.10\\nVTAB BREAKDOWN\\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\\n20\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 20, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\n88\\n89\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n101\\n102\\n103\\n104\\n105\\n106\\n107\\n108\\n109\\n110\\n111\\n112\\n113\\n114\\n115\\n116\\n117\\n118\\n119\\n120\\n121\\n122\\n123\\n124\\n125\\n126\\n127\\n128\\nFigure 14: Further example attention maps as in Figure 6 (random selection).\\n21\\n'),\n",
       " Document(metadata={'source': 'docs/vision_transformer.pdf', 'file_path': 'docs/vision_transformer.pdf', 'page': 21, 'total_pages': 22, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210604001958Z', 'modDate': 'D:20210604001958Z', 'trapped': ''}, page_content='Published as a conference paper at ICLR 2021\\nTable 9: Breakdown of VTAB-1k performance across tasks.\\nCaltech101\\nCIFAR-100\\nDTD\\nFlowers102\\nPets\\nSun397\\nSVHN\\nCamelyon\\nEuroSAT\\nResisc45\\nRetinopathy\\nClevr-Count\\nClevr-Dist\\nDMLab\\ndSpr-Loc\\ndSpr-Ori\\nKITTI-Dist\\nsNORB-Azim\\nsNORB-Elev\\nMean\\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\\n22\\n')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0427010d-dd62-4cbf-b9f0-6f6bdb9b6d13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ze6DA3dGaTln",
    "outputId": "f8d9dd73-8eb2-4afd-8431-8002ed3f239c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'docs/cnn_paper.pdf', 'file_path': 'docs/cnn_paper.pdf', 'page': 8, 'total_pages': 11, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.12', 'creationDate': 'D:20151203014807Z', 'modDate': 'D:20151203014807Z', 'trapped': ''}, page_content='Introduction to Convolutional Neural Networks\\n9\\nAnother common CNN architecture is to stack two convolutional layers before\\neach pooling layer, as illustrated in Figure 5. This is strongly recommended as\\nstacking multiple convolutional layers allows for more complex features of the\\ninput vector to be selected.\\ninput\\nconvolution w/ ReLu\\npooling\\nconvolution\\nw/ ReLu\\npooling\\nfully-connected\\nw/ ReLu\\nfully-connected\\nconvolution w/ ReLu\\npooling\\n0\\n9\\noutput \\n...\\nFig. 5: A common form of CNN architecture in which convolutional layers are\\nstacked between ReLus continuously before being passed through the pooling\\nlayer, before going between one or many fully connected ReLus.\\nIt is also advised to split large convolutional layers up into many smaller sized\\nconvolutional layers. This is to reduce the amount of computational complexity\\nwithin a given convolutional layer. For example, if you were to stack three con-\\nvolutional layers on top of each other with a receptive ﬁeld of 3×3. Each neuron\\nof the ﬁrst convolutional layer will have a 3×3 view of the input vector. A neu-\\nron on the second convolutional layer will then have a 5 × 5 view of the input\\nvector. A neuron on the third convolutional layer will then have a 7 × 7 view of\\nthe input vector. As these stacks feature non-linearities which in turn allows us\\nto express stronger features of the input with fewer parameters. However, it is\\nimportant to understand that this does come with a distinct memory allocation\\nproblem - especially when making use of the backpropagation algorithm.\\nThe input layer should be recursively divisible by two. Common numbers in-\\nclude 32 × 32, 64 × 64, 96 × 96, 128 × 128 and 224 × 224.\\nWhilst using small ﬁlters, set stride to one and make use of zero-padding as to\\nensure that the convolutional layers do not reconﬁgure any of the dimension-\\nality of the input. The amount of zero-padding to be used should be calculated\\nby taking one away from the receptive ﬁeld size and dividing by two.activation\\nCNNs are extremely powerful machine learning algorithms, however they can\\nbe horrendously resource-heavy. An example of this problem could be in ﬁlter-\\ning a large image (anything over 128 × 128 could be considered large), so if the\\ninput is 227 × 227 (as seen with ImageNet) and we’re ﬁltering with 64 kernels\\neach with a zero padding of then the result will be three activation vectors of\\nsize 227 × 227 × 64 - which calculates to roughly 10 million activations - or an\\nenormous 70 megabytes of memory per image. In this case you have two op-\\ntions. Firstly, you can reduce the spatial dimensionality of the input images by\\n')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824d000e-f67b-4d65-9fbc-64715104e613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRVEmiTvXmpn",
    "outputId": "aa0ac7a9-48f3-4c02-cd98-4840c88bb770"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docx_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "483d4108-363a-4937-a623-e65abccb19bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'docs/Intel Strategy.docx', 'emphasized_text_contents': ['The Superpowers', 'Pervasive Connectivity', 'Ubiquitous compute'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': 'docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T17:58:29', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '6d73a46d675acd5cfcb940ccb7c37977'}, page_content='Intel Strategy\\n\\nOver the last few years, Intel, one of the world’s biggest chipmakers, has been transitioning towards a more datacentric approach than PC-centric. This is a welcome move, not only for the company but for innovation in technology as well, says Pat Gelsinger, CEO, Intel. According to him, the changing times as well as strides in innovation have placed Intel in a position where it can leverage the “superpowers” to make the world of computing better sustainable and far superior to the present scenario.\\n\\nThe Superpowers\\n\\nPervasive connectivity, Ubiquitous compute, AI and Cloud-to-Edge Infrastructure -- the four superpowers that will bolster Intel’s footprints into the future, will also play a key role in transforming the world of computing in any device.\\n\\n“Each of these superpowers is impressive on its own, but when they come together, that’s magic. If you’re not applying AI to every one of your business processes, you’re falling behind. We’re seeing this across every industry,” Gelsinger said.\\n\\nPervasive Connectivity: 5G-empowered pervasive connectivity, that intends to connect all, allows customers to gather, store, write, access, and analyze data regardless of device or location. This level of connectivity is essential in creating an improved quality of life, Gelsinger said. He added that Intel was partnering with Taiwan’s Pegatron to produce 5G networking that could be deployed in extreme conditions, too. “Think of it … earthquakes, tornadoes, natural disasters, where the communications infrastructure is knocked out. And imagine that you were a first responder. You’re showing up for a disaster relief situation and you have no communications.” “We’re taking advantage of the advances in 5G availability of wireless spectrum. And you can think about this as a blueprint for next-generation, commercial 5G, the ramp deployments,” Gelsinger said.\\n\\nUbiquitous compute: “Everything has become a computer, essentially any device we touch. Literally compute is now how we experience the world.” Gelsinger said. It is in line with the company’s data-centric approach as well, which include Server and Storage, including CPUs, chipsets, accelerators, memory and storage media in servers and storage systems; Networking and Connectivity, including CPUs, chipsets, accelerators, memory and storage media, and connectivity devices in network appliances and network function virtualization (NFV) systems; Internet of Things, including addressable logic application-specific integrated circuits and standard products, microprocessors, microcontrollers, digital signal processors, memory and storage media and modems in industrial, transportation, automated driving, retail, video surveillance, healthcare, public sector, office automation, gaming and smart home.'),\n",
       " Document(metadata={'source': 'docs/Intel Strategy.docx', 'emphasized_text_contents': ['AI and Cloud-to-Edge Infrastructure', 'IPU roadmap', 'Single GPU solution for media transcode, visual graphics and inference in the cloud:'], 'emphasized_text_tags': ['b', 'b', 'b'], 'file_directory': 'docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T17:58:29', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'ac0ef24a663cbac16d56ffc905957d57'}, page_content='AI and Cloud-to-Edge Infrastructure: This allows for high performance computing to be immediately available, which is the backbone of ubiquitous compute. “With the unlimited reach of the intelligent edge, we can have low latency, high bandwidth, and real-time inference capabilities anywhere we want them,” he explained.  Intelligent Edge could make visual experience of streaming services, cloud gaming, and visual workloads possible, however, there are hurdles to be overcome for that. Intel stands to overcome the challenges of deploying a complete cloud to edge infrastructure in today’s time with the launch of Habana Gaudi2 AI processor for training data centre workloads, and 12th Gen Intel Core HX processors for hybrid work.Habana Gaudi2 and Greco AI Accelerators are built on a single software stack, Synapse AI, that supports different architectures, enabling end-users to take advantage of the processors’ performance and efficiency. In addition, Gaudi2 delivers two times better AI training performance compared with current in-market A100-based offerings for key vision and NLP workloads, the company announced. The company also announced the shipment of the 4th Gen Intel Xeon Scalable processors, which will support DDR5, PCIe Gen5 and CXL 1.1, and are equipped with new integrated accelerators that deliver up to 30x performance versus the prior generation through software and hardware optimizations for AI workloads, along with new capabilities that deliver upto two times capacity gains for virtual radio access network (vRAN) deployments, for telco networks. Also, in partnership with Accenture, Intel has kickstarted Project Apollo, a program that will provide enterprises with more than 30 opensource AI solutions kits that are designed to make AI more accessible to customers in on-prem, cloud and edge environments. The company also unveiled its IPU roadmap, featuring new FPGA + Intel architecture platforms (code-named Hot Springs Canyon) and the Mount Morgan (MMG) ASIC, as well as next generation 800GB products. IPUs are dedicated products with hardened acceleration for infrastructure compute needs, allowing businesses to accomplish tasks quicker and solve problems faster.\\n\\nSingle GPU solution for media transcode, visual graphics and inference in the cloud:\\xa0Intel’s data center GPU, code-named Arctic Sound-M (ATS-M), is the industry’s first discrete GPU with an AV1 hardware encoder. ATS-M is a versatile GPU with leadership transcode quality and performance targeting 150 trillion operations per second (TOPS). Developers will be able to easily design for ATS-M with an open software stack through oneAPI. ATS-M will be available in two form factors and in more than 15 system designs from partners including Dell Technologies, Supermicro, Cisco, HPE,\\xa0Inspur\\xa0and H3C. It will launch in 2022’s third quarter.'),\n",
       " Document(metadata={'source': 'docs/Intel Strategy.docx', 'emphasized_text_contents': ['New 12th Gen Intel Core HX processors for hybrid work:', 'High performance computing to solve the world’s most complex challenges:', 'Confidence with confidential computing:', 'Agriculture autonomy with private wireless networks'], 'emphasized_text_tags': ['b', 'b', 'b', 'b'], 'file_directory': 'docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T17:58:29', 'orig_elements': 'eJzVV9tu3DYQ/RVin1pgZez94jc7Te0AaRDEaRMgCQyKHK0IU6JKUrveBPn3nqG0aztNASPwQ/Nir8ThXM+ZGX34MiBLFdXx2ujBqRjMxqvRaq4oW87WRTabzRfZaqrG2Qx/9FSOium8GAzFoKIotYwSd74MlIy0cX5/ramJJV6NIEFVU8pgPpO+jnQbr5WrI+wEHH8YvKKdGE9iKS6oFi9wYMUz50lcvheNd4pCcD6IwnlR7nNvtNg5f3M6+PQdvVFuOp15Oi6MpWttPKkIjzgk7VQY9Ce1rIjfdSavomfP9ycQuWURK+tNKzfUKaR6k1RaGeJ15bQpDKUkTUaTeTaaZqPF2/HydL46nawHXyHI/vD5j0X3sR2N5OhtSUK5qpH1Pv23FEmLiLdHjYWsjN2LncEzH1jZ1qoUrkhP9WOMn4hnniRrZi/wvsB742ppg9iVDkpwVMlbU7WVaMhDqpK1IiFrXLF0a3JjTdyLCFm5NRvoEvIQDtVb413NsBqmG8nVtmHp8QJheQrpvbJO3YjQwFroz+fi4vLzMEVyz3mzzsaT9Wh0PwhhQhJD8qz+2E5G43UQOYUoKgfv0sFNiDIiLtFYGTmIEy5z3DcJBS9NiC8iVVy8b4mgF/OlXst5tpitltlM53kmx/NJtiqWi/k81xMlZ09AhEuzKR8kmGveRlNvOBnB2S39O8jKIcgOHLdCldIC2ADtT8OPJw26o82Z37i6JvEqlVta8VLmDt47b4A1IAUYQDRAGyxosmZLXkwE3crCuiYweRqSNw+8OhLsrPXQJUKL085VXPZtXbPHrPgI1vfEWDsgdAh/NWWcVC2uZNOUyLt4IxsDuCftKRPnoMLOaDz+QRWKIn65PP/j1445d6oZZEIBOLB98frPB7pfM6TEX6RUadywU93dcjWdvX7BLm2NZnc1bQkRExpQIFlZOCrCPoAFwuDGxqcEPpomC1nMtCwwIApSmBL5PJNytc5UMZvQbJXPirl6Apo8c3VhNB2rovrnaFDqI3p+GgI8MpwO2uLcBbR3RkPnlXLWduhG6YE3KdBPSXoIUQH0xg7jqc44fKD87AXzq009MZYyCvDI7UJvA/cQsamFiQEzoPW4UVNMjfSBHowf0+G+aXNrFHfyVp+It06UZGEVQKTEYlAPVoKSloa9FXT0Jnkv3hz8RsFE3hobv+PvoXWLxu3II+R83yfiyhVxJ0Gpi1Z6LZ7fAjA8xWByKw1yZPv8TnF6Nw4TSa/gURI4qA+PxnyR53qVT5fZfDLFjjSnWbYeqyIbq/FSL+VqodbLJ8D82cYb1drYIkDZRle7qh/6jTdbTt8OKE4EPtToZ8H/D4R22hXPmg0SJEhv6AjkAPj0E6NxsQdOB+U6JOQUzgGc57Yl8a40DDyXu2hUOLZDDeDx5sT152HQe+VaYOnoLNQ+JM/RAqTExjM+D+MKK1JIE43+bk3D6OH+6iCH9YniNzZ4NEVeChVPMZVWPsjWqSd75CDLZUgrWY3sJ23HjUa848Ql2hXeVT3IuV38Tpq6LvGuT+fwu0mopD5QNTE18eJhpBZ58lz4A/UqiT7znMtw15gSsX67v+PyGfBo3T709UGKOPZUwLuxz3KhlMxubIMqeqydSIBKSyq8+i9YpPaHTbmQqDJ+7Ep0iEcTebZaT8dylGdaTfNsphYqW6/wsVMslkrqsVrQcvw4Iv8fSNVpN7wpbVNOuZkmOLJTvJc3UtEwveF2aAOkUI0gDcjRCb4r0SX3rmU01DcCU6blacK3hxgKB3BLXs10f5zW967SNfnNPv0sUTgwro2HOXEYXTW7ixWFOl3cAGwA1JvG7pPXYpuS2M2+9E2DQHrN/LESKHaQEV2+GaKJWWylaLmnnHQx3ofBK+l5s9nSW87W10//AK/7Iyc=', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b55d958ee4e244f7d5c5471ffbfac525'}, page_content='New 12th Gen Intel Core HX processors for hybrid work:\\xa0The company completed the 12th Gen family with the launch of the new 12th Gen Intel Core HX processors. Created for professionals who need maximum performance and flexibility to navigate a hybrid environment, and with up to 16 cores and clock speeds up to 5 GHz, the Intel Core i9-12900HX processor is the world’s best mobile workstation platform.\\n\\nHigh performance computing to solve the world’s most complex challenges:\\xa0Argonne National Laboratories is on track to deliver 2 exaflops of peak performance with the Aurora supercomputer running on the Intel Xeon processor, code-named Sapphire Rapids with High Bandwidth Memory (HBM), and the Intel data center GPU, code-named Ponte Vecchio, with Intel oneAPI providing developers seamless system integration.\\n\\nConfidence with confidential computing:\\xa0 Bosch and Intel collaborated on a research effort to develop a confidential AI solution that allows Bosch to train its neural networks confidentially in the public cloud. To help achieve this at scale, Bosch Corporate Research has built a confidential AI platform powered by Intel Software Guard Extensions available with 3rd Gen Intel Xeon Scalable platforms.\\n\\nAgriculture autonomy with private wireless networks: Intelligent edge solutions have the potential to transform food. Blue White Robotics developed a new type of autonomous agricultural solution that transforms a grower’s existing equipment into a fleet of autonomous tractors connected to an internet-based management platform. With help from Intel and Federated Wireless, Blue White Robotics made this a scalable solution that leverages Intel Smart Edge and Intel Xeon D processors and employs the power of edge computing and shared spectrum to create a private wireless network on any farm anywhere.\\n\\nIntel is moving at a “torrid pace,” Gelsinger said. “When you think about torrid, it’s a word about speed and energy and heat. But in the Intel context, we’re also applying a vector\\xa0of that energy for setting a direction into the future.”'),\n",
       " Document(metadata={'source': 'docs/Intel Strategy.docx', 'emphasized_text_contents': ['Intel IDM 2.0', 'Intel’s global, internal factory network for at-scale manufacturing', 'Expanded use of third-party foundry capacity.', 'Intel Foundry Services.', 'Expanding in the U.S. and Europe'], 'emphasized_text_tags': ['b', 'b', 'b', 'b', 'b'], 'file_directory': 'docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T17:58:29', 'orig_elements': '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', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': '9b9fec55d420d43d84e13646715885e6'}, page_content='Intel IDM 2.0\\n\\nThe Intel IDM 2.0 strategy revealed by new company CEO Pat Gelsinger\\xa0is an ambitious plan to restore the company’s leadership in semiconductor production. Gelsinger described IDM 2.0 as the second generation of Intel’s integrated device manufacturing model.\\n\\nIntel’s global, internal factory network for at-scale manufacturing\\xa0is a key competitive advantage that enables product optimization, improved economics and supply resilience. Gelsinger re-affirmed the company’s expectation to continue manufacturing most of its products internally. The company’s 7 Nanometer Processors development is driven by increased use of extreme ultraviolet lithography (EUV) in a rearchitected, simplified process flow.\\n\\nExpanded use of third-party foundry capacity.\\xa0 Gelsinger said he expects Intel’s engagement with third-party foundries to grow and to include manufacturing for a range of modular tiles on advanced process technologies, including products at the core of Intel’s computing offerings for both client and data center segments beginning in 2023. This will provide the increased flexibility and scale needed to optimize Intel’s roadmaps for cost, performance, schedule and supply, giving the company a unique competitive advantage.\\n\\nIntel Foundry Services.\\xa0The launch of Intel Foundry Services means the company is not only going to manufacture its own chips, but it will also produce them for other semiconductor companies, including its competitors. \\xa0Intel announced plans to become a major provider of U.S. and Europe-based foundry capacity to serve the global demand for semiconductor manufacturing. Hence, Intel is establishing a new standalone business unit, Intel Foundry Services (IFS), led by semiconductor industry veteran Dr. Randhir Thakur, who will report directly to Gelsinger. IFS will be differentiated from other foundry offerings with a combination of leading-edge process technology and packaging, committed capacity in the U.S. and Europe, and a world-class IP portfolio for customers, including x86 cores as well as ARM and RISC-V ecosystem IPs. Gelsinger noted that Intel’s foundry plans have received strong statements of support from across the industry. Intel conservatively sizes the foundry opportunity as a $100 billion addressable market by 2025.\\n\\nExpanding in the U.S. and Europe. Intel is expanding its manufacturing capacity in the U.S. and Europe to provide less dependence on any specific region. Noting that 80% of leading-edge foundry capacity is concentrated in Asia, Gelsinger believes “the industry needs more geographically balanced manufacturing capacity.”'),\n",
       " Document(metadata={'source': 'docs/Intel Strategy.docx', 'emphasized_text_contents': ['Intel Sustainability'], 'emphasized_text_tags': ['b'], 'file_directory': 'docs', 'filename': 'Intel Strategy.docx', 'languages': ['eng'], 'last_modified': '2025-03-06T17:58:29', 'orig_elements': 'eJzVVk1v3DYQ/SuDRT8uliFp9elb2gRBgKAoaueUBAZFjrREKFIhKW+2Qf97H+VN6xQukN7qm6ThzDy+eXzU2887Njyzjbda7a5o1/Ssxkrm2ViofVYVXZV1os6zuhT9uG+Y227YXdBu5iiUiAI5n3dSRJ6cP90qXuIBn3Ks4Hk5iKB/Z3Ub+VO8lc5G9AkIv929wrOh6zVEoa0YtNHxtHv/SFYU033GsIVHbfhWac8yol8CrJwMu3PEipnTt3P16BOu0yWWfEpLjLDTKia+L8h22koaEeLt7JQeNW8UlHlZZ/k+y5ubor2qu6uy3/2BhQnPg+pfY0/x07J1v9HRcMr4J7cVy2rgYcjKssizqmxF1rV4bVTb9oKHpu7238bt/4GGd2uZF/LmwKTnRchIbiRp9IxmJA/ogUAgYWn1EzigybhBGIoHzyJe0q/eRcDXdiK3elqAiiMpnoVVASVnVjqVQmXtLOooGj2HAwpo+yGlicGtkQ7uiE9MR+eNIrdw2m24pGeBnOUE6q/oj4EMC5VyA88aglRrIhBdg57ue6D9OqLn6tMy6bA1qzlc0D2Z2JHGQlqt/rgyLS7oDV50yPwAtAQCR/ZsJZN1IMWaU0pJe3RHewaIFJQcAH+rdhQnoBRxq4EOkRhngD2NACchNDezR8IifLR42pCmiseDM0x3wqwb53oDEhOQmYHbTuNqvjAYnbvYZqboJZuAIBoEodUlPVTvL8IngHd8k+b8iIr7pi1rLvpsX1RDVvV5kfUyb2ET1dhWVZ4PXD0dFd84gh4NDCcJC8zOQ5roGmhywnwZOywJkomblEZnjDsmdWjEvJ5pBlrMyGJ0aWBlvs+v3tl3a56L/D9RO5ZlJ+ESWT42BaitVTZ0dZsNUtayKIdGdeXTofaZPGi+Yyry/HuQbPkoBqgVW5bRawnXpDWkA+5dSKIPXyzi7zNy+ZC+1zrEV5Hnx5irO9E2auBMqrrPqqEsMtE0eJWiEiUXVVc+IeZe2TsIisSyePdpc1SYyHf7PE9SM+kw46SDUD+dYFE2sL8T9y4ZNx5hYOlWgm8lPxDnOVQ0nLPlOq9mUyJ90BCziMlHVxgL7PJcN4g7aPzbJwB51vsxr7JmqLusUnuV9Xsls1YWYyebspd9/nQm8NOqcZtAsolLwEhUblfQV8zODE94c00vPbOlLSf5ws9utVhGr1+8eE4Y4eTFTPhbsEp4BRvXVpp1W4k9gkEMV1ibklghmGafiN2umuQ4b64v6MXqcSw2DM+CFt9+MMo9fHnMs75iuDWXfdaV+OfoZMtlI0Q/cP10xvJagKQDbszNMzKNCzxEf6LffngOsjCXe1FjNFqBAz2eaEqzwU0nD+nKFybQUccDQfX4eDaco/BzGsfi0h+qxpfEM6oonBvjlk0IYsCWErPEH1e9pKd/mcL7PwE+MLes', 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'CompositeElement', 'element_id': 'b8ae863a5df10800d4307a8d409a5034'}, page_content=\"Intel Sustainability\\n\\n“The impact of climate change is an urgent global threat. Protecting our planet demands immediate action and fresh thinking about how the world operates. As one of the world's leading semiconductor design and manufacturing companies, Intel is in a unique position to make a difference not only in our own operations, but in a way that makes it easier for customers, partners and our whole value chain to take meaningful action too,” Gelsinger said. \\n\\nTo realize this ambitious goal, Intel has set the following interim milestones for 2030:\\n\\xa0\\n\\nAchieve 100% renewable electricity use across its global operations.\\n\\nInvest approximately $300 million in energy conservation at its facilities to achieve 4 billion cumulative kilowatt hours of energy savings.\\n\\nBuild new factories and facilities to meet US Green Building Council LEED program standards, including recently announced investments in the US, Europe and Asia.\\n\\nLaunch a cross-industry R&D initiative to identify greener chemicals with lower global warming potential and to develop new abatement equipment.\")]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a2a2b6-ec17-4c37-bc8d-6c4a4d0cabee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### YouTube Transcript Loader\n",
    "You can get the transcript from any YouTube link. First, you have to install the given libraries. If you want to keep the video information like title, and description in the documents, then keep add_video_info=True Otherwise keep False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67783ec2-1400-47af-bbaa-8d4d864c52a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  youtube-transcript-api\n",
    "%pip install --upgrade --quiet  pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4559a00-0de4-4c58-84fa-9fa0230543dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pytube in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cb31971-3946-41bf-96a8-49828c55f0b2/lib/python3.10/site-packages (15.0.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pytube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7c2cfc5-ff63-48ae-83a2-fb8cdcf847c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Video Info: []\n\n\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=LAfrShnpVIk\",\n",
    "    add_video_info=False,\n",
    ")\n",
    "\n",
    "result = loader.load()\n",
    "print(f\"Without Video Info: {result}\\n\\n\")\n",
    "\n",
    "# loader = YoutubeLoader.from_youtube_url(\n",
    "#     \"https://www.youtube.com/watch?v=ZL-cwYRMPjI\",\n",
    "#     add_video_info=True,\n",
    "# )\n",
    "# result = loader.load()\n",
    "# print(\"With Video Info: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d01243b-a8d1-411a-a81d-99c61694af31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you want to format the Transcripts, you can do it through the TranscriptFormat class. Here I have created the documents where each document will contain the transcript of 10-second contexts from the video. You can customize it in your desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade424e5-7610-428d-852f-bec1db717b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.youtube import TranscriptFormat\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=TKCMw0utiak\",\n",
    "    add_video_info=False,\n",
    "    transcript_format=TranscriptFormat.CHUNKS,\n",
    "    chunk_size_seconds=10,\n",
    ")\n",
    "print(\"\\n\\n\".join(map(repr, loader.load())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de425cd-224b-4ed8-8558-c25bb7f0fd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scraping data from URLs\n",
    "\n",
    "We can scrape data from URLs in various ways such as HTMLloader, RecursiveUrlLoader, FireCrawl, and others. The RecursiveUrlLoader is used to recursively scrape all the child links from a root URL and then convert the data into Documents.\n",
    "\n",
    "Recursive Url loader extracts the content with HTML tags. I have created a function for extracting tags and returning the actual content. You can also define the extractor in the Recursive URL Loader class. Look at the code and run it on your device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992c2e72-2b6d-41b7-9397-826173b85378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-engineering 0.2.1 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.2.0 requires numpy<1.24,>=1.16.0, but you have numpy 2.2.3 which is incompatible.\nydata-profiling 4.2.0 requires pydantic<2,>=1.8.1, but you have pydantic 2.10.6 which is incompatible.\ntensorflow-cpu 2.14.1 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.3 which is incompatible.\nscipy 1.10.0 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.2.3 which is incompatible.\nnumba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 2.2.3 which is incompatible.\ndatabricks-feature-engineering 0.2.1 requires numpy<2,>=1.19.2, but you have numpy 2.2.3 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: lxml in /databricks/python3/lib/python3.10/site-packages (4.9.1)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community beautifulsoup4\n",
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c728f26-2e27-47bb-b08e-539086d9e862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n<!DOCTYPE html><html\n\tclass=\"hasSidebar hasPageActions hasBreadcrumb conceptual has-default-focus theme-light\"\n\tlang=\"en-us\"\n\tdir=\"ltr\"\n\tdata-authenticated=\"false\"\n\tdata-auth-status-determined=\"false\"\n\tdata-target=\"docs\"\n\tx-ms-format-detection=\"none\">\n\n<head>\n\t<meta charset=\"utf-8\" />\n\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n\t<meta property=\"og:title\" content=\"Mosaic AI Vector Search - Azure Databricks\" />\n\t<meta property=\"og:type\" content=\"website\" />\n\t<meta property=\"og:url\" content=\"https://learn.microsoft.com/en-us/azure/databricks/generative-ai/vector-search\" /><meta property=\"og:description\" content=\"Learn about Mosaic AI Vector Search, Databricks’ vector database solution, including what it is and how it works.\" /><meta property=\"og:image\" content=\"https://learn.microsoft.com/en-us/media/open-graph-image.png\" />\n\n\t<meta property=\"og:image:alt\" content=\"Microsoft Learn\" />\n\n\t<meta name=\"twitter:card\" content=\"summary_large_image\" />\n\n\t<meta name=\"twitter:site\" content=\"@MicrosoftLearn\" />\n\n\t<meta name=\"color-scheme\" content=\"light dark\"><meta name=\"author\" content=\"mssaperla\" />\r\n<meta name=\"breadcrumb_path\" content=\"/azure/databricks/breadcrumb/toc.json\" />\r\n<meta name=\"depot_name\" content=\"MSDN.databricks\" />\r\n<meta name=\"description\" content=\"Learn about Mosaic AI Vector Search, Databricks’ vector database solution, including what it is and how it works.\" />\r\n<meta name=\"document_id\" content=\"b0bd4ac3-2839-688a-b02a-55e56ddedc77\" />\r\n<meta name=\"document_version_independent_id\" content=\"b0bd4ac3-2839-688a-b02a-55e56ddedc77\" />\r\n<meta name=\"feedback_help_link_type\" content=\"\" />\r\n<meta name=\"feedback_help_link_url\" content=\"\" />\r\n<meta name=\"feedback_product_url\" content=\"https://feedback.azure.com/d365community/forum/2efba7dc-ef24-ec11-b6e6-000d3a4f0da0\" />\r\n<meta name=\"feedback_system\" content=\"Standard\" />\r\n<meta name=\"git_commit_id\" content=\"e132b846bea02264be4954b1d1f1e339e3f7e9e9\" />\r\n<meta name=\"gitcommit\" content=\"https://github.com/MicrosoftDocs/databricks-pr/blob/e132b846bea02264be4954b1d1f1e339e3f7e9e9/databricks/generative-ai/vector-search.md\" />\r\n<meta name=\"locale\" content=\"en-us\" />\r\n<meta name=\"ms.author\" content=\"saperla\" />\r\n<meta name=\"ms.custom\" content=\"databricksmigration\" />\r\n<meta name=\"ms.date\" content=\"02/03/2025\" />\r\n<meta name=\"ms.reviewer\" content=\"jasonh\" />\r\n<meta name=\"ms.service\" content=\"azure-databricks\" />\r\n<meta name=\"ms.topic\" content=\"conceptual\" />\r\n<meta name=\"original_content_git_url\" content=\"https://github.com/MicrosoftDocs/databricks-pr/blob/live/databricks/generative-ai/vector-search.md\" />\r\n<meta name=\"page_type\" content=\"conceptual\" />\r\n<meta name=\"pdf_url_template\" content=\"https://learn.microsoft.com/pdfstore/en-us/MSDN.databricks/{branchName}{pdfName}\" />\r\n<meta name=\"schema\" content=\"Conceptual\" />\r\n<meta name=\"site_name\" content=\"Docs\" />\r\n<meta name=\"toc_rel\" content=\"../toc.json\" />\r\n<meta name=\"uhfHeaderId\" content=\"azure\" />\r\n<meta name=\"updated_at\" content=\"2025-03-03 07:40 PM\" />\r\n<meta name=\"word_count\" content=\"1587\" />\r\n<meta name=\"persistent_id\" content=\"56873fb6-374e-5f1d-7fb0-32efd91f1176\" />\n\t<meta name=\"platform_id\" content=\"56873fb6-374e-5f1d-7fb0-32efd91f1176\" />\n\t\n\t<meta name=\"cmProducts\" content=\"https://microsoft-devrel.poolparty.biz/DevRelOfferingOntology/12ed19f9-ebdf-4c8a-8bcd-7a681836774d\" data-source=\"generated\" />\n\t\n\t<meta name=\"spProducts\" content=\"https://microsoft-devrel.poolparty.biz/DevRelOfferingOntology/3a764584-4f97-452b-8f1d-36f19b12f6ae\" data-source=\"generated\" />\n\t<meta name=\"scope\" content=\"Azure, Azure Databricks\" /><meta name=\"github_feedback_content_git_url\" content=\"https://github.com/MicrosoftDocs/databricks-pr/blob/live/databricks/generative-ai/vector-search.md\" /><link href=\"https://learn.microsoft.com/en-us/azure/databricks/generative-ai/vector-search\" rel=\"canonical\"><title>Mosaic AI Vector Search - Azure Databricks | Microsoft Learn</title><link rel=\"stylesheet\" href=\"/static/assets/0.4.029836350/styles/site-ltr.css\">\n\n\t<script id=\"msdocs-script\">\n\tvar msDocs = {environment: {\n\t\t\tsupportLevel: 'production',\n\t\t\taccessLevel: 'online',\n\t\t\treviewFeatures: false,\n\t\t\tsystemContent: true,\n\t\t\tazurePortalHostname: 'portal.azure.com',\n\t\t\tlegacyHosting: false,\n\t\t\tsiteName: 'learn',\n\t\t},data: {\n\t\t\ttimeOrigin: Date.now(),\n\t\t\tcontentLocale: 'en-us',\n\t\t\tcontentDir: 'ltr',\n\t\t\tuserLocale: 'en-us',\n\t\t\tuserDir: 'ltr',\n\t\t\tpageTemplate: 'Conceptual',\n\t\t\tbrand: '',\n\t\t\tcontext: {},\n\t\t\thasBinaryRating: true,\n\t\t\tfeedbackHelpLinkType:'',\n\t\t\tfeedbackHelpLinkUrl:'',\n\t\t\tstandardFeedback: true,\n\t\t\tshowFeedbackReport: false,\n\t\t\tenableTutorialFeedback: false,\n\t\t\tfeedbackSystem: 'Standard',\n\t\t\tfeedbackGitHubRepo: 'MicrosoftDocs/azure-docs',\n\t\t\tfeedbackProductUrl: 'https://feedback.azure.com/d365community/forum/2efba7dc-ef24-ec11-b6e6-000d3a4f0da0',extendBreadcrumb: false,isEditDisplayable: false,\n\t\t\thideViewSource: false,\n\t\t\thasPageActions: true,\n\t\t\thasPrintButton: true,\n\t\t\thasBookmark: true,\n\t\t\thasShare: true,\n\t\t\tisPermissioned: false,\n\t\t\tisPrivateUnauthorized: false,hasRecommendations: true,contributors: [{ name: \"mssaperla\", url: \"https://github.com/mssaperla\" },{ name: \"docs-preview-bot\", url: \"https://github.com/docs-preview-bot\" }],},\n\t\tfunctions:{}\n\t};\n\t</script><script src=\"https://wcpstatic.microsoft.com/mscc/lib/v2/wcp-consent.js\"></script>\n\t<script src=\"https://js.monitor.azure.com/scripts/c/ms.jsll-4.min.js\"></script><script src=\"/static/assets/0.4.029836350/global/deprecation.js\"></script><script src=\"/static/assets/0.4.029836350/scripts/en-us/index-docs.js\"></script></head>\n\n<body lang=\"en-us\" dir=\"ltr\">\n\t<div class=\"header-holder has-default-focus\">\n\t\t<a href=\"#main\" style=\"z-index: 1070\" class=\"outline-color-text visually-hidden-until-focused position-fixed inner-focus focus-visible top-0 left-0 right-0 padding-xs text-align-center  has-body-background\" tabindex=\"1\">Skip to main content</a><div hidden id=\"cookie-consent-holder\" data-test-id=\"cookie-consent-container\"></div>\n\n\t\t<div id=\"unsupported-browser\" style=\"\n\t\t\tbackground-color: white;\n\t\t\tcolor: black;\n\t\t\tpadding: 16px;\n\t\t\tborder-bottom: 1px solid grey;\"\n\t\t\thidden\n\t\t>\n\t\t\t<div style=\"max-width: 800px; margin: 0 auto;\">\n\t\t\t\t<p style=\"font-size: 24px\">This browser is no longer supported.</p>\n\t\t\t\t<p style=\"font-size: 16px; margin-top: 16px;\">Upgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.</p>\n\t\t\t\t<div style=\"margin-top: 12px;\">\n\t\t\t\t\t<a href=\"https://go.microsoft.com/fwlink/p/?LinkID=2092881 \"\n\t\t\t\t\t\tstyle=\"\n\t\t\t\t\t\tbackground-color: #0078d4;\n\t\t\t\t\t\tborder: 1px solid #0078d4;\n\t\t\t\t\t\tcolor: white;\n\t\t\t\t\t\tpadding: 6px 12px;\n\t\t\t\t\t\tborder-radius: 2px;\n\t\t\t\t\t\tdisplay: inline-block;\n\t\t\t\t\t\t\">Download Microsoft Edge</a>\n\t\t\t\t\t<a href=\"https://learn.microsoft.com/en-us/lifecycle/faq/internet-explorer-microsoft-edge\"\n\t\t\t\t\t\tstyle=\"\n\t\t\t\t\t\t\tbackground-color: white;\n\t\t\t\t\t\t\tpadding: 6px 12px;\n\t\t\t\t\t\t\tborder: 1px solid #505050;\n\t\t\t\t\t\t\tcolor: #171717;\n\t\t\t\t\t\t\tborder-radius: 2px;\n\t\t\t\t\t\t\tdisplay: inline-block;\n\t\t\t\t\t\t\t\">More info about Internet Explorer and Microsoft Edge</a>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t\t<!-- liquid-tag banners global -->\n\n\t\t<!-- site header -->\n\t\t<header id=\"ms--site-header\" data-test-id=\"site-header-wrapper\" role=\"banner\" itemscope=\"itemscope\" itemtype=\"http://schema.org/Organization\">\n\t\t\t<div id=\"ms--mobile-nav\" class=\"site-header display-none-tablet padding-inline-none gap-none\" data-bi-name=\"mobile-header\" data-test-id=\"mobile-header\"></div>\n\t\t\t<div id=\"ms--primary-nav\" class=\"site-header display-none display-flex-tablet\" data-bi-name=\"L1-header\" data-test-id=\"primary-header\"></div>\n\t\t\t<div id=\"ms--secondary-nav\" class=\"site-header display-none display-flex-tablet\" data-bi-name=\"L2-header\" data-test-id=\"secondary-header\"></div>\n\t\t</header><div id=\"content-header\" class=\"content-header uhf-container has-padding has-default-focus border-bottom-none\" data-bi-name=\"content-header\">\n\t\t\t\t<div class=\"content-header-controls margin-xxs margin-inline-sm-tablet\">\n\t\t\t\t\t<button type=\"button\" class=\"contents-button button button-sm margin-right-xxs\" data-bi-name=\"contents-expand\" aria-haspopup=\"true\" data-contents-button>\n\t\t\t\t\t\t<span class=\"icon\"><span class=\"docon docon-menu\" aria-hidden=\"true\"></span></span>\n\t\t\t\t\t\t<span class=\"contents-expand-title\">Table of contents</span>\n\t\t\t\t\t</button>\n\t\t\t\t\t<button type=\"button\" class=\"ap-collapse-behavior ap-expanded button button-sm\" data-bi-name=\"ap-collapse\" aria-controls=\"action-panel\">\n\t\t\t\t\t\t<span class=\"icon\"><span class=\"docon docon-exit-mode\" aria-hidden=\"true\"></span></span>\n\t\t\t\t\t\t<span>Exit focus mode</span>\n\t\t\t\t\t</button>\n\t\t\t\t</div>\n\t\t\t</div><div id=\"disclaimer-holder\" class=\"has-overflow-hidden has-default-focus\">\n\t\t\t<!-- liquid-tag banners sectional -->\n\t\t</div>\n\t</div>\n\n\t<div class=\"mainContainer  uhf-container has-default-focus\" data-bi-name=\"body\">\n\n\t\t<div class=\"columns has-large-gaps is-gapless-mobile \"><div id=\"left-container\" class=\"left-container is-hidden-mobile column is-one-third-tablet is-one-quarter-desktop\">\n\t\t\t\t<nav id=\"affixed-left-container\" class=\"margin-top-sm-tablet position-sticky display-flex flex-direction-column\" aria-label=\"Primary\"></nav>\n\t\t\t</div><!-- .primary-holder -->\n\t\t\t<section class=\"primary-holder column is-two-thirds-tablet is-three-quarters-desktop\">\n\t\t\t\t<!--div.columns -->\n\t\t\t\t<div class=\"columns is-gapless-mobile has-large-gaps \"><div id=\"main-column\" class=\"column  is-full is-8-desktop\" data-main-column>\n\n\t\t\t\t\t\t<main id=\"main\" class=\"\" role=\"main\" data-bi-name=\"content\" lang=\"en-us\" dir=\"ltr\"><!-- article-header -->\n\t\t\t\t\t\t\t<div id=\"article-header\" class=\"background-color-body margin-top-sm-tablet margin-bottom-xs display-none-print\">\n\t\t\t\t\t\t\t\t<div class=\"display-flex align-items-center \"><details id=\"article-header-breadcrumbs-overflow-popover\" class=\"popover\" data-for=\"article-header-breadcrumbs\">\n\t\t\t\t\t\t\t\t\t\t<summary class=\"button button-clear button-primary button-sm inner-focus\" aria-label=\"All breadcrumbs\">\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\">\n\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-more\"></span>\n\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t</summary>\n\t\t\t\t\t\t\t\t\t\t<div id=\"article-header-breadcrumbs-overflow\" class=\"popover-content padding-none\">\n\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</details>\n\n\t\t\t\t\t\t\t\t\t<bread-crumbs id=\"article-header-breadcrumbs\" data-test-id=\"article-header-breadcrumbs\" class=\"overflow-hidden flex-grow-1 margin-right-sm margin-right-md-tablet margin-right-lg-desktop margin-left-negative-xxs padding-left-xxs\"></bread-crumbs><div id=\"article-header-page-actions\"  class=\"opacity-none margin-left-auto display-flex flex-wrap-no-wrap align-items-stretch\"><a\n\t\t\t\t\t\t\t\t\t\t\tid=\"lang-link-tablet\"\n\t\t\t\t\t\t\t\t\t\t\tclass=\"button button-primary button-clear button-sm display-none display-inline-flex-tablet\"\n\t\t\t\t\t\t\t\t\t\t\ttitle=\"Read in English\" data-bi-name=\"language-toggle\"\n\t\t\t\t\t\t\t\t\t\t\tdata-read-in-link\n\t\t\t\t\t\t\t\t\t\t\thidden>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon margin-none\" aria-hidden=\"true\" data-read-in-link-icon>\n\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-locale-globe\"></span>\n\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"is-visually-hidden\" data-read-in-link-text>Read in English</span>\n\t\t\t\t\t\t\t\t\t\t</a><button\n\t\t\t\t\t\t\t\t\t\t\t\ttype=\"button\"\n\t\t\t\t\t\t\t\t\t\t\t\tclass=\"collection button button-clear button-sm button-primary display-none display-inline-flex-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\tdata-list-type=\"collection\"\n\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"collection\"\n\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Add to collection\">\n\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon margin-none\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-circle-addition\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"collection-status is-visually-hidden\">Save</span>\n\t\t\t\t\t\t\t\t\t\t\t</button><a\tdata-contenteditbtn\n\t\t\t\t\t\t\t\t\t\t\t\tclass=\"button button-clear button-sm text-decoration-none button-primary display-none display-inline-flex-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\taria-label=\"Edit\"\n\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Edit This Document\"\n\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"edit\"\n\t\t\t\t\t\t\t\t\t\t\t\t\thidden\n\t\t\t\t\t\t\t\t\t\t\t\t\t\thref=\"https://github.com/MicrosoftDocs/databricks-pr/blob/live/databricks/generative-ai/vector-search.md\">\n\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon margin-none\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-edit-outline\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\t\t\t<details class=\"popover popover-right\" id=\"article-header-page-actions-overflow\">\n\t\t\t\t\t\t\t\t\t\t\t<summary class=\"justify-content-flex-start button button-clear button-sm button-primary\" aria-label=\"More actions\" title=\"More actions\">\n\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-more-vertical\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t</summary>\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"popover-content padding-xs\"><button\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-page-action-item=\"overflow-mobile\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttype=\"button\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"justify-content-flex-start button-block button-sm has-inner-focus button button-clear display-none-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"contents-expand\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-contents-button\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-popover-close>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-editor-list-bullet\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span><span class=\"contents-expand-title\">Table of contents</span></button><a\n\t\t\t\t\t\t\t\t\t\t\t\t\tid=\"lang-link-overflow\"\n\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"justify-content-flex-start button-sm has-inner-focus button button-clear button-block display-none-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Read in English\" data-bi-name=\"language-toggle\"\n\t\t\t\t\t\t\t\t\t\t\t\t\tdata-page-action-item=\"overflow-mobile\"\n\t\t\t\t\t\t\t\t\t\t\t\t\tdata-check-hidden=\"true\"\n\t\t\t\t\t\t\t\t\t\t\t\t\tdata-read-in-link\n\t\t\t\t\t\t\t\t\t\t\t\t\thidden\n\t\t\t\t\t\t\t\t\t\t\t\t\t>\n\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\" data-read-in-link-icon>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-locale-globe\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t<span data-read-in-link-text>Read in English</span>\n\t\t\t\t\t\t\t\t\t\t\t\t</a><button\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttype=\"button\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"collection justify-content-flex-start button button-clear button-sm has-inner-focus button-block display-none-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-list-type=\"collection\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"collection\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Save\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-page-action-item=\"overflow-mobile\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-check-hidden=\"true\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-popover-close>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-circle-addition\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"collection-status\">Save</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t</button>\n\n\t\t\t \t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t<button\n\t\t\t\t\t\t\t\t\t\t\t\t\t \ttype=\"button\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"collection justify-content-flex-start button button-clear button-sm has-inner-focus button-block display-none-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-list-type=\"plan\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"plan\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Add to plan\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-page-action-item=\"overflow-mobile\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-check-hidden=\"true\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-popover-close\n\t\t\t\t\t\t\t\t\t\t\t\t\t\thidden>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-circle-addition\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"plan-status\">Add to plan</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t</button><a\tdata-contenteditbtn\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"button button-clear button-block button-sm has-inner-focus justify-content-flex-start text-decoration-none display-none-tablet\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\taria-label=\"Edit\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Edit This Document\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"edit\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thidden\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thref=\"https://github.com/MicrosoftDocs/databricks-pr/blob/live/databricks/generative-ai/vector-search.md\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-edit-outline\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span>Edit</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t</a><div aria-hidden=\"true\" class=\"margin-none\" data-page-action-item=\"overflow-all\"></div>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<hr class=\"display-none-tablet margin-bottom-xxs margin-top-xxs\" />\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<h4 class=\"font-size-sm padding-left-xxs\">Share via</h4>\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t<a class=\"button button-clear button-sm button-block has-inner-focus text-decoration-none justify-content-flex-start share-facebook\" data-bi-name=\"facebook\" data-page-action-item=\"overflow-all\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-facebook-share font-size-md color-primary\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"margin-left-xxs\">Facebook</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a class=\"button button-clear button-sm has-inner-focus button-block text-decoration-none justify-content-flex-start share-twitter\" data-bi-name=\"twitter\" data-page-action-item=\"overflow-all\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-xlogo-share font-size-xxs\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"margin-left-xxs\">x.com</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a class=\"button button-clear button-sm has-inner-focus button-block text-decoration-none justify-content-flex-start share-linkedin\" data-bi-name=\"linkedin\" data-page-action-item=\"overflow-all\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-linked-in-logo font-size-sm color-primary\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"margin-left-xxs\">LinkedIn</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<a class=\"button button-clear button-sm button-block has-inner-focus text-decoration-none justify-content-flex-start margin-bottom-xxs share-email\" data-bi-name=\"email\" data-page-action-item=\"overflow-all\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-mail-message font-size-sm color-primary\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"margin-left-xxs\">Email</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</a><hr />\n\t\t\t\t\t\t\t\t\t\t\t\t\t<button\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tclass=\"button button-block button-clear button-sm justify-content-flex-start has-inner-focus margin-top-xxs\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttitle=\"Print\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\ttype=\"button\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\taria-label=\"Print\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-bi-name=\"print\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-page-action-item=\"overflow-all\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-popover-close\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-print-page\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tdata-check-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-print font-size-sm color-primary\"></span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"margin-left-xxs\">Print</span>\n\t\t\t\t\t\t\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</details>\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</div></div>\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<!-- end article-header --><div>\n\t\t\t\t\t\t\t\t<button type=\"button\" class=\"border contents-button button button-clear button-sm is-hidden-tablet has-inner-focus\" data-bi-name=\"contents-expand\" data-contents-button hidden>\n\t\t\t\t\t\t\t\t\t<span class=\"icon\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"docon docon-editor-list-bullet\" aria-hidden=\"true\"></span>\n\t\t\t\t\t\t\t\t\t</span><span class=\"contents-expand-title\">Table of contents</span></button>\n\t\t\t\t\t\t\t</div><!-- end mobile-contents button  -->\n\n\t\t\t\t\t\t\t<div class=\"content \"><h1 id=\"mosaic-ai-vector-search\">Mosaic AI Vector Search</h1><div class=\"display-flex justify-content-space-between align-items-center flex-wrap-wrap page-metadata-container\">\n\t\t\t\t\t\t\t\t\t\t<div class=\"margin-right-xxs\">\n\t\t\t\t\t\t\t\t\t\t\t<ul class=\"metadata page-metadata\" data-bi-name=\"page info\" lang=\"en-us\" dir=\"ltr\"><li>Article</li><li class=\"visibility-hidden-visual-diff\"><time class=\"is-invisible\" data-article-date aria-label=\"Article review date\" datetime=\"2025-02-03T08:00:00Z\" data-article-date-source=\"calculated\">02/03/2025</time>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t</li><li class=\"contributors-holder display-none-print\">\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t<button type=\"button\" class=\"contributors-button link-button\" data-bi-name=\"contributors\">2 contributors</button>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t</li></ul>\n\t\t\t\t\t\t\t\t\t\t</div>\n<div id=\"user-feedback\" class=\"margin-block-xxs display-none-print\" data-hide-on-archived>\n\t<button\n\t\tid=\"user-feedback-button\"\n\t\tdata-test-id=\"conceptual-feedback-button\"\n\t\tclass=\"button button-sm button-clear button-primary\"\n\t\ttype=\"button\"\n\t\tdata-bi-name=\"user-feedback-button\"\n\t\tdata-user-feedback-button\n\t>\n\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t<span class=\"docon docon-like\"></span>\n\t\t</span>\n\t\t<span>Feedback</span>\n\t</button>\n</div></div><nav id=\"center-doc-outline\" class=\"doc-outline is-hidden-desktop display-none-print margin-bottom-sm\" data-bi-name=\"intopic toc\" aria-label=\"In this article\">\n\t\t\t\t\t\t\t\t\t\t\t<h2 id=\"ms--in-this-article\" class=\"title is-6 margin-block-xs\">In this article</h2>\n\t\t\t\t\t\t\t\t\t\t</nav><!-- <content> --><p>This article gives an overview of Databricks’ vector database solution, Mosaic AI Vector Search, including what it is and how it works.</p>\n<h2 id=\"what-is-mosaic-ai-vector-search\">What is Mosaic AI Vector Search?</h2>\n<p>Mosaic AI Vector Search is a vector database that is built into the Databricks Data Intelligence Platform and integrated with its governance and productivity tools. A vector database is a database that is optimized to store and retrieve embeddings. Embeddings are mathematical representations of the semantic content of data, typically text or image data. Embeddings are generated by a large language model and are a key component of many GenAI applications that depend on finding documents or images that are similar to each other. Examples are RAG systems, recommender systems, and image and video recognition.</p>\n<p>With Mosaic AI Vector Search, you create a vector search index from a Delta table. The index includes embedded data with metadata. You can then query the index using a REST API to identify the most similar vectors and return the associated documents. You can structure the index to automatically sync when the underlying Delta table is updated.</p>\n<p>Mosaic AI Vector Search supports the following:</p>\n<ul>\n<li><a href=\"#hybrid\" data-linktype=\"self-bookmark\">Hybrid keyword-similarity search</a>.</li>\n<li><a href=\"create-query-vector-search#filters\" data-linktype=\"relative-path\">Filtering</a>.</li>\n<li><a href=\"../security/auth/access-control/#vector-search-endpoints\" data-linktype=\"relative-path\">Access control lists (ACLs) to manage vector search endpoints</a>.</li>\n<li><a href=\"create-query-vector-search#create-a-vector-search-index\" data-linktype=\"relative-path\">Sync only selected columns</a>.</li>\n<li><a href=\"create-query-vector-search#create-a-vector-search-index\" data-linktype=\"relative-path\">Save and sync generated embeddings</a>.</li>\n</ul>\n<h2 id=\"how-does-mosaic-ai-vector-search-work\"><a id=\"hybrid\"></a>How does Mosaic AI Vector Search work?</h2>\n<p>Mosaic AI Vector Search uses the Hierarchical Navigable Small World (HNSW) algorithm for its approximate nearest neighbor searches and the L2 distance distance metric to measure embedding vector similarity. If you want to use cosine similarity you need to normalize your datapoint embeddings before feeding them into vector search. When the data points are normalized, the ranking produced by L2 distance is the same as the ranking produces by cosine similarity.</p>\n<p>Mosaic AI Vector Search also supports hybrid keyword-similarity search, which combines vector-based embedding search with traditional keyword-based search techniques. This approach matches exact words in the query while also using a vector-based similarity search to capture the semantic relationships and context of the query.</p>\n<p>By integrating these two techniques, hybrid keyword-similarity search retrieves documents that contain not only the exact keywords but also those that are conceptually similar, providing more comprehensive and relevant search results. This method is particularly useful in RAG applications where source data has unique keywords such as SKUs or identifiers that are not well suited to pure similarity search.</p>\n<p>For details about the API, see the <a href=\"https://api-docs.databricks.com/python/vector-search/databricks.vector_search.html\" data-linktype=\"external\">Python SDK reference</a> and <a href=\"create-query-vector-search#query\" data-linktype=\"relative-path\">Query a vector search endpoint</a>.</p>\n<h3 id=\"similarity-search-calculation\">Similarity search calculation</h3>\n<p>The similarity search calculation uses the following formula:</p>\n<div class=\"mx-imgBorder\">\n<p><img src=\"../_static/images/generative-ai/similarity-score.png\" alt=\"reciprocal of 1 plus the squared distance\" data-linktype=\"relative-path\"></p>\n</div>\n<p>where <code>dist</code> is the Euclidean distance between the query <code>q</code> and the index entry <code>x</code>:</p>\n<div class=\"mx-imgBorder\">\n<p><img src=\"../_static/images/generative-ai/euclidean-distance.png\" alt=\"Eucidean distance, square root of the sum of squared differences\" data-linktype=\"relative-path\"></p>\n</div>\n<h3 id=\"keyword-search-algorithm\">Keyword search algorithm</h3>\n<p>Relevance scores are calculated using <a href=\"https://en.wikipedia.org/wiki/Okapi_BM25\" data-linktype=\"external\">Okapi BM25</a>. All text or \n\n*** WARNING: max output size exceeded, skipping output. ***\n\nelative-path\">Use Delta Lake change data feed on Azure Databricks</a>.</li>\n<li>To create a vector search index, you must have CREATE TABLE privileges on the catalog schema where the index will be created.</li>\n</ul>\n<p>Permission to create and manage vector search endpoints is configured using access control lists. See <a href=\"../security/auth/access-control/#vector-search-endpoints\" data-linktype=\"relative-path\">Vector search endpoint ACLs</a>.</p>\n<h2 id=\"authentication\"><a id=\"data-protection-and-authentication\"></a>Data protection and authentication</h2>\n<p>Databricks implements the following security controls to protect your data:</p>\n<ul>\n<li>Every customer request to Mosaic AI Vector Search is logically isolated, authenticated, and authorized.</li>\n<li>Mosaic AI Vector Search encrypts all data at rest (AES-256) and in transit (TLS 1.2+).</li>\n</ul>\n<p>Mosaic AI Vector Search supports two modes of authentication:</p>\n<ul>\n<li><p>Service principal token. An admin can generate a service principal token and pass it to the SDK or API. See <a href=\"../admin/users-groups/service-principals\" data-linktype=\"relative-path\">use service principals</a>. For production use cases, Databricks recommends using a service principal token.</p>\n<pre><code class=\"lang-python\"># Pass in a service principal\nvsc = VectorSearchClient(workspace_url=\"...\",\n        service_principal_client_id=\"...\",\n        service_principal_client_secret=\"...\"\n        )\n</code></pre>\n</li>\n<li><p>Personal access token. You can use a personal access token to authenticate with Mosaic AI Vector Search. See <a href=\"../dev-tools/auth/pat\" data-linktype=\"relative-path\">personal access authentication token</a>. If you use the SDK in a notebook environment, the SDK automatically generates a PAT token for authentication.</p>\n<pre><code class=\"lang-python\"># Pass in the PAT token\nclient = VectorSearchClient(workspace_url=\"...\", personal_access_token=\"...\")\n</code></pre>\n</li>\n</ul>\n<p><a href=\"../security/keys/customer-managed-keys\" data-linktype=\"relative-path\">Customer Managed Keys (CMK)</a> are supported on endpoints created on or after May 8, 2024.</p>\n<h2 id=\"monitor-usage-and-costs\">Monitor usage and costs</h2>\n<p>The billable usage system table lets you monitor usage and costs associated with vector search indexes and endpoints. Here is an example query:</p>\n<pre><code class=\"lang-sql\">WITH all_vector_search_usage (\n  SELECT *,\n         CASE WHEN usage_metadata.endpoint_name IS NULL THEN 'ingest'\n              WHEN usage_type = \"STORAGE_SPACE\" THEN 'storage'\n              ELSE 'serving'\n        END as workload_type\n    FROM system.billing.usage\n   WHERE billing_origin_product = 'VECTOR_SEARCH'\n),\ndaily_dbus AS (\n  SELECT workspace_id,\n       cloud,\n       usage_date,\n       workload_type,\n       usage_metadata.endpoint_name as vector_search_endpoint,\n       CASE WHEN workload_type = 'serving' THEN SUM(usage_quantity)\n            WHEN workload_type = 'ingest' THEN SUM(usage_quantity)\n            ELSE null\n            END as dbus,\n       CASE WHEN workload_type = 'storage' THEN SUM(usage_quantity)\n            ELSE null\n            END as dsus\n FROM all_vector_search_usage\n GROUP BY all\nORDER BY 1,2,3,4,5 DESC\n)\nSELECT * FROM daily_dbus\n</code></pre>\n<p>For details about the contents of the billing usage table, see <a href=\"../admin/system-tables/billing\" data-linktype=\"relative-path\">Billable usage system table reference</a>. Additional queries are in the following example notebook.</p>\n<h4 id=\"vector-search-system-tables-queries-notebook\">Vector search system tables queries notebook</h4>\n<p><a href=\"https://docs.databricks.com/notebooks/source/generative-ai/vector-search-system-tables-queries.html\" data-linktype=\"external\">Get notebook</a></p>\n<h2 id=\"resource-and-data-size-limits\">Resource and data size limits</h2>\n<p>The following table summarizes resource and data size limits for vector search endpoints and indexes:</p>\n<table>\n<thead>\n<tr>\n<th>Resource</th>\n<th>Granularity</th>\n<th>Limit</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Vector search endpoints</td>\n<td>Per workspace</td>\n<td>100</td>\n</tr>\n<tr>\n<td>Embeddings</td>\n<td>Per endpoint</td>\n<td>320,000,000</td>\n</tr>\n<tr>\n<td>Embedding dimension</td>\n<td>Per index</td>\n<td>4096</td>\n</tr>\n<tr>\n<td>Indexes</td>\n<td>Per endpoint</td>\n<td>50</td>\n</tr>\n<tr>\n<td>Columns</td>\n<td>Per index</td>\n<td>50</td>\n</tr>\n<tr>\n<td>Columns</td>\n<td></td>\n<td>Supported types: Bytes, short, integer, long, float, double, boolean, string, timestamp, date</td>\n</tr>\n<tr>\n<td>Metadata fields</td>\n<td>Per index</td>\n<td>50</td>\n</tr>\n<tr>\n<td>Index name</td>\n<td>Per index</td>\n<td>128 characters</td>\n</tr>\n</tbody>\n</table>\n<p>The following limits apply to the creation and update of vector search indexes:</p>\n<table>\n<thead>\n<tr>\n<th>Resource</th>\n<th>Granularity</th>\n<th>Limit</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Row size for Delta Sync Index</td>\n<td>Per index</td>\n<td>100KB</td>\n</tr>\n<tr>\n<td>Embedding source column size for Delta Sync index</td>\n<td>Per Index</td>\n<td>32764 bytes</td>\n</tr>\n<tr>\n<td>Bulk upsert request size limit for Direct Vector index</td>\n<td>Per Index</td>\n<td>10MB</td>\n</tr>\n<tr>\n<td>Bulk delete request size limit for Direct Vector index</td>\n<td>Per Index</td>\n<td>10MB</td>\n</tr>\n</tbody>\n</table>\n<p>The following limits apply to the query API.</p>\n<table>\n<thead>\n<tr>\n<th>Resource</th>\n<th>Granularity</th>\n<th>Limit</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Query text length</td>\n<td>Per query</td>\n<td>32764 bytes</td>\n</tr>\n<tr>\n<td>Maximum number of results returned</td>\n<td>Per query</td>\n<td>10,000</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"limitation\">Limitation</h2>\n<p>Row and column level permissions are not supported. However, you can implement your own application level ACLs using the filter API.</p>\n<h2 id=\"additional-resources\">Additional resources</h2>\n<ul>\n<li><a href=\"https://www.databricks.com/resources/demos/tutorials/data-science-and-ai/lakehouse-ai-deploy-your-llm-chatbot\" data-linktype=\"external\">Deploy Your LLM Chatbot With Retrieval Augmented Generation (RAG), Foundation Models and Vector Search</a>.</li>\n<li><a href=\"create-query-vector-search\" data-linktype=\"relative-path\">How to create and query a vector search index</a>.</li>\n<li><a href=\"create-query-vector-search#example-notebooks\" data-linktype=\"relative-path\">Example notebooks</a></li>\n</ul>\n</div><div id=\"ms--inline-notifications\" class=\"margin-block-xs\" data-bi-name=\"inline-notification\"></div><div id=\"assertive-live-region\" role=\"alert\" aria-live=\"assertive\" class=\"visually-hidden\" aria-relevant=\"additions\" aria-atomic=\"true\"></div>\n\t\t\t\t\t\t\t<div id=\"polite-live-region\" role=\"status\" aria-live=\"polite\" class=\"visually-hidden\" aria-relevant=\"additions\" aria-atomic=\"true\"></div>\n\t\t\t\t\t\t\t<!-- </content> -->\n\n\t\t\t\t\t\t</main><!-- recommendations section --><!-- end recommendations section -->\n\n\t\t\t\t\t\t<!-- feedback section --><section id=\"site-user-feedback-footer\" class=\"font-size-sm margin-top-md\" data-test-id=\"site-user-feedback-footer\" data-bi-name=\"site-feedback-section\">\n\t<hr class=\"hr\" />\n\t<h2 id=\"feedback\" class=\"title is-3\">Feedback</h2>\n\t<div class=\"display-flex flex-wrap-wrap align-items-center\">\n\t\t<p class=\"font-weight-semibold margin-xxs margin-left-none\">Was this page helpful?</p>\n\t\t<div class=\"buttons\">\n\t\t\t<button\n\t\t\t\tclass=\"thumb-rating-button like button button-primary button-sm\"\n\t\t\t\tdata-test-id=\"footer-rating-yes\"\n\t\t\t\tdata-binary-rating-response=\"rating-yes\"\n\t\t\t\ttype=\"button\"\n\t\t\t\ttitle=\"This article is helpful\"\n\t\t\t\tdata-bi-name=\"button-rating-yes\"\n\t\t\t\taria-pressed=\"false\"\n\t\t\t>\n\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t<span class=\"docon docon-like\"></span>\n\t\t\t\t</span>\n\t\t\t\t<span>Yes</span>\n\t\t\t</button>\n\t\t\t<button\n\t\t\t\tclass=\"thumb-rating-button dislike button button-primary button-sm\"\n\t\t\t\tdata-test-id=\"footer-rating-no\"\n\t\t\t\tdata-binary-rating-response=\"rating-no\"\n\t\t\t\ttype=\"button\"\n\t\t\t\ttitle=\"This article is not helpful\"\n\t\t\t\tdata-bi-name=\"button-rating-no\"\n\t\t\t\taria-pressed=\"false\"\n\t\t\t>\n\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t<span class=\"docon docon-dislike\"></span>\n\t\t\t\t</span>\n\t\t\t\t<span>No</span>\n\t\t\t</button>\n\t\t</div>\n\t</div><div class=\"display-flex flex-wrap-wrap margin-top-xxs\"><div>\n\t\t\t<a\n\t\t\t\tdata-bi-name=\"provide-feedback-cta\"\n\t\t\t\tclass=\"has-external-link-indicator\"\n\t\t\t\thref=\"https://feedback.azure.com/d365community/forum/2efba7dc-ef24-ec11-b6e6-000d3a4f0da0\"\n\t\t\t\tdata-bi-name=\"product-feedback\"\n\t\t\t>\n\t\t\t\t<span>Provide product feedback</span>\n\t\t\t</a></div></div>\n\n</section><!-- end feedback section -->\n\n\t\t\t\t\t\t<!-- feedback report section --><!-- end feedback report section --><aside\n\t\t\t\t\t\t\t\tid=\"ms--additional-resources-mobile\"\n\t\t\t\t\t\t\t\taria-label=\"Additional resources\"\n\t\t\t\t\t\t\t\tclass=\"display-none-desktop display-none-print\"\n\t\t\t\t\t\t\t>\n\t\t\t\t\t\t\t\t<hr class=\"hr\" hidden />\n\t\t\t\t\t\t\t\t<h2 id=\"ms--additional-resources-mobile-heading\" class=\"title is-3\" hidden>Additional resources</h2>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-recommendations-mobile\" data-bi-name=\"recommendations\" hidden></section>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-training-mobile\" data-bi-name=\"learning-resources-card\" hidden></section>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-events-mobile\" data-bi-name=\"events-card\" hidden></section>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-qna-mobile\" data-bi-name=\"qna-link-card\" hidden></section>\n\t\t\t\t\t\t\t</aside><div class=\"border-top is-visible-interactive has-default-focus margin-top-sm \"><footer id=\"footer-interactive\" data-bi-name=\"footer\" class=\"footer-layout\"><div class=\"display-flex gap-xs flex-wrap-wrap is-full-height padding-right-lg-desktop\"><a\n\t\t\t\tdata-mscc-ic=\"false\"\n\t\t\t\tclass=\"locale-selector-link button button-sm button-clear flex-shrink-0\"\n\t\t\t\thref=\"#\"\n\t\t\t\tdata-bi-name=\"select-locale\">\n\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t<span class=\"docon docon-world\"></span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span class=\"local-selector-link-text\"></span></a><div class=\"ccpa-privacy-link\" data-ccpa-privacy-link hidden>\n<a\n\thref=\"https://aka.ms/yourcaliforniaprivacychoices\"\n\tclass=\"button button-sm button-clear flex-shrink-0\"\n\tdata-mscc-ic=\"false\"\n\tdata-bi-name=\"your-privacy-choices\"\n>\n\t<svg\n\t\txmlns=\"http://www.w3.org/2000/svg\"\n\t\tviewBox=\"0 0 30 14\"\n\t\txml:space=\"preserve\"\n\t\theight=\"16\"\n\t\twidth=\"43\"\n\t\taria-hidden=\"true\"\n\t\tfocusable=\"false\"\n\t>\n\t\t<path d=\"M7.4 12.8h6.8l3.1-11.6H7.4C4.2 1.2 1.6 3.8 1.6 7s2.6 5.8 5.8 5.8z\" style=\"fill-rule:evenodd;clip-rule:evenodd;fill:#fff\"></path>\n\t\t<path d=\"M22.6 0H7.4c-3.9 0-7 3.1-7 7s3.1 7 7 7h15.2c3.9 0 7-3.1 7-7s-3.2-7-7-7zm-21 7c0-3.2 2.6-5.8 5.8-5.8h9.9l-3.1 11.6H7.4c-3.2 0-5.8-2.6-5.8-5.8z\" style=\"fill-rule:evenodd;clip-rule:evenodd;fill:#06f\"></path>\n\t\t<path d=\"M24.6 4c.2.2.2.6 0 .8L22.5 7l2.2 2.2c.2.2.2.6 0 .8-.2.2-.6.2-.8 0l-2.2-2.2-2.2 2.2c-.2.2-.6.2-.8 0-.2-.2-.2-.6 0-.8L20.8 7l-2.2-2.2c-.2-.2-.2-.6 0-.8.2-.2.6-.2.8 0l2.2 2.2L23.8 4c.2-.2.6-.2.8 0z\" style=\"fill:#fff\"></path>\n\t\t<path d=\"M12.7 4.1c.2.2.3.6.1.8L8.6 9.8c-.1.1-.2.2-.3.2-.2.1-.5.1-.7-.1L5.4 7.7c-.2-.2-.2-.6 0-.8.2-.2.6-.2.8 0L8 8.6l3.8-4.5c.2-.2.6-.2.9 0z\" style=\"fill:#06f\"></path>\n\t</svg>\n\t<span>Your Privacy Choices</span>\n</a>\n\n\t\t</div>\n\t\t<div class=\"flex-shrink-0\">\n<div class=\"dropdown has-caret-up\">\n\t<button class=\"dropdown-trigger button button-clear button-sm has-inner-focus theme-dropdown-trigger\"\n\t\taria-controls=\"theme-menu-interactive\" aria-expanded=\"false\" title=\"Theme\" data-bi-name=\"theme\">\n\t\t<span class=\"icon\">\n\t\t\t<span class=\"docon docon-sun\" aria-hidden=\"true\"></span>\n\t\t</span>\n\t\t<span>Theme</span>\n\t\t<span class=\"icon expanded-indicator\" aria-hidden=\"true\">\n\t\t\t<span class=\"docon docon-chevron-down-light\"></span>\n\t\t</span>\n\t</button>\n\t<div class=\"dropdown-menu\" id=\"theme-menu-interactive\" role=\"menu\">\n\t\t<ul class=\"theme-selector padding-xxs\" role=\"none\">\n\t\t\t<li class=\"theme display-block\" role=\"menuitem\">\n\t\t\t\t<button class=\"button button-clear button-sm theme-control button-block justify-content-flex-start\"\n\t\t\t\t\tdata-theme-to=\"light\">\n\t\t\t\t\t<span class=\"theme-light margin-right-xxs\">\n\t\t\t\t\t\t<span\n\t\t\t\t\t\t\tclass=\"theme-selector-icon border display-inline-block has-body-background\"\n\t\t\t\t\t\t\taria-hidden=\"true\">\n\t\t\t\t\t\t\t<svg class=\"svg\" xmlns=\"http://www.w3.org/2000/svg\"\n\t\t\t\t\t\t\t\tviewBox=\"0 0 22 14\">\n\t\t\t\t\t\t\t\t<rect width=\"22\" height=\"14\" class=\"has-fill-body-background\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"5\" width=\"12\" height=\"4\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"8\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"11\" y=\"2\" width=\"3\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"10\" width=\"7\" height=\"2\" rx=\"0.3\" class=\"has-fill-primary\" />\n\t\t\t\t\t\t\t\t<rect x=\"19\" y=\"1\" width=\"2\" height=\"2\" rx=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t</svg>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span>Light</span>\n\t\t\t\t</button>\n\t\t\t</li>\n\t\t\t<li class=\"theme display-block\" role=\"menuitem\">\n\t\t\t\t<button class=\"button button-clear button-sm theme-control button-block justify-content-flex-start\"\n\t\t\t\t\tdata-theme-to=\"dark\">\n\t\t\t\t\t<span class=\"theme-dark margin-right-xxs\">\n\t\t\t\t\t\t<span\n\t\t\t\t\t\t\tclass=\"border theme-selector-icon display-inline-block has-body-background\"\n\t\t\t\t\t\t\taria-hidden=\"true\">\n\t\t\t\t\t\t\t<svg class=\"svg\" xmlns=\"http://www.w3.org/2000/svg\"\n\t\t\t\t\t\t\t\tviewBox=\"0 0 22 14\">\n\t\t\t\t\t\t\t\t<rect width=\"22\" height=\"14\" class=\"has-fill-body-background\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"5\" width=\"12\" height=\"4\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"8\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"11\" y=\"2\" width=\"3\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"10\" width=\"7\" height=\"2\" rx=\"0.3\" class=\"has-fill-primary\" />\n\t\t\t\t\t\t\t\t<rect x=\"19\" y=\"1\" width=\"2\" height=\"2\" rx=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t</svg>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span>Dark</span>\n\t\t\t\t</button>\n\t\t\t</li>\n\t\t\t<li class=\"theme display-block\" role=\"menuitem\">\n\t\t\t\t<button class=\"button button-clear button-sm theme-control button-block justify-content-flex-start\"\n\t\t\t\t\tdata-theme-to=\"high-contrast\">\n\t\t\t\t\t<span class=\"theme-high-contrast margin-right-xxs\">\n\t\t\t\t\t\t<span\n\t\t\t\t\t\t\tclass=\"border theme-selector-icon display-inline-block has-body-background\"\n\t\t\t\t\t\t\taria-hidden=\"true\">\n\t\t\t\t\t\t\t<svg class=\"svg\" xmlns=\"http://www.w3.org/2000/svg\"\n\t\t\t\t\t\t\t\tviewBox=\"0 0 22 14\">\n\t\t\t\t\t\t\t\t<rect width=\"22\" height=\"14\" class=\"has-fill-body-background\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"5\" width=\"12\" height=\"4\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"8\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"11\" y=\"2\" width=\"3\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"10\" width=\"7\" height=\"2\" rx=\"0.3\" class=\"has-fill-primary\" />\n\t\t\t\t\t\t\t\t<rect x=\"19\" y=\"1\" width=\"2\" height=\"2\" rx=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t</svg>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span>High contrast</span>\n\t\t\t\t</button>\n\t\t\t</li>\n\t\t</ul>\n\t</div>\n</div>\n\n\t\t</div>\n\t</div>\n\t<ul class=\"links\" data-bi-name=\"footerlinks\">\n\t\t<li class=\"manage-cookies-holder\" hidden></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"/en-us/previous-versions/\" data-bi-name=\"archivelink\">Previous Versions</a></li>\n\t\t\t\t<li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"https://techcommunity.microsoft.com/t5/microsoft-learn-blog/bg-p/MicrosoftLearnBlog\" data-bi-name=\"bloglink\">Blog</a></li>\n\t\t\t\t<li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"/en-us/contribute/\" data-bi-name=\"contributorGuide\">Contribute</a></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"https://go.microsoft.com/fwlink/?LinkId=521839\" data-bi-name=\"privacy\">Privacy</a></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"/en-us/legal/termsofuse\" data-bi-name=\"termsofuse\">Terms of Use</a></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"https://www.microsoft.com/legal/intellectualproperty/Trademarks/\" data-bi-name=\"trademarks\">Trademarks</a></li><li>&copy; Microsoft 2025</li>\n\t</ul>\n</footer></div></div><div\n\t\t\t\t\t\t\tid=\"ms--additional-resources\"\n\t\t\t\t\t\t\tclass=\"right-container column is-4-desktop display-none display-block-desktop\"\n\t\t\t\t\t\t\tdata-bi-name=\"pageactions\"\n\t\t\t\t\t\t\trole=\"complementary\"\n\t\t\t\t\t\t\taria-label=\"Additional resources\"\n\t\t\t\t\t\t>\n\t\t\t\t\t\t\t<div id=\"affixed-right-container\" class=\"margin-top-sm-tablet\" data-bi-name=\"right-column\">\n\t\t\t\t\t\t\t\t<h2 id=\"ms--additional-resources-heading\" class=\"title is-6 margin-top-md\" hidden>Additional resources</h2>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-events\" data-bi-name=\"events-card\" hidden></section>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-training\" data-bi-name=\"learning-resources-card\" hidden></section>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-recommendations\" data-bi-name=\"recommendations\" hidden></section>\n\t\t\t\t\t\t\t\t<nav id=\"side-doc-outline\" class=\"doc-outline\" data-bi-name=\"intopic toc\" aria-label=\"In this article\">\n\t\t\t\t\t\t\t\t\t<h3>In this article</h3>\n\t\t\t\t\t\t\t\t</nav>\n\t\t\t\t\t\t\t\t<section id=\"right-rail-qna\" class=\"margin-top-xxs\" data-bi-name=\"qna-link-card\" hidden></section>\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t</div></div>\n\t\t\t\t<!--end of div.columns -->\n\n\t\t\t</section>\n\t\t\t<!--end of .primary-holder -->\n\n\t\t\t<!-- interactive container -->\n\t\t\t<aside id=\"interactive-container\" class=\"interactive-container is-visible-interactive column has-body-background-dark \">\n\t\t\t</aside>\n\t\t\t<!-- end of interactive container -->\n\t\t</div>\n\n\t</div>\n\t<!--end of .mainContainer -->\n\n\t<section class=\"border-top has-default-focus is-hidden-interactive margin-top-sm \"><footer id=\"footer\" data-bi-name=\"footer\" class=\"footer-layout uhf-container has-padding\" role=\"contentinfo\"><div class=\"display-flex gap-xs flex-wrap-wrap is-full-height padding-right-lg-desktop\"><a\n\t\t\t\tdata-mscc-ic=\"false\"\n\t\t\t\tclass=\"locale-selector-link button button-sm button-clear flex-shrink-0\"\n\t\t\t\thref=\"#\"\n\t\t\t\tdata-bi-name=\"select-locale\">\n\t\t\t\t\t<span class=\"icon\" aria-hidden=\"true\">\n\t\t\t\t\t\t<span class=\"docon docon-world\"></span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span class=\"local-selector-link-text\"></span></a><div class=\"ccpa-privacy-link\" data-ccpa-privacy-link hidden>\n<a\n\thref=\"https://aka.ms/yourcaliforniaprivacychoices\"\n\tclass=\"button button-sm button-clear flex-shrink-0\"\n\tdata-mscc-ic=\"false\"\n\tdata-bi-name=\"your-privacy-choices\"\n>\n\t<svg\n\t\txmlns=\"http://www.w3.org/2000/svg\"\n\t\tviewBox=\"0 0 30 14\"\n\t\txml:space=\"preserve\"\n\t\theight=\"16\"\n\t\twidth=\"43\"\n\t\taria-hidden=\"true\"\n\t\tfocusable=\"false\"\n\t>\n\t\t<path d=\"M7.4 12.8h6.8l3.1-11.6H7.4C4.2 1.2 1.6 3.8 1.6 7s2.6 5.8 5.8 5.8z\" style=\"fill-rule:evenodd;clip-rule:evenodd;fill:#fff\"></path>\n\t\t<path d=\"M22.6 0H7.4c-3.9 0-7 3.1-7 7s3.1 7 7 7h15.2c3.9 0 7-3.1 7-7s-3.2-7-7-7zm-21 7c0-3.2 2.6-5.8 5.8-5.8h9.9l-3.1 11.6H7.4c-3.2 0-5.8-2.6-5.8-5.8z\" style=\"fill-rule:evenodd;clip-rule:evenodd;fill:#06f\"></path>\n\t\t<path d=\"M24.6 4c.2.2.2.6 0 .8L22.5 7l2.2 2.2c.2.2.2.6 0 .8-.2.2-.6.2-.8 0l-2.2-2.2-2.2 2.2c-.2.2-.6.2-.8 0-.2-.2-.2-.6 0-.8L20.8 7l-2.2-2.2c-.2-.2-.2-.6 0-.8.2-.2.6-.2.8 0l2.2 2.2L23.8 4c.2-.2.6-.2.8 0z\" style=\"fill:#fff\"></path>\n\t\t<path d=\"M12.7 4.1c.2.2.3.6.1.8L8.6 9.8c-.1.1-.2.2-.3.2-.2.1-.5.1-.7-.1L5.4 7.7c-.2-.2-.2-.6 0-.8.2-.2.6-.2.8 0L8 8.6l3.8-4.5c.2-.2.6-.2.9 0z\" style=\"fill:#06f\"></path>\n\t</svg>\n\t<span>Your Privacy Choices</span>\n</a>\n\n\t\t</div>\n\t\t<div class=\"flex-shrink-0\">\n<div class=\"dropdown has-caret-up\">\n\t<button class=\"dropdown-trigger button button-clear button-sm has-inner-focus theme-dropdown-trigger\"\n\t\taria-controls=\"theme-menu\" aria-expanded=\"false\" title=\"Theme\" data-bi-name=\"theme\">\n\t\t<span class=\"icon\">\n\t\t\t<span class=\"docon docon-sun\" aria-hidden=\"true\"></span>\n\t\t</span>\n\t\t<span>Theme</span>\n\t\t<span class=\"icon expanded-indicator\" aria-hidden=\"true\">\n\t\t\t<span class=\"docon docon-chevron-down-light\"></span>\n\t\t</span>\n\t</button>\n\t<div class=\"dropdown-menu\" id=\"theme-menu\" role=\"menu\">\n\t\t<ul class=\"theme-selector padding-xxs\" role=\"none\">\n\t\t\t<li class=\"theme display-block\" role=\"menuitem\">\n\t\t\t\t<button class=\"button button-clear button-sm theme-control button-block justify-content-flex-start\"\n\t\t\t\t\tdata-theme-to=\"light\">\n\t\t\t\t\t<span class=\"theme-light margin-right-xxs\">\n\t\t\t\t\t\t<span\n\t\t\t\t\t\t\tclass=\"theme-selector-icon border display-inline-block has-body-background\"\n\t\t\t\t\t\t\taria-hidden=\"true\">\n\t\t\t\t\t\t\t<svg class=\"svg\" xmlns=\"http://www.w3.org/2000/svg\"\n\t\t\t\t\t\t\t\tviewBox=\"0 0 22 14\">\n\t\t\t\t\t\t\t\t<rect width=\"22\" height=\"14\" class=\"has-fill-body-background\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"5\" width=\"12\" height=\"4\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"8\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"11\" y=\"2\" width=\"3\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"10\" width=\"7\" height=\"2\" rx=\"0.3\" class=\"has-fill-primary\" />\n\t\t\t\t\t\t\t\t<rect x=\"19\" y=\"1\" width=\"2\" height=\"2\" rx=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t</svg>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span>Light</span>\n\t\t\t\t</button>\n\t\t\t</li>\n\t\t\t<li class=\"theme display-block\" role=\"menuitem\">\n\t\t\t\t<button class=\"button button-clear button-sm theme-control button-block justify-content-flex-start\"\n\t\t\t\t\tdata-theme-to=\"dark\">\n\t\t\t\t\t<span class=\"theme-dark margin-right-xxs\">\n\t\t\t\t\t\t<span\n\t\t\t\t\t\t\tclass=\"border theme-selector-icon display-inline-block has-body-background\"\n\t\t\t\t\t\t\taria-hidden=\"true\">\n\t\t\t\t\t\t\t<svg class=\"svg\" xmlns=\"http://www.w3.org/2000/svg\"\n\t\t\t\t\t\t\t\tviewBox=\"0 0 22 14\">\n\t\t\t\t\t\t\t\t<rect width=\"22\" height=\"14\" class=\"has-fill-body-background\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"5\" width=\"12\" height=\"4\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"8\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"11\" y=\"2\" width=\"3\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"10\" width=\"7\" height=\"2\" rx=\"0.3\" class=\"has-fill-primary\" />\n\t\t\t\t\t\t\t\t<rect x=\"19\" y=\"1\" width=\"2\" height=\"2\" rx=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t</svg>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span>Dark</span>\n\t\t\t\t</button>\n\t\t\t</li>\n\t\t\t<li class=\"theme display-block\" role=\"menuitem\">\n\t\t\t\t<button class=\"button button-clear button-sm theme-control button-block justify-content-flex-start\"\n\t\t\t\t\tdata-theme-to=\"high-contrast\">\n\t\t\t\t\t<span class=\"theme-high-contrast margin-right-xxs\">\n\t\t\t\t\t\t<span\n\t\t\t\t\t\t\tclass=\"border theme-selector-icon display-inline-block has-body-background\"\n\t\t\t\t\t\t\taria-hidden=\"true\">\n\t\t\t\t\t\t\t<svg class=\"svg\" xmlns=\"http://www.w3.org/2000/svg\"\n\t\t\t\t\t\t\t\tviewBox=\"0 0 22 14\">\n\t\t\t\t\t\t\t\t<rect width=\"22\" height=\"14\" class=\"has-fill-body-background\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"5\" width=\"12\" height=\"4\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"8\" y=\"2\" width=\"2\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"11\" y=\"2\" width=\"3\" height=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"1\" y=\"1\" width=\"2\" height=\"2\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t\t<rect x=\"5\" y=\"10\" width=\"7\" height=\"2\" rx=\"0.3\" class=\"has-fill-primary\" />\n\t\t\t\t\t\t\t\t<rect x=\"19\" y=\"1\" width=\"2\" height=\"2\" rx=\"1\" class=\"has-fill-secondary\" />\n\t\t\t\t\t\t\t</svg>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</span>\n\t\t\t\t\t<span>High contrast</span>\n\t\t\t\t</button>\n\t\t\t</li>\n\t\t</ul>\n\t</div>\n</div>\n\n\t\t</div>\n\t</div>\n\t<ul class=\"links\" data-bi-name=\"footerlinks\">\n\t\t<li class=\"manage-cookies-holder\" hidden></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"/en-us/previous-versions/\" data-bi-name=\"archivelink\">Previous Versions</a></li>\n\t\t\t\t<li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"https://techcommunity.microsoft.com/t5/microsoft-learn-blog/bg-p/MicrosoftLearnBlog\" data-bi-name=\"bloglink\">Blog</a></li>\n\t\t\t\t<li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"/en-us/contribute/\" data-bi-name=\"contributorGuide\">Contribute</a></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"https://go.microsoft.com/fwlink/?LinkId=521839\" data-bi-name=\"privacy\">Privacy</a></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"/en-us/legal/termsofuse\" data-bi-name=\"termsofuse\">Terms of Use</a></li><li><a class=\"external-link-indicator\" data-mscc-ic=\"false\" href=\"https://www.microsoft.com/legal/intellectualproperty/Trademarks/\" data-bi-name=\"trademarks\">Trademarks</a></li><li>&copy; Microsoft 2025</li>\n\t</ul>\n</footer>\n\n\t</section>\n\n\t<div id=\"action-panel\" role=\"region\" aria-label=\"Action Panel\" class=\"action-panel has-default-focus\" tabindex=\"-1\"></div>\n</body>\n</html>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# create recursiveurlloader instace with specific url\n",
    "loader = RecursiveUrlLoader(\n",
    "    \"https://learn.microsoft.com/en-us/azure/databricks/generative-ai/vector-search/\",\n",
    "    timeout=10,\n",
    "    max_depth=2,\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)  # see the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63003f26-f1da-4e0a-b44a-1f5d48989272",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing the html tags: Mosaic AI Vector Search - Azure Databricks | Microsoft Learn\n\nSkip to main content\n\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\n\nDownload Microsoft Edge\nMore info about Internet Explorer and Microsoft Edge\n\nTable of contents\n\nExit focus mode\n\nRead in English\n\nSave\n\nTable of contents\n\nRead in English\n\nSave\n\nAdd to plan\n\nEdit\n\nShare via\n\nFacebook\n\nx.com\n\nLinkedIn\n\nEmail\n\nPrint\n\nTable of contents\n\nMosaic AI Vector Search\n\nArticle02/03/2025\n\n2 contributors\n\nFeedback\n\nIn this article\nThis article gives an overview of Databricks’ vector database solution, Mosaic AI Vector Search, including what it is and how it works.\nWhat is Mosaic AI Vector Search?\nMosaic AI Vector Search is a vector database that is built into the Databricks Data Intelligence Platform and integrated with its governance and productivity tools. A vector database is a database that is optimized to store and retrieve embeddings. Embeddings are mathematical representations of the semantic content of data, typically text or image data. Embeddings are generated by a large language model and are a key component of many GenAI applications that depend on finding documents or images that are similar to each other. Examples are RAG systems, recommender systems, and image and video recognition.\nWith Mosaic AI Vector Search, you create a vector search index from a Delta table. The index includes embedded data with metadata. You can then query the index using a REST API to identify the most similar vectors and return the associated documents. You can structure the index to automatically sync when the underlying Delta table is updated.\nMosaic AI Vector Search supports the following:\n\nHybrid keyword-similarity search.\nFiltering.\nAccess control lists (ACLs) to manage vector search endpoints.\nSync only selected columns.\nSave and sync generated embeddings.\n\nHow does Mosaic AI Vector Search work?\nMosaic AI Vector Search uses the Hierarchical Navigable Small World (HNSW) algorithm for its approximate nearest neighbor searches and the L2 distance distance metric to measure embedding vector similarity. If you want to use cosine similarity you need to normalize your datapoint embeddings before feeding them into vector search. When the data points are normalized, the ranking produced by L2 distance is the same as the ranking produces by cosine similarity.\nMosaic AI Vector Search also supports hybrid keyword-similarity search, which combines vector-based embedding search with traditional keyword-based search techniques. This approach matches exact words in the query while also using a vector-based similarity search to capture the semantic relationships and context of the query.\nBy integrating these two techniques, hybrid keyword-similarity search retrieves documents that contain not only the exact keywords but also those that are conceptually similar, providing more comprehensive and relevant search results. This method is particularly useful in RAG applications where source data has unique keywords such as SKUs or identifiers that are not well suited to pure similarity search.\nFor details about the API, see the Python SDK reference and Query a vector search endpoint.\nSimilarity search calculation\nThe similarity search calculation uses the following formula:\n\nwhere dist is the Euclidean distance between the query q and the index entry x:\n\nKeyword search algorithm\nRelevance scores are calculated using Okapi BM25. All text or string columns are searched, including the source text embedding and metadata columns in text or string format. The tokenization function splits at word boundaries, removes punctuation, and converts all text to lowercase.\nHow similarity search and keyword search are combined\nThe similarity search and keyword search results are combined using the Reciprocal Rank Fusion (RRF) function.\nRRF rescores each document from each method using the score:\n\nIn the above equation, rank starts at 0, sums the scores for each document and returns the highest scoring documents.\nrrf_param controls the relative importance of higher-ranked and lower-ranked documents. Based on the literature, rrf_param is set to 60.\nScores are normalized so that the highest score is 1 and the lowest score is 0 using the following equation:\n\nOptions for providing vector embeddings\nTo create a vector database in Databricks, you must first decide how to provide vector embeddings. Databricks supports three options:\n\nOption 1: Delta Sync Index with embeddings computed by Databricks You provide a source Delta table that contains data in text format. Databricks calculates the embeddings, using a model that you specify, and optionally saves the embeddings to a table in Unity Catalog. As the Delta table is updated, the index stays synced with the Delta table.\nThe following diagram illustrates the process:\n\nCalculate query embeddings. Query can include metadata filters.\nPerform similarity search to identify most relevant documents.\nReturn the most relevant documents and append them to the query.\n\nOption 2: Delta Sync Index with self-managed embeddings You provide a source Delta table that contains pre-calculated embeddings. As the Delta table is updated, the index stays synced with the Delta table.\nThe following diagram illustrates the process:\n\nQuery consists of embeddings and can include metadata filters.\nPerform similarity search to identify most relevant documents. Return the most relevant documents and append them to the query.\n\nOption 3: Direct Vector Access Index You must manually update the index using the REST API when the embeddings table changes.\nThe following diagram illustrates the process:\n\nHow to set up Mosaic AI Vector Search\nTo use Mosaic AI Vector Search, you must create the following:\n\nA vector search endpoint. This endpoint serves the vector search index. You can query and update the endpoint using the REST API or the SDK. See Create a vector search endpoint for instructions.\nEndpoints scale up automatically to support the size of the index or the number of concurrent requests. Endpoints do not scale down automatically.\n\nA vector search index. The vector search index is created from a Delta table and is optimized to provide real-time approximate nearest neighbor searches. The goal of the search is to identify documents that are similar to the query. Vector search indexes appear in and are governed by Unity Catalog. See Create a vector search index for instructions.\n\nIn addition, if you choose to have Databricks compute the embeddings, you can use a pre-configured Foundation Model APIs endpoint or create a model serving endpoint to serve the embedding model of your choice. See Pay-per-token Foundation Model APIs or Create foundation model serving endpoints for instructions.\nTo query the model serving endpoint, you use either the REST API or the Python SDK. Your query can define filters based on any column in the Delta table. For details, see Use filters on queries, the API reference, or the Python SDK reference.\nRequirements\n\nUnity Catalog enabled workspace.\nServerless compute enabled. For instructions, see Connect to serverless compute.\nSource table must have Change Data Feed enabled. For instructions, see Use Delta Lake change data feed on Azure Databricks.\nTo create a vector search index, you must have CREATE TABLE privileges on the catalog schema where the index will be created.\n\nPermission to create and manage vector search endpoints is configured using access control lists. See Vector search endpoint ACLs.\nData protection and authentication\nDatabricks implements the following security controls to protect your data:\n\nEvery customer request to Mosaic AI Vector Search is logically isolated, authenticated, and authorized.\nMosaic AI Vector Search encrypts all data at rest (AES-256) and in transit (TLS 1.2+).\n\nMosaic AI Vector Search supports two modes of authentication:\n\nService principal token. An admin can generate a service principal token and pass it to the SDK or API. See use service principals. For production use cases, Databricks recommends using a service principal token.\n# Pass in a service principal\nvsc = VectorSearchClient(workspace_url=\"...\",\n        service_principal_client_id=\"...\",\n        service_principal_client_secret=\"...\"\n        )\n\nPersonal access token. You can use a personal access token to authenticate with Mosaic AI Vector Search. See personal access authentication token. If you use the SDK in a notebook environment, the SDK automatically generates a PAT token for authentication.\n# Pass in the PAT token\nclient = VectorSearchClient(workspace_url=\"...\", personal_access_token=\"...\")\n\nCustomer Managed Keys (CMK) are supported on endpoints created on or after May 8, 2024.\nMonitor usage and costs\nThe billable usage system table lets you monitor usage and costs associated with vector search indexes and endpoints. Here is an example query:\nWITH all_vector_search_usage (\n  SELECT *,\n         CASE WHEN usage_metadata.endpoint_name IS NULL THEN 'ingest'\n              WHEN usage_type = \"STORAGE_SPACE\" THEN 'storage'\n              ELSE 'serving'\n        END as workload_type\n    FROM system.billing.usage\n   WHERE billing_origin_product = 'VECTOR_SEARCH'\n),\ndaily_dbus AS (\n  SELECT workspace_id,\n       cloud,\n       usage_date,\n       workload_type,\n       usage_metadata.endpoint_name as vector_search_endpoint,\n       CASE WHEN workload_type = 'serving' THEN SUM(usage_quantity)\n            WHEN workload_type = 'ingest' THEN SUM(usage_quantity)\n            ELSE null\n            END as dbus,\n       CASE WHEN workload_type = 'storage' THEN SUM(usage_quantity)\n            ELSE null\n            END as dsus\n FROM all_vector_search_usage\n GROUP BY all\nORDER BY 1,2,3,4,5 DESC\n)\nSELECT * FROM daily_dbus\n\nFor details about the contents of the billing usage table, see Billable usage system table reference. Additional queries are in the following example notebook.\nVector search system tables queries notebook\nGet notebook\nResource and data size limits\nThe following table summarizes resource and data size limits for vector search endpoints and indexes:\n\nResource\nGranularity\nLimit\n\nVector search endpoints\nPer workspace\n100\n\nEmbeddings\nPer endpoint\n320,000,000\n\nEmbedding dimension\nPer index\n4096\n\nIndexes\nPer endpoint\n50\n\nColumns\nPer index\n50\n\nColumns\n\nSupported types: Bytes, short, integer, long, float, double, boolean, string, timestamp, date\n\nMetadata fields\nPer index\n50\n\nIndex name\nPer index\n128 characters\n\nThe following limits apply to the creation and update of vector search indexes:\n\nResource\nGranularity\nLimit\n\nRow size for Delta Sync Index\nPer index\n100KB\n\nEmbedding source column size for Delta Sync index\nPer Index\n32764 bytes\n\nBulk upsert request size limit for Direct Vector index\nPer Index\n10MB\n\nBulk delete request size limit for Direct Vector index\nPer Index\n10MB\n\nThe following limits apply to the query API.\n\nResource\nGranularity\nLimit\n\nQuery text length\nPer query\n32764 bytes\n\nMaximum number of results returned\nPer query\n10,000\n\nLimitation\nRow and column level permissions are not supported. However, you can implement your own application level ACLs using the filter API.\nAdditional resources\n\nDeploy Your LLM Chatbot With Retrieval Augmented Generation (RAG), Foundation Models and Vector Search.\nHow to create and query a vector search index.\nExample notebooks\n\nFeedback\n\nWas this page helpful?\n\nYes\n\nNo\n\nProvide product feedback\n\nAdditional resources\n\nYour Privacy Choices\n\nTheme\n\nLight\n\nDark\n\nHigh contrast\n\nPrevious Versions\nBlog\nContributePrivacyTerms of UseTrademarks© Microsoft 2025\n\nAdditional resources\n\nIn this article\n\nYour Privacy Choices\n\nTheme\n\nLight\n\nDark\n\nHigh contrast\n\nPrevious Versions\nBlog\nContributePrivacyTerms of UseTrademarks© Microsoft 2025\n404 - Content Not Found | Microsoft Learn\n\nSkip to main content\n\nThis browser is no longer supported.\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support.\n\nDownload Microsoft Edge\nMore info about Internet Explorer and Microsoft Edge\n\nTable of contents\n\n404 - Page not found\nWe couldn't find this page. You can try signing in, or choosing from the relevant search results below:\nWe couldn't find this page. You can try  changing directories, or choosing from the relevant search results below:\n\nYour Privacy Choices\n\nTheme\n\nLight\n\nDark\n\nHigh contrast\n\nPrevious Versions\nBlog\nContributePrivacyTerms of UseTrademarks© Microsoft 2025\n\nYour Privacy Choices\n\nTheme\n\nLight\n\nDark\n\nHigh contrast\n\nPrevious Versions\nBlog\nContributePrivacyTerms of UseTrademarks© Microsoft 2025\n"
     ]
    }
   ],
   "source": [
    "# function for removing html tag\n",
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", soup.text).strip()\n",
    "\n",
    "content = bs4_extractor(docs[0].page_content)\n",
    "print(f\"After removing the html tags: {content}\")\n",
    "\n",
    "# define the extractor in the RecursiveUrlLoader Class\n",
    "loader = RecursiveUrlLoader(\"https://learn.microsoft.com/en-us/azure/databricks/generative-ai/vector-search/\", extractor=bs4_extractor)\n",
    "docs = loader.load()\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29301ca1-00b2-439c-a599-5d7cff56770a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Research Paper Data Loader\n",
    "ArXiv is an open-source archive for over 2 million scholarly articles in the fields of physics, mathematics, computer science, finance, statistics, electrical engineering, system science, economics, biology, and so on. So ArXiv is a great source of data in various domain. We can easily properly use data for our applications.\n",
    "\n",
    "To access ArXiv data, we need to install the language community, Arxiv, and PyMuPDF integration packages. PyMuPDF helps to transform PDF files downloaded from the ArXiv website into text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792b9a2b-cef1-4f35-a108-f1eb162317c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-engineering 0.2.1 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.2.0 requires numpy<1.24,>=1.16.0, but you have numpy 2.2.3 which is incompatible.\nydata-profiling 4.2.0 requires pydantic<2,>=1.8.1, but you have pydantic 2.10.6 which is incompatible.\ndatabricks-sdk 0.1.6 requires requests<2.29.0,>=2.28.1, but you have requests 2.32.3 which is incompatible.\ndatabricks-feature-engineering 0.2.1 requires numpy<2,>=1.19.2, but you have numpy 2.2.3 which is incompatible.\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-community arxiv pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259420c8-a4cc-42da-9a6e-0f8d9591535d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='SOD-YOLOv8 - Enhancing YOLOv8 for Small Object Detection in\nTraffic Scenes\nBoshra Khalili1 and Andrew W.Smyth2\nAbstract— Object detection as part of computer vision can\nbe crucial for traffic management, emergency response, au-\ntonomous vehicles, and smart cities. Despite significant ad-\nvances in object detection, detecting small objects in images\ncaptured by distant cameras remains challenging due to their\nsize, distance from the camera, varied shapes, and cluttered\nbackgrounds. To address these challenges, we propose Small\nObject Detection YOLOv8 (SOD-YOLOv8), a novel model\nspecifically designed for scenarios involving numerous small\nobjects. Inspired by Efficient Generalized Feature Pyramid Net-\nworks (GFPN), we enhance multi-path fusion within YOLOv8\nto integrate features across different levels, preserving details\nfrom shallower layers and improving small object detection\naccuracy. Additionally, a fourth detection layer is introduced\nto utilize high-resolution spatial information effectively. The\nEfficient Multi-Scale Attention Module (EMA) in the C2f-EMA\nmodule enhances feature extraction by redistributing weights\nand prioritizing relevant features. We introduce Powerful-IoU\n(PIoU) as a replacement for CIoU, focusing on moderate-\nquality anchor boxes and adding a penalty based on differences\nbetween predicted and ground truth bounding box corners.\nThis approach simplifies calculations, speeds up convergence,\nand enhances detection accuracy. SOD-YOLOv8 significantly\nimproves small object detection, surpassing widely used models\nin various metrics, without substantially increasing computa-\ntional cost or latency compared to YOLOv8s. Specifically, it\nincreases recall from 40.1% to 43.9%, precision from 51.2%\nto 53.9%, mAP0.5 from 40.6% to 45.1%, and mAP0.5:0.95\nfrom 24% to 26.6%. In dynamic real-world traffic scenes,\nSOD-YOLOv8 demonstrated notable improvements in diverse\nconditions, proving its reliability and effectiveness in detecting\nsmall objects even in challenging environments.\nI. INTRODUCTION\nObject detection in computer vision plays a crucial role\nacross various fields, including Autonomous Vehicles [1],\n[2], [3], traffic scene monitoring [4], [5], enhancing intelli-\ngent driving systems [6], and facilitating search and rescue\nmissions [7]. Accurate detection of small objects such as\npedestrians, vehicles, motorcycles, bicycles, traffic signs, and\nlights is crucial for safe navigation and decision-making\nin autonomous vehicles and intelligent driving systems [1],\n[3]. Furthermore, detecting small objects enhances traffic\nflow management, pedestrian safety, and overall traffic scene\nanalysis. This capability is essential for improving urban\nplanning and transportation systems [4], [5].\n1Boshra Khalili is Graduate Research Assistant in the Department of\nCivil Engineering and Engineering Mechanics, Columbia University, New\nYork, NY 10027, USA bk2898@columbia.edu\n2Andrew W. Smyth (corresponding author) is the Robert A. W. and Chris-\ntine S. Carleton Professor of Civil Engineering and Engineering Mechanics\nand the Director of the Center for the Smart Streetscapes (CS3), Columbia\nUniversity, New York, NY 10027, USA aws16@columbia.edu\nAs the cost of UAV production decreases and flight\ncontrol techniques advance, these small, flexible devices are\nincreasingly used for intelligent traffic monitoring [8]. UAVs\ntypically operate at higher altitudes to capture broader views,\nwhich reduces the apparent size of ground objects due to\ngreater distances. This distance complicates object detection\nwithin captured images [8]. Despite significant progress in\nobject detection, detecting small objects such as pedestrians,\nmotorcycles, bicycles, and vehicles in urban traffic remains\nchallenging due to their size, varied shapes, and cluttered\nbackgrounds. This challenge is further amplified when work-\ning with limited hardware resources in computer vision and\nobject detection.\nSmall objects, which occupy a small portion of an image\nand have lower resolution and less distinct visual charac-\nteristics compared to larger objects, are more challenging to\ndetect accurately. Moreover, shallow layers in networks such\nas YOLOv8 may filter out essential spatial details required\nfor detecting these small objects, resulting in data loss.\nAdditionally, smaller objects can be overshadowed by larger\nones during feature extraction, potentially causing the loss of\nrelevant details crucial for accurate detection. Overcoming\nthese challenges is crucial for improving overall detection\naccuracy and reliability in real-world scenarios.\nTo address small object detection in UAV aerial pho-\ntography and traffic scenes, we introduce a novel model\nbased on YOLOv8. Our model integrates multi-scale spatial\nand contextual information using an enhanced GFPN [9].We\nintegrate EMA Attention [10] into the C2f module to ensure\nthat small object features are given sufficient emphasis. We\nalso include a fourth detection layer to utilize high-resolution\nspatial details effectively. Furthermore, due to the significant\nimpact of bounding box regression in object detection, we\nused the PIoU method, which enhances performance and\nreduces convergence time by incorporating an improved\npenalty term and attention mechanism. Our key contributions\nare as follows:\n• Inspired by the Efficient RepGFPN in DAMO-YOLO\nmodels [11], we enhance multi-path fusion within\nthe YOLOv8 architecture. This enhancement facilitates\nbetter fusion of features across different levels and\nsimplifies the GFPN structure through reparameteriza-\ntion. By preserving crucial information from shallower\nlayers, our approach significantly improves detection\naccuracy, especially for small objects. Additionally, we\nadd a fourth detection layer to effectively leverage high-\nresolution and detailed spatial information.\n• We integrate a C2f-EMA structure into the network,\narXiv:2408.04786v1  [cs.CV]  8 Aug 2024\nleveraging the Efficient Multi-Scale Attention Module\nto replace C2f in the neck layers. This enhancement\nimproves feature extraction by redistributing feature\nweights, prioritizing relevant features and spatial details\nacross image channels. Consequently, it enhances the\nnetwork’s ability to detect targets of different sizes.\n• We use PIoU to replace CIoU in the original network.\nPIoU enhances existing IoU-based loss functions by\nbetter balancing difficult and easy samples, with a\nspecific focus on anchor boxes of moderate quality.\nIt incorporates a penalty term based on differences\nbetween predicted and ground truth bounding box cor-\nners, improving the efficiency and accuracy of bounding\nbox regression. PIoU also simplifies calculations by\nrequiring only one hyperparameter, leading to faster\nconvergence and enhanced object detection accuracy.\n• We conduct visual analyses on various challenging sce-\nnarios to demonstrate the effectiveness of our proposed\napproach in enhancing small object detection. Addi-\ntionally, we perform experiments in real-world traffic\nscenes using images captured from cameras mounted on\nbuildings. These images contain numerous small objects\nand help validate our enhanced model for small object\ndetection.\nThe structure of the remaining sections of this paper is as\nfollows: Section 2 discusses related work. Section 3 provides\nan overview of the YOLOv8 network architecture. Section\n4 details the proposed enhanced YOLOv8. Section 5 covers\nour experimental setup and result analysis. Finally, Section\n6 concludes the paper.\nII. RELATED WORK\nSmall object detection has been a significant challenge in\nthe field of computer vision, particularly in traffic scenarios.\nThis section reviews mainstream object detection algorithms,\nrecent advancements in small object detection, and specific\nenhancements made to the YOLO framework.\nMainstream object detection algorithms predominantly use\ndeep learning techniques, categorized into two types: two-\nstage and one-stage methods. Two-stage methods process\ncandidate frames with a classifier and perform deep learning\non corresponding frames [13]. Typical two-stage detection\nalgorithms include R-CNN [13], Fast R-CNN [14], and\nFaster R-CNN [15]. The R-CNN family is a classic two-\nstage algorithm known for high detection accuracy but faces\nchallenges such as slow speed, training complexity, and\noptimization [16]. One-stage detectors, like the YOLO series\n[17], [18] and SSD [19], use a single neural network to\npredict box coordinates and categories in one pass. Conse-\nquently, single-stage networks excel in applications where\nspeed is crucial. However, they sacrifice some accuracy.\nDespite advancements in speed, these methods struggle with\naccuracy due to the multi-scale nature of objects and the\nprevalence of small objects in UAV and traffic scenes.\nRecent research has focused on improving small object\ndetection in UAV aerial photography and traffic scenarios,\nwhich is challenging due to their lower resolution and less\ndistinct visual characteristics compared to larger objects.\nStudies have explored diverse backbone architectures to\nenhance feature representation, reduce false positives, and\nextract relevant features from complex backgrounds in UAV\nimagery.\nLiu et al. [20] introduced a model for small target detection\nin UAV images, addressing leakage and false positives by\nintegrating ResNet units and optimizing convolutional op-\nerations to expand the network’s receptive field. Liu et al.\n[21] proposed CBSSD, a specialized detector for small object\ndetection in UAV traffic images. By integrating ResNet50’s\nlower-level features with VGG16, CBSSD improves feature\nrepresentation, enhances object recognition accuracy, and\nreduces false positives under challenging lighting condi-\ntions. Additionally, Liu et al. [22] utilized Multi-branch\nParallel Feature Pyramid Networks (MPFPN) and SSAM\nfor detecting small objects in UAV images. Their approach\nenhances feature extraction through MPFPN for deep layer\ndetail recovery, while SSAM reduces background noise,\nsignificantly boosting accuracy. Experimental results on the\nVisDrone-DET dataset [23] showcase their method’s com-\npetitive performance.\nAdaptations and optimizations within the YOLO frame-\nwork have also been explored to address challenges in small\nobject detection. Lai et al. [24] introduced STC-YOLO, a\nspecialized variant of YOLOv5 designed for challenging\ntraffic sign detection. Their improvements include refined\ndown-sampling, a dedicated small object detection layer,\nand a CNN-based feature extraction module with multi-\nhead attention. STC-YOLO demonstrated a significant 9.3%\nimprovement in mean Average Precision (mAP) compared\nto YOLOv5 on benchmark datasets.\nFurther enhancements have been made to YOLOv8, focus-\ning on improving backbone architectures, integrating atten-\ntion mechanisms to focus on relevant features and suppress\nirrelevant ones, and modifying loss functions. Shen et al. [25]\nintroduced DS-YOLOv8 to enhance small object detection\nwithin images by integrating Deformable Convolution C2f\n(DCN C2f) and Self-Calibrating Shuffle Attention (SC SA)\nfor adaptive feature adjustment, alongside Wise-IoU [26] and\nposition regression loss to boost performance. Experimental\nresults across diverse datasets show significant enhancements\nin mAP0.5. Wang et al. [8] improved YOLOv8 for UAV\naerial photography with a BiFormer attention mechanism for\nfocusing on important information and FFNB for effective\nmultiscale feature fusion. This resulted in a significant 7.7%\nincrease in mean detection accuracy over baseline mod-\nels, surpassing widely-used alternatives in detecting small\nobjects. This marks substantial progress in UAV object\ndetection, though it necessitates further optimization due to\nincreased computational complexity from additional detec-\ntion layers.\nWang et al. [27] improved YOLOv8 for detecting targets\nin remote sensing images, focusing on complex backgrounds\nand diverse small targets. They introduced a small target\ndetection layer and incorporated a C2f-E structure using\nthe EMA attention module. Experimental results on the\nDOTAv1.0 dataset [28] demonstrate a notable 1.3% increase\nin mAP0.5 to 82.7%, highlighting significant advancements\nin target detection accuracy. However, their approach intro-\nduces increased computational complexity. Xu et al. [29] in-\ntroduced YOLOv8-MPEB, specialized for small target detec-\ntion in UAV images, addressing scale variations and complex\nscenes. Enhancements include replacing CSPDarknet53 [30]\nwith MobileNetV3 for efficiency, integrating Efficient Multi-\nScale Attention in C2f for better feature extraction, and in-\ncorporating Bidirectional Feature Pyramid Network (BiFPN)\n[31] in the Neck segment for enhanced adaptability. Exper-\nimental results on a custom dataset demonstrate YOLOv8-\nMPEB achieving a 91.9% mAP, a 2.2% improvement over\nstandard YOLOv8, while reducing parameters by 34% and\nmodel size by 32%. However, accurately detecting dense\nsmall targets remains a challenge.\nDespite advancements in the reviewed studies, small ob-\nject detection methods still face challenges in UAV aerial\nphotography and traffic scenarios. These methods primarily\nfocus on feature fusion but often neglect inner block con-\nnections. In contrast, our approach integrates an optimized\nGFPN, inspired by Efficient-RepGFPN, into YOLOv8. This\nenhancement incorporates skip connections and queen fusion\nstructures to improve efficacy without significantly increas-\ning computational complexity or latency. Additionally, the\nintroduced C2f-EMA module enhances feature extraction\nby redistributing feature weights using the EMA attention\nmechanism. Unlike other attention mechanisms, it overcomes\nlimitations such as neglecting interactions among spatial de-\ntails and the limited receptive field of 1x1 kernel convolution,\nwhich limits local cross-channel interaction and contextual\ninformation modeling.\nFurthermore, our method avoids the enlargement issues\ncommon in other bounding box regression methods. The\nused PIoU loss function effectively guides anchor boxes\nduring training, resulting in faster convergence and demon-\nstrating its effectiveness. While existing methods perform\nwell in controlled datasets, they often struggle to generalize\nacross diverse environments and dynamic lighting conditions\nin real-world settings. In this paper, we experiment with\nreal-world traffic scenes captured by building-mounted cam-\neras, assessing diverse environments, lighting conditions, and\ndynamic scenarios such as nighttime and crowded scenes.\nThis challenges the generalization capabilities of small object\ndetection method beyond controlled datasets.\nIII. INTRODUCTION OF YOLOV8 DETECTION NETWORK\nAs shown in Figure 1, the YOLOv8 architecture consists\nof three main elements: the backbone, neck, and detection\nlayers. Each of these components will be introduced in the\nsubsequent sections.\nA. Backbone Layer\nThe architecture of YOLOv8 is based on the CSPDark-\nnet53 [30] backbone, employing five downsampling stages to\nextract distinct scale features. It improves information flow\nand stays lightweight by using the C2f module instead of\nFig. 1.\nThe network structure of YOLOv8.\nthe Cross Stage Partial (CSP) module [32]. The C2f module\nincludes dense and residual structures for better gradient flow\nand feature representation. The backbone also includes the\nSpatial Pyramid Pooling Fast (SPPF) module, which captures\nfeatures at multiple scales to boost detection performance.\nThe SPPF layer reduces computational complexity and la-\ntency while optimizing feature extraction [34].\nB. Neck Layer\nFor multi-scale feature fusion, YOLOv8’s neck uses Fea-\nture Pyramid Network (FPN) [35] and Path Aggregation\nNetworks (PANet) [36]. FPN enhances hierarchical feature\nfusion, improving object detection across various scales\nthrough a top-down pathway, while PANet enhances fea-\nture representation and information reuse with a bottom-up\npathway, though it increases computational cost. Combining\nFPN-PANet structures and C2f modules integrates feature\nmaps of various scales, merging both shallow and deep\ninformation.\nC.\nDetection Head Layer\nYOLOv8, a state-of-the-art object detection model, en-\nhances accuracy and robustness by using the Task-Aligned\nAssigner [37] instead of traditional anchors. This assigner\ndynamically categorizes samples as positives or negatives,\nrefining the model’s ability to detect objects accurately. The\ndetection head features a decoupled structure with sepa-\nrate branches for object classification and bounding box\nregression. For classification, it employs binary cross-entropy\nloss (BCE Loss). For regression, it uses a combination of\ndistribution focal loss (DFL) [38] and Complete Intersection\nover Union (CIoU) [39] loss. These efficient loss functions\nFig. 2.\nProposed improved YOLOv8 for small object detection\nare crucial for precise object localization, further boosting\nthe model’s performance.\nBounding box loss functions aim to accurately localize\nobjects by penalizing differences between predicted and\nground truth bounding boxes. IoU-based loss functions [40]\nare crucial for bounding box regression in the detection layer.\nIoU Loss measures the overlap between predicted and ground\ntruth boxes by comparing the ratio of their intersection\narea to their union area. However, its gradient diminishes\nwhen there is no overlap, making it less effective in such\ncases. Various IoU-based loss functions have been developed\nwith different methodologies and constraints. CIoU, used in\nYOLOv8, minimizes the normalized distance between the\ncenter points of predicted and ground truth boxes and in-\ncludes an aspect ratio penalty term. This approach improves\nconvergence speed and overall performance.\nIV. METHOD\nIn this section, we introduce three pivotal enhancements\nto improve small object detection in our study. First, we\nenhance feature fusion within the YOLOv8 architecture’s\nneck to better retain crucial spatial details typically filtered\nout by shallower layers. This modification aims to mitigate\ninformation loss, especially for smaller objects overshadowed\nby larger ones during feature extraction. Second, we pro-\npose the C2f-EMA module, integrating an EMA attention\nmechanism to prioritize relevant features and spatial details\nacross different channels. This method enhances feature\nextraction efficiency by redistributing feature weights ef-\nfectively. Finally, we use PIoU as an improved bounding\nbox regression metric, replacing CIoU. PIoU incorporates a\npenalty term that minimizes the Euclidean distance between\ncorresponding corners of predicted and ground truth boxes,\noffering a more intuitive measure of similarity and stability in\nbox regression tasks. These methods contribute to enhancing\nthe accuracy and robustness of our small object detection\nframework. The enhanced structure depicted in Figure 2 is\nutilized in this paper.\nA. Improved GFPN for Multilevel Feature Integration\nIn YOLOv8, crucial spatial details are primarily encoded\nin the network’s shallower layers. However, these layers\noften filter out less prominent details, leading to significant\ndata loss for small object detection. Additionally, smaller\nobjects may be overshadowed by larger ones during feature\nextraction, resulting in a gradual loss of information and the\npotential disappearance of relevant details. To address these\nchallenges, this study introduces an enhanced feature fusion\nmethod in the neck of the YOLOv8 architecture. This method\nfocuses on preserving and effectively utilizing important\ninformation from the shallower layers, thereby enhancing\noverall detection accuracy, especially for small objects.\nThe FPN merges features of different resolutions ex-\ntracted from a backbone network. It begins with the highest-\nresolution feature map and progressively combines features\nfrom higher to lower resolutions using a top-down approach.\nPAFPN improves FPN by adding a bottom-up approach that\nenhances bidirectional information flow. It merges features\nfrom lower to higher network layers, prioritizing spatial\ndetail preservation, even with increased computational de-\nmands.\nThe BiFPN [31] enhances object detection by integrating\nfeatures across different resolutions bidirectionally, using\nboth bottom-up and top-down pathways. This method op-\ntimizes multi-scale feature utilization, simplifies the network\nby reducing computational complexity, and includes skip\nconnections at each level. These connections allow for\nadaptable use of input features, enhancing feature fusion\nacross scales and details [31]. However, deep stacking of\nBiFPN blocks may cause gradient vanishing during training,\npotentially affecting overall network performance [33].\nPrior methods focused mainly on combining features with-\nout considering inner block connections. In contrast, GFPN\nintroduces skip connections and queen fusion structures. It\nemploys skip-layer and cross-scale connections to enhance\nfeature combination. GFPN implements skip connections in\ntwo forms: log2(n)-link and dense-link.\nThe log2(n)-link method optimizes information transmis-\nsion by allowing the lth layer at level k to receive feature\nmaps from up to log2(l) + 1 preceding layers. These skip\nconnections help mitigate gradient vanishing during back-\npropagation by extending the shortest gradient distance from\none layer to approximately 1 + log2(n) layers [9]. This\nextension facilitates more effective gradient propagation over\nlonger distances, potentially enhancing the scalability of\ndeeper networks.\nIn contrast, the dense-link method ensures that each scale\nfeature P l\nk at level k receives feature maps from all preceding\nFig. 3.\nskip-layer links: (a) dense-link: concatenates features from all\npreceding layers; (b) log2 n-link: concatenates features from up to log2(l)+\n1 layers at each level.\nFig. 4.\nDifferent Feature Pyramid Network designs: (a) FPN uses a top-\ndown strategy; (b) PANet enhances FPN with a bottom-up pathway; (c)\nBiFPN integrates cross-scale pathways bidirectionally; (d) GFPN includes\na queen-fusion style pathway and skip-layer connections.\nlayers up to the lth layer. This promotes robust information\nflow and integration of features across multiple scales. In\nGFPN, this structure facilitates seamless connectivity be-\ntween layers, enhancing feature reuse and improving the\nnetwork’s efficiency in tasks like object detection. During\nback-propagation, the dense connectivity in GFPN supports\nefficient transmission of feature information across the net-\nwork hierarchy. Dense-link and log2(n)-link configurations\nare illustrated in Figure 3.\nAnother significant improvement in GFPN is the Queen-\nFusion module, which facilitates cross-scale connections to\nenhance adaptability to multi-scale variations. This module\nutilizes a 3x3 convolution to merge features across different\nscales, gathering input features from diagonally adjacent\nnodes above and below to minimize information loss during\nfeature fusion. Implementing this approach enhances the\nnetwork’s capability to handle multi-scale variations, po-\ntentially improving overall performance robustness. Figure\n4 illustrates various methods for integrating features across\ndifferent layers, including FPN, PANet, BiFPN, and GFPN.\nIn YOLOv8, integrating PAFPN with C2f modules effec-\ntively combines feature maps across scales, thereby enhanc-\ning object detection capabilities. This study aims to enhance\nYOLOv8’s small object detection using advanced feature\nfusion techniques. However, replacing PAFPN with GFPN\nin YOLOv8 improves precision while introducing higher\nlatency compared to the PAFPN-based model.\nThis paper introduces an enhanced and efficient GFPN,\ndepicted in Figure 5, inspired by Efficient-RepGFPN [11].\nBy integrating it into YOLOv8, the model achieves superior\nperformance without significantly increasing computational\ncomplexity or latency. Efficient-RepGFPN simplifies com-\nplexity by parameterizing and eliminating additional upsam-\npling operations in queen-fusion. Furthermore, it upgrades\nthe feature fusion module to CSPNet, enhancing the merging\nof features across different scales [11].\nIn the feature fusion block of the GFPN architecture, we\nreplace the conventional 3x3 convolution-based feature fu-\nsion with C2f-EMA, incorporating an attention mechanism.\nFig. 5.\nEnhanced and efficient GPFN structure\nThis module merges high-level semantic features with low-\nlevel spatial details, thereby enhancing the representation\nand detection accuracy of small objects. These modifications\nmaintain GFPN’s ability to improve feature interaction and\nefficiency by effectively managing both types of informa-\ntion in the neck section. Inspired by Efficient-RepGFPN,\nwe also reparametrize and eliminate additional upsampling\noperations in queen-fusion. Ultimately, these enhancements\nimprove the efficiency and effectiveness of YOLOv8 for\nobject detection tasks without significantly increasing com-\nputational complexity or latency.\nWe enhance the n\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nording to\nTable VI, our proposed efficient model enhances object de-\ntection performance significantly without adding significant\ncomputational cost or latency compared to YOLOv8s. It\nimproves recall from 43% to 44%, precision from 45% to\n46%, mAP0.5 from 40% to 45.1%, and mAP0.5:0.95 from\n20% to 26.6%.\nFigure 10 depicts the evaluation metrics for SOD-\nYOLOv8 and YOLOv8s across 200 training epochs. Our\nmodel outperforms YOLOv8s in precision and mAP0.5\nstarting around epoch 15 and stabilizes after 50 epochs.\nTABLE III\nDIFFERENT YOLO MODELS’ RESULTS, PRESENTED AS PERCENTAGES.(THE BEST-PERFORMING OUTCOMES ARE HIGHLIGHTED IN BOLD)\nModels\nModel’s Size\nBackbone\nPrecision\nRecall\nmAP0.5\nmAP0.5:0.95\nTime/ms\nParameter/106\nYOLOv3 [30]\n-\nDarknet-53\n53.6\n43.2\n42\n23.1\n209\n18.3\nYOLOv5s\n-\nCSP-Darknet-53\n46.7\n34.8\n34.7\n19.2\n13.9\n12.0\nYOLOv7 [50]\n-\nELAN\n51.5\n42.3\n40.1\n21.8\n71.5\n1.7\nYOLOv8 [51]\nYOLOv8n\nCSP-Darknet-53\n44.0\n33.2\n33.5\n19.5\n6.7\n4.2\nYOLOv8s\n51.1\n39.1\n39.6\n23.8\n7.8\n11.1\nYOLOv8m\n55.8\n42.6\n44.5\n26.6\n16.8\n25.9\nSOD-YOLOv8s\n-\nCSP-Darknet-53\n53.9\n43.9\n45.1\n26.6\n17.7\n11.5\nTABLE IV\nRESULTS FROM DIFFERENT WIDELY USED MODELS, PRESENTED AS\nPERCENTAGES.(THE BEST-PERFORMING OUTCOMES ARE HIGHLIGHTED\nIN BOLD)\nModels\nBackbone\nAP0.5\nAP0.5:0.95\nFaster R-CNN [48]\nResNet\n37.8\n21.5\nCascade R-CNN [52]\nResNet\n39.4\n24.2\nCenterNet [53]\nResNet50 [54]\n39.1\n22.8\nSSD [19]\nMobileNetV2 [55]\n33.7\n19\nSOD-YOLOv8s\nCSP-Darknet-53\n45.1\n26.6\nFig. 10.\n(a) Training progress plot comparing YOLOv8s-GFPN-EMA,\nYOLOv8s-GFPN, and YOLOv8s based on mAP0.5 (b) and precision\nThis illustrates that SOD-YOLOv8 significantly enhances\ndetection performance, particularly for small and challenging\nobjects, without introducing significant complexity.\n5) Visual assessment: We conducted visual experiments\nto evaluate our model’s detection performance. Our analysis\nincluded various metrics such as confusion matrices and\ninference test results. To validate the effectiveness of our\nmethod in challenging real-world scenarios, we performed\ninference tests using images captured by a camera mounted\non the 12th floor of a building. This scenario involves\ncapturing\nimages\nfrom\na\nhigh\nvantage\npoint,\nposing\nchallenges in detecting numerous small objects in a crowded\ntraffic scene at an intersection.\nVisDrone2019 dataset results\nTo visualize the performance of SOD-YOLOv8s, we uti-\nlize a confusion matrix. This matrix organizes predictions\ninto a format where each row corresponds to instances of a\ntrue class label, and each column corresponds to instances\npredicted by the model. Diagonal elements indicate correct\npredictions, where the predicted class matches the actual\nclass. Off-diagonal elements represent incorrect predictions,\nFig. 11.\nConfusion matrix of YOLOv8s; (b) confusion matrix of proposed\nmodel.\nFig. 12.\nInference results for (a) YOLOv8s and (b) SOD-YOLOv8s across\ndiverse scenarios including distant and high-density objects, as well as\nnighttime scenarios, using the VisDrone2019 dataset.\nwhere the predicted class does not match the actual class.\nFigure 11 demonstrates improved detection performance\nof SOD-YOLOv8s across most object categories. The confu-\nsion matrix shows lighter shades in the last row compared to\nYOLOv8s, indicating reduced misclassifications of objects as\nbackground. However, challenges remain in accurately iden-\ntifying bicycles, tricycles, and awning-tricycles, which are\noften mislabeled as background. Despite these issues, SOD-\nYOLOv8s shows darker shades along the main diagonal,\nindicating an overall increase in correctly detected objects.\nAs depicted in Figure 12, we assess the efficacy of\nTABLE V\nCOMPARATIVE EXPERIMENTS BETWEEN THE ENHANCED MODEL AND YOLOV8S ACROSS VARIOUS CATEGORIES, WITH PERCENTAGES PRESENTED\n(BEST-PERFORMING OUTCOMES HIGHLIGHTED IN BOLD).\nModels\nPedestrian\nPeople\nBicycle\nCar\nVan\nTruck\nBus\nMotorcycle\nmAP0.5\nYOLOv8s\n43.5\n34.2\n14.9\n79.5\n45.0\n40.3\n58.1\n45.4\n40.6\nYOLOv8s-PIoU\n46\n38.2\n15.6\n80.4\n45.8\n39\n60.2\n47\n41.7\nYOLOv8s-PIoU-GFPN\n52.8\n44.0\n17.7\n84.2\n47.7\n39.4\n60.8\n53.2\n44.6\nYOLOv8s-PIoU-GFPN-EMA\n53.1\n44.5\n18.2\n83.9\n47.1\n41.0\n60.9\n53.8\n45.1\nTABLE VI\nDETECTION RESULTS FOLLOWING THE ADOPTION OF DIFFERENT IMPROVEMENT STRATEGIES, PRESENTED AS PERCENTAGES.(THE\nBEST-PERFORMING OUTCOMES ARE HIGHLIGHTED IN BOLD)\nBaseline\nPIoU\nGFPN\nEMA\nPrecision\nRecall\nmAP0.5\nmAP0.5:0.95\nDetection Time/ms\nParameter/106\n✓\n51.2\n40.1\n40.6\n24\n7.8\n11.1\n✓\n✓\n52.8\n40.5\n41.7\n24.2\n7.4\n11.1\n✓\n✓\n✓\n52.7\n44.3\n44.6\n26.3\n11.5\n11.5\n✓\n✓\n✓\n✓\n53.9\n43.9\n45.1\n26.6\n11.6\n11.5\nSOD-YOLOv8 across three challenging scenarios within\nthe\nVisdrone\ndataset:\nnighttime\nconditions,\ncrowded\nscenes with high-density objects, and scenes with distant\nobjects. Remarkably, across all three scenarios, notable\nimprovements are observed. In the nighttime scenario,\nillustrated in the first row of Figure 12, objects are\ndetected with higher IoU values, and a greater number\nof\nsmaller\nobjects\nare\nsuccessfully\nidentified.\nIn\nthe\nsecond scenario, depicted in the second row of Figure\n12,\nSOD-YOLOv8\ndemonstrates\nsuperior\nperformance\nby successfully detecting numerous small objects located\nat the corners of intersections, a task which YOLOv8s\nstruggles with. Similarly, in the third scenario involving\nobjects positioned farther from the camera, SOD-YOLOv8s\nexcels in detecting objects with higher IoU values and\nsuccessfully identifying a greater number of smaller objects.\nThese results demonstrating the substantial improvements\nprovided by SOD-YOLOv8s across different environmental\nconditions, indicating its reliability and effectiveness in\ndetecting objects in challenging scenarios.\nReal dataset results\nThis section evaluates the model’s performance in dy-\nnamic, real-world challenging scenarios where the camera\nis mounted on a building at a significant distance from the\nobjects of interest. To assess the applicability and general-\nization of the proposed SOD-YOLOv8 model, we conducted\ninference experiments using real-world data from a traffic\nscene scenario. The image data were primarily captured\nby NSF PAWR COSMOS testbed cameras [56], [57], [58]\nmounted on the 12th floor of Columbia’s Mudd building (Fig.\n13), overlooking the intersection of 120th St. and Amsterdam\nAve. in New York City. Images were specifically selected\nfrom this vantage point to utilize its elevated perspective\nand greater distance from the street. This viewpoint poses\na unique challenge for object detection, requiring enhanced\nFig. 13.\nThe perspective captured by COSMOS cameras on the 12th floor\nof Columbia’s Mudd building overlooking the intersection [58].\nperception due to the reduced scale of objects, including\nvarious vehicle types and pedestrians.\nAs depicted in Figure 14, we assess SOD-YOLOv8’s\nperformance in three challenging real-world traffic scenarios\nusing images from cameras on the 12th floor such as\ncrowded scenes with high-density objects, distant objects,\nand nighttime conditions. Significant improvements are\nobserved across all three scenarios. In the first scenario,\nshown\nin\nthe\ntop\nrow\nof\nFigure\n14,\nSOD-YOLOv8\noutperforms YOLOv8s by successfully detecting numerous\nsmall-scale pedestrians at the corners of intersections, a task\nFig. 14.\nInference results for (a) YOLOv8s and (b) SOD-YOLOv8s across\nvarious scenarios, including scenes with distant and high-density objects, as\nwell as nighttime scenarios, using the traffic scene dataset.\nwhere YOLOv8s struggles. In the second scenario, with\ndistant objects, SOD-YOLOv8 shows superior performance,\nachieving higher IoU values and effectively detecting more\nsmall objects. In the nighttime scenario, shown in the\nthird row of Figure 14, SOD-YOLOv8 achieves higher\nIoU values for detected objects and identifies more small\nobjects compared to the YOLOv8s baseline model, despite\nchallenging lighting conditions. These results illustrate the\nsubstantial improvements achieved by SOD-YOLOv8 across\ndiverse environmental conditions, highlighting its reliability\nand effective object detection capabilities in challenging\nscenarios.\nVI. CONCLUSION\nDetecting small-scale objects in traffic scenarios presents\nsignificant challenges that can reduce overall effectiveness.\nTo address these issues, we introduced SOD-YOLOv8, a\nspecialized object detection model designed for aerial pho-\ntography and traffic scenes dominated by small objects. Built\nupon YOLOv8, this model integrates enhanced multi-path\nfusion inspired by the GFPN architecture of DAMO-YOLO\nmodels, facilitating effective feature fusion across layers\nand simplifying architecture through reparameterization. By\nleveraging a high-resolution fourth layer and incorporating a\nC2f-EMA structure, SOD-YOLOv8 prioritizes small objects,\nenhances feature fusion, and improves precise localization.\nAlso PIoU is used as a replacement for CIoU, the IoU-based\nloss function in YOLOv8.\nThe SOD-YOLOv8 model outperforms widely used mod-\nels such as CenterNet, Cascade R-CNN, SSD, and Faster R-\nCNN across various evaluation metrics. Our efficient model\nsignificantly enhances object detection performance without\nsubstantially increasing computational cost or detection time\ncompared to YOLOv8s. It improves recall from 40.1%\nto 43.9%, precision from 51.2% to 53.9%, mAP0.5 from\n40.6% to 45.1%, and mAP0.5:0.95 from 24% to 26.6%. In\nreal-world traffic scenarios captured by building-mounted\ncameras, SOD-YOLOv8 achieves higher IoU values and\nidentifies more small objects than YOLOv8s, even under\nchallenging conditions like poor lighting and crowded back-\ngrounds. These capabilities make it ideal for applications\nsuch as UAV-based traffic monitoring.\nHowever, challenges remain in deploying small object de-\ntection methods in resource-constrained environments. While\nattention mechanisms and complex feature fusion improve\nperformance in controlled settings, they may struggle with\ngeneralization across diverse environments and conditions,\ncomplicating real-world deployment and maintenance. In this\nstudy, given the promising results of the used PIoU method\non the VisDrone dataset and real-world traffic scenes, which\ninvolve numerous small objects, future research will prior-\nitize evaluating PIoU across various datasets. Additionally,\nefforts will focus on refining the GFPN architecture, explor-\ning alternative processing methods, and assessing the model’s\nperformance in adverse weather conditions to enhance its\nadaptability and robustness across diverse scenarios.\nACKNOWLEDGMENT\nThis work was supported by the Center for Smart\nStreetscapes, an NSF Engineering Research Center, under\ngrant agreement EEC-2133516. The authors are grateful to\nEric Valasek and Nicholas D’Andre from Gridmatrix for\nmotivating the small object detection problem through a\nnumber of discussions\nREFERENCES\n[1] Chen, X., Ma, H., Wan, J., Li, B., and Xia, T. (2017). Multi-view\n3D object detection network for autonomous driving. In Proceedings\nof the IEEE conference on computer vision and pattern recognition\n(CVPR) (pp. 6526–6534).\n[2] M. Alqarqaz, M. Bani Younes, and R. Qaddoura, “An Object Classi-\nfication Approach for Autonomous Vehicles Using Machine Learning\nTechniques,” World Electric Vehicle Journal, vol. 14, no. 2, p. 41,\n2023.\n[3] Y. Lim, S. S. Tiang, W. H. Lim, C. H. Wong, M. Mastaneh, K. S.\nChong, and B. Sun, “Object Detection in Autonomous Vehicles: A\nPerformance Analysis,” in International Conference on Mechatron-\nics and Intelligent Robotics, Singapore, August 2023, pp. 277-291.\nSpringer Nature Singapore.\n[4] Feng, J., Wang, J., and Qin, R. (2023). Lightweight detection network\nfor arbitrary-oriented vehicles in UAV imagery via precise positional\ninformation encoding and bidirectional feature fusion. International\nJournal of Remote Sensing, 44(15), 4529-4558.\n[5] Q. Chuai, X. He, and Y. Li, “Improved Traffic Small Object Detection\nvia Cross-Layer Feature Fusion and Channel Attention,” Electronics,\nvol. 12, no. 16, p. 3421, 2023.\n[6] Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford,\nA., and Chen, X. (2016). Improved Techniques for Training GANs.\nAdvances in Neural Information Processing Systems (NeurIPS).\n[7] Alsamhi, S. H., Shvetsov, A. V., Kumar, S., Shvetsova, S. V., Alhar-\ntomi, M. A., Hawbani, A., ... and Nyangaresi, V. O. (2022). UAV\ncomputing-assisted search and rescue mission framework for disaster\nand harsh environment mitigation. Drones, 6(7), 154.\n[8] Wang, G., Chen, Y., An, P., Hong, H., Hu, J., and Huang, T. (2023).\nUAV-YOLOv8: a small-object-detection model based on improved\nYOLOv8 for UAV aerial photography scenarios. Sensors, 23(16), 7190.\n[9] Jiang, Y., Tan, Z., Wang, J., Sun, X., Lin, M., and Li, H. (2022).\nGiraffedet: A heavy-neck paradigm for object detection. arXiv preprint\narXiv:2202.04256.\n[10] D. Ouyang, S. He, G. Zhang, M. Luo, H. Guo, J. Zhan, and Z. Huang,\n”Efficient multi-scale attention module with cross-spatial learning,”\nin ICASSP 2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 1-5, IEEE, June 2023.\n[11] Xu, X., Jiang, Y., Chen, W., Huang, Y., Zhang, Y., and Sun, X.\n(2022). Damo-yolo: A report on real-time object detection design.\narXiv preprint arXiv:2211.15444.\n[12] Du, Y., and Jiang, X. (2024). A Real-Time Small Target Vehicle\nDetection Algorithm with an Improved YOLOv5m Network Model.\nComputers, Materials and Continua, 78(1).\n[13] Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). Rich\nfeature hierarchies for accurate object detection and semantic segmen-\ntation. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (pp. 580–587). Columbus, OH, USA.\n[14] Girshick, R. (2015). Fast R-CNN. arXiv preprint arXiv:1504.08083.\n[15] Ren, S., He, K., Girshick, R., and Sun, J. (2015). Faster R-CNN:\nTowards real-time object detection with region proposal networks. In\nProceedings of the 28th International Conference on Neural Infor-\nmation Processing Systems (pp. 21–37). Montreal, QC, Canada: MIT\nPress.\n[16] Deng, Z., Sun, H., Zhou, S., Zhao, J., Lei, L., and Zou, H. (2018).\nMulti-scale object detection in remote sensing imagery with convolu-\ntional neural networks. ISPRS Journal of Photogrammetry and Remote\nSensing, 145, 3-22.\n[17] Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). You\nOnly Look Once: Unified, Real-Time Object Detection. In Proceedings\nof the IEEE Computer Society Conference on Computer Vision and\nPattern Recognition, Las Vegas, NV, USA, 27–30 June 2016 (pp.\n779–788). IEEE Computer Society: Washington DC, USA.\n[18] Redmon, J., and Farhadi, A. (2017). YOLO9000: Better, faster,\nstronger. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017 (pp.\n7263–7271).\n[19] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.\nY., and Berg, A. C. (2016). SSD: Single shot multibox detector. In\nProceedings of the 14th European Conference on Computer Vision,\nAmsterdam, The Netherlands, 11–14 October 2016 (pp. 21–37).\nSpringer: Cham, Switzerland.\n[20] Liu, M., Wang, X., Zhou, A., Fu, X., Ma, Y., and Piao, C. (2020). UAV-\nYOLO: Small object detection on unmanned aerial vehicle perspective.\nSensors, 20(8), 2238.\n[21] Liu, W., Qiang, J., Li, X., Guan, P., and Du, Y. (2022). UAV image\nsmall object detection based on composite backbone network. Mobile\nInformation Systems, 2022(1), 7319529.\n[22] Liu, Y., Yang, F., and Hu, P. (2020). Small-object detection in UAV-\ncaptured images via multi-branch parallel feature pyramid networks.\nIEEE Access, 8, 145740-145750.\n[23] Zhu, P.; Wen, L.; Du, D.; Bian, X.; Fan, H.; Hu, Q.; Ling, H. Detection\nand Tracking Meet Drones Challenge. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 2021, 44, 7380–7399.\n[24] Lai, H., Chen, L., Liu, W., Yan, Z., and Ye, S. (2023). STC-\nYOLO: Small object detection network for traffic signs in complex\nenvironments. Sensors, 23(11), 5307.\n[25] Shen, L., Lang, B., and Song, Z. (2023). DS-YOLOv8-based object de-\ntection method for remote sensing images. IEEE Access, 11, 125122-\n125137.\n[26] Tong, Z., Chen, Y., Xu, Z., and Yu, R. (2023). Wise-IoU: bounding\nbox regression loss with dynamic focusing mechanism. arXiv preprint\narXiv:2301.10051.\n[27] Wang, H., Yang, H., Chen, H., Wang, J., Zhou, X., and Xu, Y. (2024).\nA remote sensing image target detection algorithm based on improved\nYOLOv8. Applied Sciences, 14(4), 1557.\n[28] Xia, G. S., Bai, X., Ding, J., Zhu, Z., Belongie, S., Luo, J., ... and\nZhang, L. (2018). DOTA: A large-scale dataset for object detection in\naerial images. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (pp. 3974-3983).\n[29] Xu, W., Cui, C., Ji, Y., Li, X., and Li, S. (2024). YOLOv8-MPEB small\ntarget detection algorithm based on UAV images. Heliyon, 10(8).\n[30] Redmon, J., and Farhadi, A. (2018). Yolov3: An incremental improve-\nment. arXiv preprint arXiv:1804.02767.\n[31] Tan, M., Pang, R., and Le, Q. V. (2020). Efficientdet: Scalable and\nefficient object detection. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2020, pp.\n10781–10790.\n[32] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang\nChen, Jun-Wei Hsieh, and I-Hau Yeh. CSPNet: A new backbone\nthat can enhance learning capability of CNN. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops, pages 390–391, 2020.\n[33] Kang, M., Ting, C. M., Ting, F. F., and Phan, R. C. W. (2023). Bgf-\nyolo: Enhanced yolov8 with multiscale attentional feature fusion for\nbrain tumor detection. arXiv preprint arXiv:2309.12585.\n[34] He, K., Zhang, X., Ren, S., and Sun, J. (2015). Spatial pyramid\npooling in deep convolutional networks for visual recognition. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 37(9),\n1904-1916.\n[35] Lin, T.-Y., Dollar, P., Girshick, R., He, K., Hariharan, B., and Be-\nlongie, S. (2017). Feature pyramid networks for object detection. In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017, pp. 2117–2125.\n[36] Liu, S.; Qi, L.; Qin, H.; Shi, J.; Jia, J. Path Aggregation Network\nfor Instance Segmentation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, Salt Lake City, UT, USA,\n18–23 June 2018; pp. 8759–8768.\n[37] Feng, C.; Zhong, Y.; Gao, Y.; Scott, M.R.; Huang, W. TOOD: Task-\nAligned One-Stage Object Detection. In Proceedings of the 2021 IEEE\nInternational Conference on Computer Vision (ICCV), Montreal, QC,\nCanada, 10–17 October 2021; pp. 3490–3499.\n[38] Li, X., Wang, W., Wu, L., Chen, S., Hu, X., Li, J., Tang, J.,\nand Yang, J. (2020). Generalized Focal Loss: Learning Qualified\nand Distributed Bounding Boxes for Dense Object Detection. arXiv,\narXiv:2006.04388.\n[39] Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., and Ren, D. (2020).\nDistance-iou loss: Faster and better learning for bounding box regres-\nsion. In Proceedings of the AAAI Conference on Artificial Intelligence,\nVol. 34 (07), (pp. 12993–13000).\n[40] Yu, J., Jiang, Y., Wang, Z., Cao, Z., and Huang, T. (2016). Unitbox:\nAn advanced object detection network. In Proceedings of the 24th\nACM International Conference on Multimedia (pp. 516–520).\n[41] Wang, H., Yang, H., Chen, H., Wang, J., Zhou, X., and Xu, Y. (2024).\nA remote sensing image target detection algorithm based on improved\nYOLOv8. Applied Sciences, 14(4), 1557.\n[42] Liu, C., Wang, K., Li, Q., Zhao, F., Zhao, K., and Ma, H. (2024).\nPowerful-IoU: More straightforward and faster bounding box regres-\nsion loss with a nonmonotonic focusing mechanism. Neural Networks,\n170, 276-284.\n[43] Loshchilov, I., Hutter, F. (2017). SGDR: Stochastic Gradient Descent\nwith Warm Restarts. Proceedings of the 5th International Conference\non Learning Representations (ICLR), Toulon, France, 24-26 April\n2017.\n[44] Siliang, M., and Yong, X. (2023). Mpdiou: a loss for efficient and\naccurate bounding box regression. arXiv preprint arXiv:2307.07662.\n[45] Woo, S., Park, J., Lee, J. Y., and Kweon, I. S. (2018). CBAM: Con-\nvolutional Block Attention Module. In Proceedings of the European\nConference on Computer Vision (ECCV) (pp. 3-19).\n[46] Hou, Q., Zhou, D., and Feng, J. (2021). Coordinate Attention for\nEfficient Mobile Network Design. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (pp. 13713-\n13722).\n[47] Hu, J., Shen, L., and Sun, G. (2018). Squeeze-and-Excitation Net-\nworks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (pp. 7132-7141).\n[48] Ren, S.; He, K.; Girshick, R.; Sun, J. (2017). Faster R-CNN: Towards\nReal-Time Object Detection with Region Proposal Networks. IEEE\nTrans. Pattern Anal. Mach. Intell., 39, 1137–1149.\n[49] Ren, S., He, K., Girshick, R., Sun, J. (2015). Faster R-CNN: Towards\nReal-Time Object Detection with Region Proposal Networks. In Pro-\nceedings of the Advances in Neural Information Processing Systems\n28, Montreal, QC, Canada, 7–12 December 2015.\n[50] Chien-Yao Wang, Alexander Bochkovskiy, and Hong-Yuan Mark Liao.\nYOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-\ntime object detectors. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 7464–7475, 2023.\n[51] Reis, D., Kupec, J., Hong, J., and Daoudi, A. (2023). Real-time flying\nobject detection with YOLOv8. arXiv preprint arXiv:2305.09972.\n[52] Cai, Z.; Vasconcelos, N. (2018). Cascade R-CNN: Delving into High\nQuality Object Detection.\nIn Proceedings of the 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition, Salt Lake\nCity, UT, USA, 18–22 June 2018; pp. 6154–6162.\n[53] Duan, K., Bai, S., Xie, L., Qi, H., Huang, Q., and Tian, Q. (2019).\nCenterNet: Keypoint triplets for object detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 6569-\n6578.\n[54] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning\nfor image recognition.\nIn Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 770-778.\n[55] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L. C.\n(2018). MobileNetV2: Inverted residuals and linear bottlenecks. In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 4510-4520.\n[56] D. Raychaudhuri, I. Seskar, G. Zussman, T. Korakis, D. Kilper, T.\nChen, J. Kolodziejski, M. Sherman, Z. Kostic, X. Gu, H. Krish-\nnaswamy, S. Maheshwari, P. Skrimponis, and C. Gutterman, ”Chal-\nlenge: COSMOS: A city-scale programmable testbed for experimen-\ntation with advanced wireless,” in Proc. ACM MobiCom’20, 2020.\n[57] Z. Kostic, A. Angus, Z. Yang, Z. Duan, I. Seskar, G. Zussman, and D.\nRaychaudhuri, ”Smart city intersections: Intelligence nodes for future\nmetropolises,” IEEE Comp., vol. 55, no. 12, pp. 74–85, 2022.\n[58] COSMOS Project, “Hardware: Cameras,” 2022. [Online]. Available:\nhttps://wiki.cosmos-lab.org/wiki/Hardware/\nCameras.\n' metadata={'Published': '2024-08-08', 'Title': 'SOD-YOLOv8 -- Enhancing YOLOv8 for Small Object Detection in Traffic Scenes', 'Authors': 'Boshra Khalili, Andrew W. Smyth', 'Summary': 'Object detection as part of computer vision can be crucial for traffic\\nmanagement, emergency response, autonomous vehicles, and smart cities. Despite\\nsignificant advances in object detection, detecting small objects in images\\ncaptured by distant cameras remains challenging due to their size, distance\\nfrom the camera, varied shapes, and cluttered backgrounds. To address these\\nchallenges, we propose Small Object Detection YOLOv8 (SOD-YOLOv8), a novel\\nmodel specifically designed for scenarios involving numerous small objects.\\nInspired by Efficient Generalized Feature Pyramid Networks (GFPN), we enhance\\nmulti-path fusion within YOLOv8 to integrate features across different levels,\\npreserving details from shallower layers and improving small object detection\\naccuracy. Also, A fourth detection layer is added to leverage high-resolution\\nspatial information effectively. The Efficient Multi-Scale Attention Module\\n(EMA) in the C2f-EMA module enhances feature extraction by redistributing\\nweights and prioritizing relevant features. We introduce Powerful-IoU (PIoU) as\\na replacement for CIoU, focusing on moderate-quality anchor boxes and adding a\\npenalty based on differences between predicted and ground truth bounding box\\ncorners. This approach simplifies calculations, speeds up convergence, and\\nenhances detection accuracy. SOD-YOLOv8 significantly improves small object\\ndetection, surpassing widely used models in various metrics, without\\nsubstantially increasing computational cost or latency compared to YOLOv8s.\\nSpecifically, it increases recall from 40.1\\\\% to 43.9\\\\%, precision from 51.2\\\\%\\nto 53.9\\\\%, $\\\\text{mAP}_{0.5}$ from 40.6\\\\% to 45.1\\\\%, and\\n$\\\\text{mAP}_{0.5:0.95}$ from 24\\\\% to 26.6\\\\%. In dynamic real-world traffic\\nscenes, SOD-YOLOv8 demonstrated notable improvements in diverse conditions,\\nproving its reliability and effectiveness in detecting small objects even in\\nchallenging environments.'}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Document(metadata={'Entry ID': 'http://arxiv.org/abs/2408.04786v1', 'Published': datetime.date(2024, 8, 8), 'Title': 'SOD-YOLOv8 -- Enhancing YOLOv8 for Small Object Detection in Traffic Scenes', 'Authors': 'Boshra Khalili, Andrew W. Smyth'}, page_content='Object detection as part of computer vision can be crucial for traffic\\nmanagement, emergency response, autonomous vehicles, and smart cities. Despite\\nsignificant advances in object detection, detecting small objects in images\\ncaptured by distant cameras remains challenging due to their size, distance\\nfrom the camera, varied shapes, and cluttered backgrounds. To address these\\nchallenges, we propose Small Object Detection YOLOv8 (SOD-YOLOv8), a novel\\nmodel specifically designed for scenarios involving numerous small objects.\\nInspired by Efficient Generalized Feature Pyramid Networks (GFPN), we enhance\\nmulti-path fusion within YOLOv8 to integrate features across different levels,\\npreserving details from shallower layers and improving small object detection\\naccuracy. Also, A fourth detection layer is added to leverage high-resolution\\nspatial information effectively. The Efficient Multi-Scale Attention Module\\n(EMA) in the C2f-EMA module enhances feature extraction by redistributing\\nweights and prioritizing relevant features. We introduce Powerful-IoU (PIoU) as\\na replacement for CIoU, focusing on moderate-quality anchor boxes and adding a\\npenalty based on differences between predicted and ground truth bounding box\\ncorners. This approach simplifies calculations, speeds up convergence, and\\nenhances detection accuracy. SOD-YOLOv8 significantly improves small object\\ndetection, surpassing widely used models in various metrics, without\\nsubstantially increasing computational cost or latency compared to YOLOv8s.\\nSpecifically, it increases recall from 40.1\\\\% to 43.9\\\\%, precision from 51.2\\\\%\\nto 53.9\\\\%, $\\\\text{mAP}_{0.5}$ from 40.6\\\\% to 45.1\\\\%, and\\n$\\\\text{mAP}_{0.5:0.95}$ from 24\\\\% to 26.6\\\\%. In dynamic real-world traffic\\nscenes, SOD-YOLOv8 demonstrated notable improvements in diverse conditions,\\nproving its reliability and effectiveness in detecting small objects even in\\nchallenging environments.')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# Supports all arguments of `ArxivAPIWrapper`\n",
    "loader = ArxivLoader(\n",
    "    query=\"Yolov8\",\n",
    "    load_max_docs=2,\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs[0])\n",
    "\n",
    "# get the summary of the paper\n",
    "docs = loader.get_summaries_as_docs()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7c6066-9d5b-4279-b93a-970519358c56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom Data Loader\n",
    "If you don’t find any Data Loader class for your specific data type, then you can create a Custom Document Loader to convert your data into LLM-ready documents. Now create a custom document loader from Subclassing from BaseLoader that loads a file and creates a document from each line in the file. The document contains text and metadata. The BaseLoader class is used to convert the raw data into documents.\n",
    "\n",
    "Now create a CustomDocumentLoader for loading the data and converting it into Documents. Then create a text file and load the text file through Custom Document Loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6e082f-cd3b-474d-80c6-239d616d1316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n<class 'langchain_core.documents.base.Document'>\npage_content='meow meow🐱 \n' metadata={'line_number': 0, 'source': './meow.txt'}\n\n<class 'langchain_core.documents.base.Document'>\npage_content=' meow meow🐱 \n' metadata={'line_number': 1, 'source': './meow.txt'}\n\n<class 'langchain_core.documents.base.Document'>\npage_content=' meow😻😻' metadata={'line_number': 2, 'source': './meow.txt'}\n"
     ]
    }
   ],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class CustomDocumentLoader(BaseLoader):\n",
    "    \"\"\"An example document loader that reads a file line by line.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str) -> None:\n",
    "        \"\"\"Initialize the loader with a file path.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path to the file to load.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments\n",
    "        \"\"\"A lazy loader that reads a file line by line.\n",
    "\n",
    "        When you're implementing lazy load methods, you should use a generator\n",
    "        to yield documents one by one.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, encoding=\"utf-8\") as f:\n",
    "            line_number = 0\n",
    "            for line in f:\n",
    "                yield Document(\n",
    "                    page_content=line,\n",
    "                    metadata={\"line_number\": line_number, \"source\": self.file_path},\n",
    "                )\n",
    "                line_number += 1\n",
    "\n",
    "# create a text file\n",
    "with open(\"./meow.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    quality_content = \"meow meow🐱 \\n meow meow🐱 \\n meow😻😻\"\n",
    "    f.write(quality_content)\n",
    "\n",
    "# create instance of Custom Document Loader with the text file\n",
    "loader = CustomDocumentLoader(\"./meow.txt\")\n",
    "\n",
    "# load the data and convert into Documents\n",
    "for doc in loader.lazy_load():\n",
    "    print()\n",
    "    print(type(doc))\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83cc5671-817a-4d57-9c5b-8c93dd229337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "M2_LangChain_Document_Loaders",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f6d1f3099c2428bbce1f41dd04cdc4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43b3d3c90225407e943fc9dcb39ad53e",
      "max": 115434268,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb61550b3a774725a24e4e2a9ace15d5",
      "value": 115434268
     }
    },
    "1a85a1b9c4494ccfb66d5c351fdc42b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bb6db9f42c047b8a65dcf4b93d84bbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c7f0cf2749146668bd8df52ffe11fad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34994b355d1841ec9019ec2a7c5dd408": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3865ca39e1bc4a50ab761d215de760d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73c52584484948f1a8bd01beb4969315",
      "max": 46807446,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46b6d128201f4f479c45b231ece02270",
      "value": 46807446
     }
    },
    "432ad00a5bd84f1e9ed0cf8c881bbd74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34994b355d1841ec9019ec2a7c5dd408",
      "placeholder": "​",
      "style": "IPY_MODEL_68f35650c8ea40caa773596bf12c1845",
      "value": " 46.8M/46.8M [00:00&lt;00:00, 238MB/s]"
     }
    },
    "43b3d3c90225407e943fc9dcb39ad53e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43b9cf97a2044db3a65ba7a013da8d80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b6d128201f4f479c45b231ece02270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a47716561f74d8fa006b47f64a6837a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6b2ee0c072e457db0d8989b8ed1aaac",
      "placeholder": "​",
      "style": "IPY_MODEL_b8bffaaa7c524cacb9cb2a5f67581e7f",
      "value": " 1.47k/1.47k [00:00&lt;00:00, 83.7kB/s]"
     }
    },
    "63a6b02ceb9340aba44b6891be465f0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63fc017f50be4f1f8c807f30a7ce11eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79141af480654819ae6c0981732355a2",
      "max": 1469,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b217d584ee945a9bfedf1a6f33bfb67",
      "value": 1469
     }
    },
    "677e724e773541898d7682c8b8d8ff61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67f5f48c907f47f7a64bfdef3fb25c8b",
       "IPY_MODEL_3865ca39e1bc4a50ab761d215de760d7",
       "IPY_MODEL_432ad00a5bd84f1e9ed0cf8c881bbd74"
      ],
      "layout": "IPY_MODEL_1a85a1b9c4494ccfb66d5c351fdc42b3"
     }
    },
    "67f5f48c907f47f7a64bfdef3fb25c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43b9cf97a2044db3a65ba7a013da8d80",
      "placeholder": "​",
      "style": "IPY_MODEL_c5aa729abde54a6692fec1aa39c4f49c",
      "value": "model.safetensors: 100%"
     }
    },
    "68f35650c8ea40caa773596bf12c1845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7194f73d47a0456a8d462102d8b24288": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73c52584484948f1a8bd01beb4969315": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79141af480654819ae6c0981732355a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822a272cf0724edb8e7fdccd5317e86e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b217d584ee945a9bfedf1a6f33bfb67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b4c4b57aa1f94e958c0d09c7558b7817": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8aab8494a2045bdbc09415b1418bd81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8bffaaa7c524cacb9cb2a5f67581e7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbdf731fc0294dd6a3c392012bc93896": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b8aab8494a2045bdbc09415b1418bd81",
      "placeholder": "​",
      "style": "IPY_MODEL_b4c4b57aa1f94e958c0d09c7558b7817",
      "value": "config.json: 100%"
     }
    },
    "c5aa729abde54a6692fec1aa39c4f49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c69e38f11339431cac851d749a68ed9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc62a8b1ab8045caa46adac2bad7aef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c7f0cf2749146668bd8df52ffe11fad",
      "placeholder": "​",
      "style": "IPY_MODEL_63a6b02ceb9340aba44b6891be465f0c",
      "value": " 115M/115M [00:00&lt;00:00, 195MB/s]"
     }
    },
    "dee9511a788d41bab9d5fdd4b6e4f6a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff2dfe08ffed46d2a633ac0b7b0eeec5",
       "IPY_MODEL_0f6d1f3099c2428bbce1f41dd04cdc4e",
       "IPY_MODEL_cc62a8b1ab8045caa46adac2bad7aef2"
      ],
      "layout": "IPY_MODEL_1bb6db9f42c047b8a65dcf4b93d84bbf"
     }
    },
    "e606cf098a0c47388ab4a2f387b3beac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbdf731fc0294dd6a3c392012bc93896",
       "IPY_MODEL_63fc017f50be4f1f8c807f30a7ce11eb",
       "IPY_MODEL_4a47716561f74d8fa006b47f64a6837a"
      ],
      "layout": "IPY_MODEL_c69e38f11339431cac851d749a68ed9a"
     }
    },
    "e6b2ee0c072e457db0d8989b8ed1aaac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb61550b3a774725a24e4e2a9ace15d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ff2dfe08ffed46d2a633ac0b7b0eeec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7194f73d47a0456a8d462102d8b24288",
      "placeholder": "​",
      "style": "IPY_MODEL_822a272cf0724edb8e7fdccd5317e86e",
      "value": "model.safetensors: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
