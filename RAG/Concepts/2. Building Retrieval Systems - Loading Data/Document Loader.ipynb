{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a728970c",
   "metadata": {},
   "source": [
    "**Reference Link:** [RAG Systems Essentials (Analytics Vidhya)](https://courses.analyticsvidhya.com/courses/take/rag-systems-essentials/lessons/60148017-hands-on-deep-dive-into-rag-evaluation-metrics-generator-metrics-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6fa8d",
   "metadata": {},
   "source": [
    "# Exploring Document Loaders in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca6b61",
   "metadata": {},
   "source": [
    "## Install OpenAI, HuggingFace and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c8a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain==0.3.11\n",
    "!pip install -qq langchain-openai==0.2.12\n",
    "!pip install -qq langchain-community==0.3.11\n",
    "!pip install -qq jq==1.7.0\n",
    "!pip install -qq pypdf==4.2.0\n",
    "!pip install -qq PyMuPDF==1.24.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec06853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.3.11 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-chroma 0.1.4 requires numpy<2,>=1; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.2 which is incompatible.\n",
      "langchain-huggingface 0.3.0 requires huggingface-hub>=0.30.2, but you have huggingface-hub 0.27.1 which is incompatible.\n",
      "langchain-huggingface 0.3.0 requires langchain-core<1.0.0,>=0.3.65, but you have langchain-core 0.3.63 which is incompatible.\n",
      "langchain 0.3.11 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# takes 2 - 5 mins to install on Colab\n",
    "!pip install -qq \"unstructured[all-docs]==0.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f54ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq -U pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "633f2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq pytesseract\n",
    "!pip install -qq pdf2image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61302e44",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "Document loaders are used to import data from various sources into LangChain as `Document` objects. A `Document` typically includes a piece of text along with its associated metadata.\n",
    "\n",
    "### Examples of Document Loaders:\n",
    "\n",
    "- **Text File Loader:** Loads data from a simple `.txt` file.\n",
    "- **Web Page Loader:** Retrieves the text content from any web page.\n",
    "- **YouTube Video Transcript Loader:** Loads transcripts from YouTube videos.\n",
    "\n",
    "### Functionality:\n",
    "\n",
    "- **Load Method:** Each document loader has a `load` method that enables the loading of data as documents from a pre-configured source.\n",
    "- **Lazy Load Option:** Some loaders also support a \"lazy load\" feature, which allows data to be loaded into memory gradually as needed.\n",
    "\n",
    "For more detailed information, visit [LangChain's document loader documentation](https://python.langchain.com/docs/modules/data_connection/document_loaders/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bb1ed",
   "metadata": {},
   "source": [
    "### Text Loader\n",
    "\n",
    "The simplest loader reads in a file as text and places it all into one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24a4afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TextLoader from langchain_community.document_loaders\n",
    "# This loader is used to read text files and convert them into Document objects\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../../docs/dummy.txt\")\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1322c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of documents : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe number of documents : {len(doc)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0144d021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6347c588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Type of Each Document in the Doc list is 'langchain_core.documents.base.Document'\n",
    "type(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92af8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Type of first documents : <class 'langchain_core.documents.base.Document'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Type of first documents : {type(doc[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c6a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole documents are: \n",
      "[Document(metadata={'source': '../../docs/dummy.txt'}, page_content='Quod equidem non reprehendo;\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?\\n\\nIam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?\\n\\nQuis istum dolorem timet?\\nSummus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.\\n\\nEx eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.\\nQuod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?\\n\\nQuem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.\\nStulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.\\nEsse enim quam vellet iniquus iustus poterat inpune.\\nQuae autem natura suae primae institutionis oblita est?\\nVerum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;\\nHoc est non modo cor non habere, sed ne palatum quidem.\\nVoluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?\\n\\nIdemne, quod iucunde?\\nHaec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the entire document object to see its structure and content\n",
    "# This will show us the Document object with its page_content and metadata\n",
    "\n",
    "print(f\"The whole documents are: \\n{doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "171919c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first document is: \n",
      "page_content='Quod equidem non reprehendo;\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?\n",
      "\n",
      "Iam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?\n",
      "\n",
      "Quis istum dolorem timet?\n",
      "Summus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.\n",
      "\n",
      "Ex eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.\n",
      "Quod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?\n",
      "\n",
      "Quem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.\n",
      "Stulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.\n",
      "Esse enim quam vellet iniquus iustus poterat inpune.\n",
      "Quae autem natura suae primae institutionis oblita est?\n",
      "Verum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;\n",
      "Hoc est non modo cor non habere, sed ne palatum quidem.\n",
      "Voluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?\n",
      "\n",
      "Idemne, quod iucunde?\n",
      "Haec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?' metadata={'source': '../../docs/dummy.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first document is: \\n{doc[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fff603bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of first document : \n",
      "Quod equidem non reprehendo;\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura iure responderit non esse verum aliunde finem beate vivendi, a se principia rei gerendae peti; Quae enim adhuc protulisti, popularia sunt, ego autem a te elegantiora desidero. Duo Reges: constructio interrete. Tum Lucius: Mihi vero ista valde probata sunt, quod item fratri puto. Bestiarum vero nullum iudicium puto. Nihil enim iam habes, quod ad corpus referas; Deinde prima illa, quae in congressu solemus: Quid tu, inquit, huc? Et homini, qui ceteris animantibus plurimum praestat, praecipue a natura nihil datum esse dicemus?\n",
      "\n",
      "Iam id ipsum absurdum, maximum malum neglegi. Quod ea non occurrentia fingunt, vincunt Aristonem; Atqui perspicuum est hominem e corpore animoque constare, cum primae sint animi partes, secundae corporis. Fieri, inquam, Triari, nullo pacto potest, ut non dicas, quid non probes eius, a quo dissentias. Equidem e Cn. An dubium est, quin virtus ita maximam partem optineat in rebus humanis, ut reliquas obruat?\n",
      "\n",
      "Quis istum dolorem timet?\n",
      "Summus dolor plures dies manere non potest? Dicet pro me ipsa virtus nec dubitabit isti vestro beato M. Tubulum fuisse, qua illum, cuius is condemnatus est rogatione, P. Quod si ita sit, cur opera philosophiae sit danda nescio.\n",
      "\n",
      "Ex eorum enim scriptis et institutis cum omnis doctrina liberalis, omnis historia.\n",
      "Quod si ita est, sequitur id ipsum, quod te velle video, omnes semper beatos esse sapientes. Cum enim fertur quasi torrens oratio, quamvis multa cuiusque modi rapiat, nihil tamen teneas, nihil apprehendas, nusquam orationem rapidam coerceas. Ita redarguitur ipse a sese, convincunturque scripta eius probitate ipsius ac moribus. At quanta conantur! Mundum hunc omnem oppidum esse nostrum! Incendi igitur eos, qui audiunt, vides. Vide, ne magis, inquam, tuum fuerit, cum re idem tibi, quod mihi, videretur, non nova te rebus nomina inponere. Qui-vere falsone, quaerere mittimus-dicitur oculis se privasse; Si ista mala sunt, in quae potest incidere sapiens, sapientem esse non esse ad beate vivendum satis. At vero si ad vitem sensus accesserit, ut appetitum quendam habeat et per se ipsa moveatur, quid facturam putas?\n",
      "\n",
      "Quem si tenueris, non modo meum Ciceronem, sed etiam me ipsum abducas licebit.\n",
      "Stulti autem malorum memoria torquentur, sapientes bona praeterita grata recordatione renovata delectant.\n",
      "Esse enim quam vellet iniquus iustus poterat inpune.\n",
      "Quae autem natura suae primae institutionis oblita est?\n",
      "Verum tamen cum de rebus grandioribus dicas, ipsae res verba rapiunt;\n",
      "Hoc est non modo cor non habere, sed ne palatum quidem.\n",
      "Voluptatem cum summum bonum diceret, primum in eo ipso parum vidit, deinde hoc quoque alienum; Sed tu istuc dixti bene Latine, parum plane. Nam haec ipsa mihi erunt in promptu, quae modo audivi, nec ante aggrediar, quam te ab istis, quos dicis, instructum videro. Fatebuntur Stoici haec omnia dicta esse praeclare, neque eam causam Zenoni desciscendi fuisse. Non autem hoc: igitur ne illud quidem. Ratio quidem vestra sic cogit. Cum audissem Antiochum, Brute, ut solebam, cum M. An quod ita callida est, ut optime possit architectari voluptates?\n",
      "\n",
      "Idemne, quod iucunde?\n",
      "Haec mihi videtur delicatior, ut ita dicam, molliorque ratio, quam virtutis vis gravitasque postulat. Sed quoniam et advesperascit et mihi ad villam revertendum est, nunc quidem hactenus; Cuius ad naturam apta ratio vera illa et summa lex a philosophis dicitur. Neque solum ea communia, verum etiam paria esse dixerunt. Sed nunc, quod agimus; A mene tu?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Content of first document : \\n{doc[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02643044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first document's metadata is: \n",
      "{'source': '../../docs/dummy.txt'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first document's metadata is: \\n{doc[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f15544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quod equidem non reprehendo;\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quibus natura \n"
     ]
    }
   ],
   "source": [
    "print(doc[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695652d4",
   "metadata": {},
   "source": [
    "### Markdown Loader\n",
    "\n",
    "* Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\n",
    "  \n",
    "* This showcases how to load Markdown documents into a langchain document format that we can use in our pipelines and chains.\n",
    "  \n",
    "* This Loader loads the whole document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3981cb",
   "metadata": {},
   "source": [
    "#### Download nltk packages if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc3baf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download NLTK data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sourav.banerjee/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/sourav.banerjee/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NLTK data downloaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "# Solution for SSL certificate verification errors when downloading NLTK data\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    # This bypasses SSL certificate verification (for development/testing environments)\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Now try downloading the NLTK data\n",
    "try:\n",
    "    print(\"Attempting to download NLTK data...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    print(\"✅ NLTK data downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error downloading NLTK data: {e}\")\n",
    "    print(\"🔄 Trying alternative approach...\")\n",
    "    \n",
    "    # Alternative: Download older/stable versions of the packages\n",
    "    try:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        print(\"✅ Alternative NLTK data downloaded successfully!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Alternative download also failed: {e2}\")\n",
    "        print(\"\\n🔧 Manual solutions:\")\n",
    "        print(\"1. Update certificates: /Applications/Python\\\\ 3.x/Install\\\\ Certificates.command\")\n",
    "        print(\"2. Use conda: conda install nltk\")\n",
    "        print(\"3. Download manually from: https://www.nltk.org/data.html\")\n",
    "        \n",
    "        # Last resort: Check if the data already exists\n",
    "        try:\n",
    "            import nltk.data\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            print(\"✅ NLTK punkt data already available!\")\n",
    "        except LookupError:\n",
    "            print(\"❌ NLTK punkt data not found. Manual installation required.\")\n",
    "            \n",
    "        try:\n",
    "            import nltk.data\n",
    "            nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "            print(\"✅ NLTK averaged_perceptron_tagger data already available!\")\n",
    "        except LookupError:\n",
    "            print(\"❌ NLTK averaged_perceptron_tagger data not found. Manual installation required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33c0ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The following code demonstrates how to use the UnstructuredMarkdownLoader from LangChain\n",
    "# to load a markdown file (\"./docs/README.md\") as a list of Document objects.\n",
    "# The loader reads the file and processes it into a format suitable for downstream retrieval tasks.\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# The `mode='single'` parameter tells the UnstructuredMarkdownLoader to load the entire markdown file as a single Document object,\n",
    "# rather than splitting it into multiple documents (e.g., by headings or sections).\n",
    "# This is useful when you want to treat the whole file as one unit for retrieval or processing,\n",
    "# preserving the full context and structure of the original markdown.\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"../../docs/README.md\", mode='single')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf9a729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of documents : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe number of documents : {len(docs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83d453db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Type of first documents : <class 'langchain_core.documents.base.Document'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Type of first documents : {type(docs[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e03b3ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad777878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../../docs/README.md'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef7f9f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦜️🔗 LangChain\n",
      "\n",
      "⚡ Build context-aware reasoning applications ⚡\n",
      "\n",
      "Looking for the JS/TS library? Check \n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf21de",
   "metadata": {},
   "source": [
    "#### Load document and separate based on elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "275d98a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredMarkdownLoader(\"../../docs/README.md\", mode=\"elements\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98e9a610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of documents : 63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nThe number of documents : {len(docs)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83fce12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a'}, page_content='🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': '80d06543c0c2b75ca147f3509e518a47'}, page_content='⚡ Build context-aware reasoning applications ⚡'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'd68276ff4183b272b9dec78754e769b1'}, page_content='Looking for the JS/TS library? Check out LangChain.js.'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'e26407512e7a4e24a519ceb1a9dc980d'}, page_content='To help you ship LangChain apps to production faster, check out LangSmith.\\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\\nFill out this form to speak with our sales team.'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '738df4293c5c58dbaa314f6e31f2c15c'}, page_content='Quick Install'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '17c3a345872b736bc4edb6cdae73f45a'}, page_content='With pip:'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '0dbefd69ddc0b2c597762e19b29f8e27'}, page_content='bash\\npip install langchain'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'Title', 'element_id': '3c98bc06e5ef8caa9446bb6f1d33dde0'}, page_content='With conda:'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '3c98bc06e5ef8caa9446bb6f1d33dde0', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'NarrativeText', 'element_id': 'e171db3a99f130885e84934a210c7dd8'}, page_content='bash\\nconda install langchain -c conda-forge'),\n",
       " Document(metadata={'source': '../../docs/README.md', 'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md', 'category': 'Title', 'element_id': 'a8858c1e5d7fad28685e95bd1bbeacc1'}, page_content='🤔 What is LangChain?')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9e5bdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ListItem': 26, 'Title': 20, 'NarrativeText': 17})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([doc.metadata['category'] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc533de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../../docs/README.md',\n",
       " 'last_modified': '2025-05-30T10:16:46',\n",
       " 'languages': ['eng'],\n",
       " 'filetype': 'text/markdown',\n",
       " 'file_directory': '../../docs',\n",
       " 'filename': 'README.md',\n",
       " 'category': 'Title',\n",
       " 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a47a08b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🦜️🔗 LangChain'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e70296c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../../docs/README.md',\n",
       " 'last_modified': '2025-05-30T10:16:46',\n",
       " 'languages': ['eng'],\n",
       " 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
       " 'filetype': 'text/markdown',\n",
       " 'file_directory': '../../docs',\n",
       " 'filename': 'README.md',\n",
       " 'category': 'NarrativeText',\n",
       " 'element_id': '80d06543c0c2b75ca147f3509e518a47'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c906bb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⚡ Build context-aware reasoning applications ⚡'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d552c3",
   "metadata": {},
   "source": [
    "#### Comparing Unstructured.io loaders vs LangChain wrapper API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8883e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "\n",
    "docs = partition_md(filename=\"../../docs/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24c4d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7918f28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x307623b90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x30762cfd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x30762ef50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x30762ebd0>,\n",
       " <unstructured.documents.elements.Title at 0x30762d2d0>,\n",
       " <unstructured.documents.elements.Title at 0x30762d010>,\n",
       " <unstructured.documents.elements.Title at 0x307121410>,\n",
       " <unstructured.documents.elements.Title at 0x305b11010>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x305b13fd0>,\n",
       " <unstructured.documents.elements.Title at 0x305b11cd0>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a9bc0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Title',\n",
       " 'element_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
       " 'text': '🦜️🔗 LangChain',\n",
       " 'metadata': {'last_modified': '2025-05-30T10:16:46',\n",
       "  'languages': ['eng'],\n",
       "  'filetype': 'text/markdown',\n",
       "  'file_directory': '../../docs',\n",
       "  'filename': 'README.md'}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "607822b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'NarrativeText',\n",
       " 'element_id': '80d06543c0c2b75ca147f3509e518a47',\n",
       " 'text': '⚡ Build context-aware reasoning applications ⚡',\n",
       " 'metadata': {'last_modified': '2025-05-30T10:16:46',\n",
       "  'languages': ['eng'],\n",
       "  'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a',\n",
       "  'filetype': 'text/markdown',\n",
       "  'file_directory': '../../docs',\n",
       "  'filename': 'README.md'}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b453f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='🦜️🔗 LangChain'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='⚡ Build context-aware reasoning applications ⚡'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='Looking for the JS/TS library? Check out LangChain.js.'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '200b8a7d0dd03f66e4f13456566d2b3a', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='To help you ship LangChain apps to production faster, check out LangSmith.\\nLangSmith is a unified developer platform for building, testing, and monitoring LLM applications.\\nFill out this form to speak with our sales team.'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='Quick Install'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='With pip:'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='bash\\npip install langchain'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='With conda:'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'parent_id': '3c98bc06e5ef8caa9446bb6f1d33dde0', 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='bash\\nconda install langchain -c conda-forge'),\n",
       " Document(metadata={'last_modified': '2025-05-30T10:16:46', 'languages': ['eng'], 'filetype': 'text/markdown', 'file_directory': '../../docs', 'filename': 'README.md'}, page_content='🤔 What is LangChain?')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "lc_docs = [Document(page_content=doc.text,\n",
    "                    metadata=doc.metadata.to_dict())\n",
    "              for doc in docs]\n",
    "lc_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8f99c",
   "metadata": {},
   "source": [
    "### CSV Loader\n",
    "\n",
    "A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\n",
    "\n",
    "LangChain implements a CSV Loader that will load CSV files into a sequence of `Document` objects. Each row of the CSV file is converted to one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1cb76e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with some dummy real estate data\n",
    "data = {\n",
    "    'Property_ID': [101, 102, 103, 104, 105],\n",
    "    'Address': ['123 Elm St', '456 Oak St', '789 Pine St', '321 Maple St', '654 Cedar St'],\n",
    "    'City': ['Springfield', 'Rivertown', 'Laketown', 'Hillside', 'Sunnyvale'],\n",
    "    'State': ['CA', 'TX', 'FL', 'NY', 'CO'],\n",
    "    'Zip_Code': [98765, 87654, 76543, 65432, 54321],\n",
    "    'Bedrooms': [3, 2, 4, 3, 5],\n",
    "    'Bathrooms': [2, 1, 3, 2, 4],\n",
    "    'Listing_Price': [500000, 350000, 600000, 475000, 750000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../../docs/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "39c0171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"../../docs/data.csv\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90785a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../../docs/data.csv', 'row': 0}, page_content='Property_ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip_Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 500000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 1}, page_content='Property_ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip_Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nListing_Price: 350000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 2}, page_content='Property_ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip_Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nListing_Price: 600000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 3}, page_content='Property_ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip_Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nListing_Price: 475000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 4}, page_content='Property_ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip_Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nListing_Price: 750000')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55cec04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property_ID: 101\n",
      "Address: 123 Elm St\n",
      "City: Springfield\n",
      "State: CA\n",
      "Zip_Code: 98765\n",
      "Bedrooms: 3\n",
      "Bathrooms: 2\n",
      "Listing_Price: 500000\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c7439",
   "metadata": {},
   "source": [
    "`CSVLoader` will accept a `csv_args` kwarg that supports customization of arguments passed to Python's csv.`DictReader`. See the [`csv` module](https://docs.python.org/3/library/csv.html) documentation for more information of what `csv` args are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fbec52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"../../docs/data.csv\",\n",
    "                   csv_args={\n",
    "                      \"delimiter\": \",\",\n",
    "                      \"quotechar\": '\"',\n",
    "                      \"fieldnames\": [\"Property ID\", \"Address\", \"City\", \"State\",\n",
    "                                     \"Zip Code\", \"Bedrooms\", \"Bathrooms\", \"Price\"],\n",
    "                   },\n",
    "                  )\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89be4b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../../docs/data.csv', 'row': 0}, page_content='Property ID: Property_ID\\nAddress: Address\\nCity: City\\nState: State\\nZip Code: Zip_Code\\nBedrooms: Bedrooms\\nBathrooms: Bathrooms\\nPrice: Listing_Price'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 1}, page_content='Property ID: 101\\nAddress: 123 Elm St\\nCity: Springfield\\nState: CA\\nZip Code: 98765\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 500000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 2}, page_content='Property ID: 102\\nAddress: 456 Oak St\\nCity: Rivertown\\nState: TX\\nZip Code: 87654\\nBedrooms: 2\\nBathrooms: 1\\nPrice: 350000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 3}, page_content='Property ID: 103\\nAddress: 789 Pine St\\nCity: Laketown\\nState: FL\\nZip Code: 76543\\nBedrooms: 4\\nBathrooms: 3\\nPrice: 600000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 4}, page_content='Property ID: 104\\nAddress: 321 Maple St\\nCity: Hillside\\nState: NY\\nZip Code: 65432\\nBedrooms: 3\\nBathrooms: 2\\nPrice: 475000'),\n",
       " Document(metadata={'source': '../../docs/data.csv', 'row': 5}, page_content='Property ID: 105\\nAddress: 654 Cedar St\\nCity: Sunnyvale\\nState: CO\\nZip Code: 54321\\nBedrooms: 5\\nBathrooms: 4\\nPrice: 750000')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f231bee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property ID: Property_ID\n",
      "Address: Address\n",
      "City: City\n",
      "State: State\n",
      "Zip Code: Zip_Code\n",
      "Bedrooms: Bedrooms\n",
      "Bathrooms: Bathrooms\n",
      "Price: Listing_Price\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1b9fd7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property ID: 101\n",
      "Address: 123 Elm St\n",
      "City: Springfield\n",
      "State: CA\n",
      "Zip Code: 98765\n",
      "Bedrooms: 3\n",
      "Bathrooms: 2\n",
      "Price: 500000\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4715032",
   "metadata": {},
   "source": [
    "#### Compare with unstructured.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824bf0b3",
   "metadata": {},
   "source": [
    "Unstructured.io loads the entire CSV as a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6180af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredCSVLoader\n",
    "\n",
    "loader = UnstructuredCSVLoader(\"../../docs/data.csv\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f00fc228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0322a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "\n",
      "\n",
      "Property_ID\n",
      "Address\n",
      "City\n",
      "State\n",
      "Zip_Code\n",
      "Bedrooms\n",
      "Bathrooms\n",
      "Listing_Price\n",
      "\n",
      "\n",
      "101\n",
      "123 Elm St\n",
      "Springfield\n",
      "CA\n",
      "98765\n",
      "3\n",
      "2\n",
      "500000\n",
      "\n",
      "\n",
      "102\n",
      "456 Oak St\n",
      "Rivertown\n",
      "TX\n",
      "87654\n",
      "2\n",
      "1\n",
      "350000\n",
      "\n",
      "\n",
      "103\n",
      "789 Pine St\n",
      "Laketown\n",
      "FL\n",
      "76543\n",
      "4\n",
      "3\n",
      "600000\n",
      "\n",
      "\n",
      "104\n",
      "321 Maple St\n",
      "Hillside\n",
      "NY\n",
      "65432\n",
      "3\n",
      "2\n",
      "475000\n",
      "\n",
      "\n",
      "105\n",
      "654 Cedar St\n",
      "Sunnyvale\n",
      "CO\n",
      "54321\n",
      "5\n",
      "4\n",
      "750000\n",
      "\n",
      "\n",
      "' metadata={'source': '../../docs/data.csv'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "554dc8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Property_ID\n",
      "Address\n",
      "City\n",
      "State\n",
      "Zip_Code\n",
      "Bedrooms\n",
      "Bathrooms\n",
      "Listing_Price\n",
      "\n",
      "\n",
      "101\n",
      "123 Elm St\n",
      "Springfield\n",
      "CA\n",
      "98765\n",
      "3\n",
      "2\n",
      "500000\n",
      "\n",
      "\n",
      "102\n",
      "456 Oak St\n",
      "Rivertown\n",
      "TX\n",
      "87654\n",
      "2\n",
      "1\n",
      "350000\n",
      "\n",
      "\n",
      "103\n",
      "789 Pine St\n",
      "Laketown\n",
      "FL\n",
      "76543\n",
      "4\n",
      "3\n",
      "600000\n",
      "\n",
      "\n",
      "104\n",
      "321 Maple St\n",
      "Hillside\n",
      "NY\n",
      "65432\n",
      "3\n",
      "2\n",
      "475000\n",
      "\n",
      "\n",
      "105\n",
      "654 Cedar St\n",
      "Sunnyvale\n",
      "CO\n",
      "54321\n",
      "5\n",
      "4\n",
      "750000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "55b0e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../../docs/data.csv'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1165501",
   "metadata": {},
   "source": [
    "### JSON Loader\n",
    "\n",
    "[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and arrays (or other serializable values).\n",
    "\n",
    "[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\n",
    "\n",
    "LangChain implements a [JSONLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) to convert JSON and JSONL data into LangChain `Document` objects. It uses a specified [`jq` schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content and metadata of the LangChain Document.\n",
    "\n",
    "It uses the `jq` python package. Check out [this manual](https://jqlang.github.io/jq/manual/) for a detailed documentation of the `jq` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca0e349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample data dictionary similar to the one you provided but with modified contents\n",
    "data = {\n",
    "    'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_meeting.jpg'},\n",
    "    'is_still_participant': True,\n",
    "    'joinable_mode': {'link': '', 'mode': 1},\n",
    "    'magic_words': [],\n",
    "    'messages': [\n",
    "        {'content': 'See you soon!',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675597571851},\n",
    "        {'content': 'Thanks for the update! See you then.',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675597435669},\n",
    "        {'content': 'Actually, the green one is sold out.',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675596277579},\n",
    "        {'content': 'I was hoping to purchase the green one!',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675595140251},\n",
    "        {'content': 'I’m really interested in the green one, not the red!',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675595109305},\n",
    "        {'content': 'Here’s the $150 for it.',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675595068468},\n",
    "        {'photos': [{'creation_timestamp': 1675595059,\n",
    "                     'uri': 'image_of_the_item.jpg'}],\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675595060730},\n",
    "        {'content': 'It typically sells for at least $200 online',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675595045152},\n",
    "        {'content': 'How much are you asking?',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675594799696},\n",
    "        {'content': 'Good morning! $50 is far too low.',\n",
    "         'sender_name': 'User B',\n",
    "         'timestamp_ms': 1675577876645},\n",
    "        {'content': 'Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!',\n",
    "         'sender_name': 'User A',\n",
    "         'timestamp_ms': 1675549022673}\n",
    "    ],\n",
    "    'participants': [{'name': 'User A'}, {'name': 'User B'}],\n",
    "    'thread_path': 'inbox/User A and User B chat',\n",
    "    'title': 'User A and User B chat'\n",
    "}\n",
    "\n",
    "# Save the modified data to a JSON file\n",
    "with open('../../docs/chat_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3dcc47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "loader = JSONLoader(file_path=\"../../docs/chat_data.json\",\n",
    "                    jq_schema='.',\n",
    "                    text_content=False)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a1e7802a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b08895cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}\n",
      "{'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "13f35af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}' metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea7c4571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}, page_content='{\"image\": {\"creation_timestamp\": 1675549016, \"uri\": \"image_of_the_meeting.jpg\"}, \"is_still_participant\": true, \"joinable_mode\": {\"link\": \"\", \"mode\": 1}, \"magic_words\": [], \"messages\": [{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}, {\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}, {\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}, {\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}, {\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}, {\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}, {\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}, {\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}, {\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}, {\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}, {\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}], \"participants\": [{\"name\": \"User A\"}, {\"name\": \"User B\"}], \"thread_path\": \"inbox/User A and User B chat\", \"title\": \"User A and User B chat\"}')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd57df",
   "metadata": {},
   "source": [
    "Suppose we are interested in extracting the values under the `messages` key of the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ae27dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}, page_content='{\"content\": \"See you soon!\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675597571851}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 2}, page_content='{\"content\": \"Thanks for the update! See you then.\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675597435669}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 3}, page_content='{\"content\": \"Actually, the green one is sold out.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675596277579}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 4}, page_content='{\"content\": \"I was hoping to purchase the green one!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595140251}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 5}, page_content='{\"content\": \"I\\\\u2019m really interested in the green one, not the red!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675595109305}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 6}, page_content='{\"content\": \"Here\\\\u2019s the $150 for it.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595068468}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 7}, page_content='{\"photos\": [{\"creation_timestamp\": 1675595059, \"uri\": \"image_of_the_item.jpg\"}], \"sender_name\": \"User B\", \"timestamp_ms\": 1675595060730}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 8}, page_content='{\"content\": \"It typically sells for at least $200 online\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675595045152}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 9}, page_content='{\"content\": \"How much are you asking?\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675594799696}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 10}, page_content='{\"content\": \"Good morning! $50 is far too low.\", \"sender_name\": \"User B\", \"timestamp_ms\": 1675577876645}'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 11}, page_content='{\"content\": \"Hello! I\\\\u2019m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!\", \"sender_name\": \"User A\", \"timestamp_ms\": 1675549022673}')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='../../docs/chat_data.json',\n",
    "    jq_schema='.messages[]',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75500e05",
   "metadata": {},
   "source": [
    "Suppose we are interested in extracting the values under the `content` field within the `messages` key of the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "607fa624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 1}, page_content='See you soon!'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 2}, page_content='Thanks for the update! See you then.'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 3}, page_content='Actually, the green one is sold out.'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 4}, page_content='I was hoping to purchase the green one!'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 5}, page_content='I’m really interested in the green one, not the red!'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 6}, page_content='Here’s the $150 for it.'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 7}, page_content=''),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 8}, page_content='It typically sells for at least $200 online'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 9}, page_content='How much are you asking?'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 10}, page_content='Good morning! $50 is far too low.'),\n",
       " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/chat_data.json', 'seq_num': 11}, page_content='Hello! I’m interested in the item you posted. I can offer $50. Let me know if that works for you. Thanks!')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='../../docs/chat_data.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80113f",
   "metadata": {},
   "source": [
    "#### Basic JSON Loading\n",
    "For robust loading, especially with diverse file types, consider these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4239a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "df920d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
      " 'is_still_participant': True,\n",
      " 'joinable_mode': {'link': '', 'mode': 1},\n",
      " 'magic_words': [],\n",
      " 'messages': [{'content': 'Bye!',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675597571851},\n",
      "              {'content': 'Oh no worries! Bye',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675597435669},\n",
      "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
      "                          'for sale',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675596277579},\n",
      "              {'content': 'I thought you were selling the blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595140251},\n",
      "              {'content': 'Im not interested in this bag. Im interested in the '\n",
      "                          'blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595109305},\n",
      "              {'content': 'Here is $129',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595068468},\n",
      "              {'content': '',\n",
      "               'photos': [{'creation_timestamp': 1675595059,\n",
      "                           'uri': 'url_of_some_picture.jpg'}],\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595060730},\n",
      "              {'content': 'Online is at least $100',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595045152},\n",
      "              {'content': 'How much do you want?',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675594799696},\n",
      "              {'content': 'Goodmorning! $50 is too low.',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675577876645},\n",
      "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
      "                          'me know if you are interested. Thanks!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675549022673}],\n",
      " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
      " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
      " 'title': 'User 1 and User 2 chat'}\n"
     ]
    }
   ],
   "source": [
    "file_path = '../../docs/facebook_chat.json'\n",
    "with open(file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9bab5",
   "metadata": {},
   "source": [
    "#### Using JSONLoader for Structured Retrieval: \n",
    "Use jq_schema to specify the data structure and extract only the required fields (Schema-Based Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "41119648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 1}, page_content='Bye!'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 2}, page_content='Oh no worries! Bye'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 3}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 4}, page_content='I thought you were selling the blue one!'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 5}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 6}, page_content='Here is $129'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 7}, page_content=''),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 8}, page_content='Online is at least $100'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 9}, page_content='How much do you want?'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 10}, page_content='Goodmorning! $50 is too low.'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 11}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]\n"
     ]
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path='../../docs/facebook_chat.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cb764",
   "metadata": {},
   "source": [
    "#### Processing JSON Lines (JSONL): \n",
    "Seamlessly handle files where each line represents a separate JSON object by setting json_lines=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "90a6f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 1}, page_content='{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 2}, page_content='{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no worries! Bye\"}'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 3}, page_content='{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im sorry it was my mistake, the blue one is not for sale\"}')]\n"
     ]
    }
   ],
   "source": [
    "# Example - JSON (Processing JSON Lines)\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='../../docs/facebook_chat_messages.jsonl',\n",
    "    jq_schema=\".\",\n",
    "    text_content=False,\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e32a9483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 1}, page_content='User 2'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 2}, page_content='User 1'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 3}, page_content='User 2')]\n"
     ]
    }
   ],
   "source": [
    "# Example - JSON (Processing JSON Lines)\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='../../docs/facebook_chat_messages.jsonl',\n",
    "    jq_schema='.sender_name',\n",
    "    text_content=False,\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7945f15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 1}, page_content='User 2'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 2}, page_content='User 1'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat_messages.jsonl', 'seq_num': 3}, page_content='User 2')]\n"
     ]
    }
   ],
   "source": [
    "# Example - JSON (Use jq_schema='.' and content_key for simpler extraction)\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='../../docs/facebook_chat_messages.jsonl',\n",
    "    jq_schema='.',\n",
    "    content_key=\"sender_name\",\n",
    "    text_content=False,\n",
    "    json_lines=True\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4aafe0",
   "metadata": {},
   "source": [
    "#### Adding Metadata from JSON: \n",
    "Use custom functions to extract additional metadata, enhancing data context and traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2b08513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}, page_content='Bye!'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}, page_content='Oh no worries! Bye'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}, page_content='I thought you were selling the blue one!'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}, page_content='Here is $129'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}, page_content=''),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}, page_content='Online is at least $100'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}, page_content='How much do you want?'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}, page_content='Goodmorning! $50 is too low.'),\n",
      " Document(metadata={'source': '/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/RAG/docs/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]\n"
     ]
    }
   ],
   "source": [
    "# Example - JSON (Adding Metadata from JSON)\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"sender_name\"] = record.get(\"sender_name\")\n",
    "    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='../../docs/facebook_chat.json',\n",
    "    jq_schema='.messages[]',\n",
    "    content_key=\"content\",\n",
    "    metadata_func=metadata_func # Add metadata from JSON\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4c591",
   "metadata": {},
   "source": [
    "### PDF Loaders\n",
    "\n",
    "[Portable Document Format (PDF)](https://en.wikipedia.org/wiki/PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\n",
    "\n",
    "LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your use-case and through experimentation.\n",
    "\n",
    "Here we will see how to load PDF documents into the LangChain `Document` format\n",
    "\n",
    "We download a research paper to experiment with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cebd23",
   "metadata": {},
   "source": [
    "If the following command fails you can download the paper manually by going to http://arxiv.org/pdf/2103.15348.pdf, save it as `layoutparser_paper.pdf`and upload it on the left in Colab from the upload files option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0f429",
   "metadata": {},
   "source": [
    "#### PyPDFLoader\n",
    "\n",
    "Here we load a PDF using `pypdf` into list of documents, where each document contains the page content and metadata with page number. Typically each PDF page becomes one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e097e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser : A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "Zejiang Shen1( \u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n",
      "1Allen Institute for AI\n",
      "shannons@allenai.org\n",
      "2Brown University\n",
      "ruochen zhang@brown.edu\n",
      "3Harvard University\n",
      "{melissadell,jacob carlson }@fas.harvard.edu\n",
      "4University of Washington\n",
      "bcgl@cs.washington.edu\n",
      "5University of Waterloo\n",
      "w422li@uwaterloo.ca\n",
      "Abstract. Recent advances in document image analysis (DIA) have been\n",
      "primarily driven by the application of neural networks. Ideally, research\n",
      "outcomes could be easily deployed in production and extended for further\n",
      "investigation. However, various factors like loosely organized codebases\n",
      "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
      "portant innovations by a wide audience. Though there have been on-going\n",
      "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
      "development in disciplines like natural language processing and computer\n",
      "vision, none of them are optimized for challenges in the domain of DIA.\n",
      "This represents a major gap in the existing toolkit, as DIA is central to\n",
      "academic research across a wide range of disciplines in the social sciences\n",
      "and humanities. This paper introduces LayoutParser , an open-source\n",
      "library for streamlining the usage of DL in DIA research and applica-\n",
      "tions. The core LayoutParser library comes with a set of simple and\n",
      "intuitive interfaces for applying and customizing DL models for layout de-\n",
      "tection, character recognition, and many other document processing tasks.\n",
      "To promote extensibility, LayoutParser also incorporates a community\n",
      "platform for sharing both pre-trained models and full document digiti-\n",
      "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
      "lightweight and large-scale digitization pipelines in real-word use cases.\n",
      "The library is publicly available at https://layout-parser.github.io .\n",
      "Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis\n",
      "·Character Recognition ·Open Source library ·Toolkit.\n",
      "1 Introduction\n",
      "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
      "document image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n",
      "{'source': '../../docs/layoutparser_paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../../docs/layoutparser_paper.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(pages[0].page_content)\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3a839710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2413852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'source': '../../docs/layoutparser_paper.pdf', 'page': 0}, page_content='LayoutParser : A Uniﬁed Toolkit for Deep\\nLearning Based Document Image Analysis\\nZejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n1Allen Institute for AI\\nshannons@allenai.org\\n2Brown University\\nruochen zhang@brown.edu\\n3Harvard University\\n{melissadell,jacob carlson }@fas.harvard.edu\\n4University of Washington\\nbcgl@cs.washington.edu\\n5University of Waterloo\\nw422li@uwaterloo.ca\\nAbstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model conﬁgurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\neﬀorts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser , an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io .\\nKeywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n·Character Recognition ·Open Source library ·Toolkit.\\n1 Introduction\\nDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classiﬁcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')\n"
     ]
    }
   ],
   "source": [
    "pprint(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5d0eb2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LayoutParser : A Uniﬁed Toolkit for Deep\\n'\n",
      " 'Learning Based Document Image Analysis\\n'\n",
      " 'Zejiang Shen1( \\x00), Ruochen Zhang2, Melissa Dell3, Benjamin Charles '\n",
      " 'Germain\\n'\n",
      " 'Lee4, Jacob Carlson3, and Weining Li5\\n'\n",
      " '1Allen Institute for AI\\n'\n",
      " 'shannons@allenai.org\\n'\n",
      " '2Brown University\\n'\n",
      " 'ruochen zhang@brown.edu\\n'\n",
      " '3Harvard University\\n'\n",
      " '{melissadell,jacob carlson }@fas.harvard.edu\\n'\n",
      " '4University of Washington\\n'\n",
      " 'bcgl@cs.washington.edu\\n'\n",
      " '5University of Waterloo\\n'\n",
      " 'w422li@uwaterloo.ca\\n'\n",
      " 'Abstract. Recent advances in document image analysis (DIA) have been\\n'\n",
      " 'primarily driven by the application of neural networks. Ideally, research\\n'\n",
      " 'outcomes could be easily deployed in production and extended for further\\n'\n",
      " 'investigation. However, various factors like loosely organized codebases\\n'\n",
      " 'and sophisticated model conﬁgurations complicate the easy reuse of im-\\n'\n",
      " 'portant innovations by a wide audience. Though there have been on-going\\n'\n",
      " 'eﬀorts to improve reusability and simplify deep learning (DL) model\\n'\n",
      " 'development in disciplines like natural language processing and computer\\n'\n",
      " 'vision, none of them are optimized for challenges in the domain of DIA.\\n'\n",
      " 'This represents a major gap in the existing toolkit, as DIA is central to\\n'\n",
      " 'academic research across a wide range of disciplines in the social sciences\\n'\n",
      " 'and humanities. This paper introduces LayoutParser , an open-source\\n'\n",
      " 'library for streamlining the usage of DL in DIA research and applica-\\n'\n",
      " 'tions. The core LayoutParser library comes with a set of simple and\\n'\n",
      " 'intuitive interfaces for applying and customizing DL models for layout de-\\n'\n",
      " 'tection, character recognition, and many other document processing tasks.\\n'\n",
      " 'To promote extensibility, LayoutParser also incorporates a community\\n'\n",
      " 'platform for sharing both pre-trained models and full document digiti-\\n'\n",
      " 'zation pipelines. We demonstrate that LayoutParser is helpful for both\\n'\n",
      " 'lightweight and large-scale digitization pipelines in real-word use cases.\\n'\n",
      " 'The library is publicly available at https://layout-parser.github.io .\\n'\n",
      " 'Keywords: Document Image Analysis ·Deep Learning ·Layout Analysis\\n'\n",
      " '·Character Recognition ·Open Source library ·Toolkit.\\n'\n",
      " '1 Introduction\\n'\n",
      " 'Deep Learning(DL)-based approaches are the state-of-the-art for a wide range '\n",
      " 'of\\n'\n",
      " 'document image analysis (DIA) tasks including document image classiﬁcation [ '\n",
      " '11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021')\n"
     ]
    }
   ],
   "source": [
    "pprint(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "46cf164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../../docs/layoutparser_paper.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735c0d7",
   "metadata": {},
   "source": [
    "#### PyMuPDFLoader\n",
    "\n",
    "This is the fastest of the PDF parsing options, and contains detailed metadata about the PDF and its pages, as well as returns one document per page. It uses the `pymupdf` library internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a50e8118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (1.24.5)\n",
      "Collecting pymupdf\n",
      "  Using cached pymupdf-1.26.3-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Using cached pymupdf-1.26.3-cp39-abi3-macosx_11_0_arm64.whl (22.4 MB)\n",
      "Installing collected packages: pymupdf\n",
      "  Attempting uninstall: pymupdf\n",
      "    Found existing installation: PyMuPDF 1.24.5\n",
      "    Uninstalling PyMuPDF-1.24.5:\n",
      "      Successfully uninstalled PyMuPDF-1.24.5\n",
      "Successfully installed pymupdf-1.26.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75f33181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n",
      "1 Allen Institute for AI\n",
      "shannons@allenai.org\n",
      "2 Brown University\n",
      "ruochen zhang@brown.edu\n",
      "3 Harvard University\n",
      "{melissadell,jacob carlson}@fas.harvard.edu\n",
      "4 University of Washington\n",
      "bcgl@cs.washington.edu\n",
      "5 University of Waterloo\n",
      "w422li@uwaterloo.ca\n",
      "Abstract. Recent advances in document image analysis (DIA) have been\n",
      "primarily driven by the application of neural networks. Ideally, research\n",
      "outcomes could be easily deployed in production and extended for further\n",
      "investigation. However, various factors like loosely organized codebases\n",
      "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
      "portant innovations by a wide audience. Though there have been on-going\n",
      "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
      "development in disciplines like natural language processing and computer\n",
      "vision, none of them are optimized for challenges in the domain of DIA.\n",
      "This represents a major gap in the existing toolkit, as DIA is central to\n",
      "academic research across a wide range of disciplines in the social sciences\n",
      "and humanities. This paper introduces LayoutParser, an open-source\n",
      "library for streamlining the usage of DL in DIA research and applica-\n",
      "tions. The core LayoutParser library comes with a set of simple and\n",
      "intuitive interfaces for applying and customizing DL models for layout de-\n",
      "tection, character recognition, and many other document processing tasks.\n",
      "To promote extensibility, LayoutParser also incorporates a community\n",
      "platform for sharing both pre-trained models and full document digiti-\n",
      "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
      "lightweight and large-scale digitization pipelines in real-word use cases.\n",
      "The library is publicly available at https://layout-parser.github.io.\n",
      "Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n",
      "· Character Recognition · Open Source library · Toolkit.\n",
      "1\n",
      "Introduction\n",
      "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
      "document image analysis (DIA) tasks including document image classiﬁcation [11,\n",
      "arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n",
      "\n",
      "{'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"../../docs/layoutparser_paper.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "print(pages[0].page_content)\n",
    "print(pages[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c4ae5867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "42b3b1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LayoutParser: A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n",
      "1 Allen Institute for AI\n",
      "shannons@allenai.org\n",
      "2 Brown University\n",
      "ruochen zhang@brown.edu\n",
      "3 Harvard University\n",
      "{melissadell,jacob carlson}@fas.harvard.edu\n",
      "4 University of Washington\n",
      "bcgl@cs.washington.edu\n",
      "5 University of Waterloo\n",
      "w422li@uwaterloo.ca\n",
      "Abstract. Recent advances in document image analysis (DIA) have been\n",
      "primarily driven by the application of neural networks. Ideally, research\n",
      "outcomes could be easily deployed in production and extended for further\n",
      "investigation. However, various factors like loosely organized codebases\n",
      "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
      "portant innovations by a wide audience. Though there have been on-going\n",
      "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
      "development in disciplines like natural language processing and computer\n",
      "vision, none of them are optimized for challenges in the domain of DIA.\n",
      "This represents a major gap in the existing toolkit, as DIA is central to\n",
      "academic research across a wide range of disciplines in the social sciences\n",
      "and humanities. This paper introduces LayoutParser, an open-source\n",
      "library for streamlining the usage of DL in DIA research and applica-\n",
      "tions. The core LayoutParser library comes with a set of simple and\n",
      "intuitive interfaces for applying and customizing DL models for layout de-\n",
      "tection, character recognition, and many other document processing tasks.\n",
      "To promote extensibility, LayoutParser also incorporates a community\n",
      "platform for sharing both pre-trained models and full document digiti-\n",
      "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
      "lightweight and large-scale digitization pipelines in real-word use cases.\n",
      "The library is publicly available at https://layout-parser.github.io.\n",
      "Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n",
      "· Character Recognition · Open Source library · Toolkit.\n",
      "1\n",
      "Introduction\n",
      "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
      "document image analysis (DIA) tasks including document image classiﬁcation [11,\n",
      "arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n",
      "' metadata={'source': '../../docs/layoutparser_paper.pdf', 'file_path': '../../docs/layoutparser_paper.pdf', 'page': 0, 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210622012710Z', 'modDate': 'D:20210622012710Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c5c61075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../../docs/layoutparser_paper.pdf',\n",
       " 'file_path': '../../docs/layoutparser_paper.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 16,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'pdfTeX-1.40.21',\n",
       " 'creationDate': 'D:20210622012710Z',\n",
       " 'modDate': 'D:20210622012710Z',\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "46b15712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for Deep\n",
      "Learning Based Document Image Analysis\n",
      "Zejiang Shen1 (\u0000), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\n",
      "Lee4, Jacob Carlson3, and Weining Li5\n",
      "1 Allen Institute for AI\n",
      "shannons@allenai.org\n",
      "2 Brown University\n",
      "ruochen zhang@brown.edu\n",
      "3 Harvard University\n",
      "{melissadell,jacob carlson}@fas.harvard.edu\n",
      "4 University of Washington\n",
      "bcgl@cs.washington.edu\n",
      "5 University of Waterloo\n",
      "w422li@uwaterloo.ca\n",
      "Abstract. Recent advances in document image analysis (DIA) have been\n",
      "primarily driven by the application of neural networks. Ideally, research\n",
      "outcomes could be easily deployed in production and extended for further\n",
      "investigation. However, various factors like loosely organized codebases\n",
      "and sophisticated model conﬁgurations complicate the easy reuse of im-\n",
      "portant innovations by a wide audience. Though there have been on-going\n",
      "eﬀorts to improve reusability and simplify deep learning (DL) model\n",
      "development in disciplines like natural language processing and computer\n",
      "vision, none of them are optimized for challenges in the domain of DIA.\n",
      "This represents a major gap in the existing toolkit, as DIA is central to\n",
      "academic research across a wide range of disciplines in the social sciences\n",
      "and humanities. This paper introduces LayoutParser, an open-source\n",
      "library for streamlining the usage of DL in DIA research and applica-\n",
      "tions. The core LayoutParser library comes with a set of simple and\n",
      "intuitive interfaces for applying and customizing DL models for layout de-\n",
      "tection, character recognition, and many other document processing tasks.\n",
      "To promote extensibility, LayoutParser also incorporates a community\n",
      "platform for sharing both pre-trained models and full document digiti-\n",
      "zation pipelines. We demonstrate that LayoutParser is helpful for both\n",
      "lightweight and large-scale digitization pipelines in real-word use cases.\n",
      "The library is publicly available at https://layout-parser.github.io.\n",
      "Keywords: Document Image Analysis · Deep Learning · Layout Analysis\n",
      "· Character Recognition · Open Source library · Toolkit.\n",
      "1\n",
      "Introduction\n",
      "Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\n",
      "document image analysis (DIA) tasks including document image classiﬁcation [11,\n",
      "arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "edeacef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser: A Uniﬁed Toolkit for DL-Based DIA\n",
      "5\n",
      "Table 1: Current layout detection models in the LayoutParser model zoo\n",
      "Dataset\n",
      "Base Model1 Large Model\n",
      "Notes\n",
      "PubLayNet [38]\n",
      "F / M\n",
      "M\n",
      "Layouts of modern scientiﬁc documents\n",
      "PRImA [3]\n",
      "M\n",
      "-\n",
      "Layouts of scanned modern magazines and scientiﬁc reports\n",
      "Newspaper [17]\n",
      "F\n",
      "-\n",
      "Layouts of scanned US newspapers from the 20th century\n",
      "TableBank [18]\n",
      "F\n",
      "F\n",
      "Table region on modern scientiﬁc and business document\n",
      "HJDataset [31]\n",
      "F / M\n",
      "-\n",
      "Layouts of history Japanese documents\n",
      "1 For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀbetween accuracy\n",
      "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
      "backbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\n",
      "R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
      "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
      "zoo in coming months.\n",
      "layout data structures, which are optimized for eﬃciency and versatility. 3) When\n",
      "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
      "API provided in the OCR module. 4) LayoutParser comes with a set of utility\n",
      "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
      "is also highly customizable, via its integration with functions for layout data\n",
      "annotation and model training. We now provide detailed descriptions for each\n",
      "component.\n",
      "3.1\n",
      "Layout Detection Models\n",
      "In LayoutParser, a layout model takes a document image as an input and\n",
      "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
      "from traditional methods, it relies on deep convolutional neural networks rather\n",
      "than manually curated rules to identify content regions. It is formulated as an\n",
      "object detection problem and state-of-the-art models like Faster R-CNN [28] and\n",
      "Mask R-CNN [12] are used. This yields prediction results of high accuracy and\n",
      "makes it possible to build a concise, generalized interface for layout detection.\n",
      "LayoutParser, built upon Detectron2 [35], provides a minimal API that can\n",
      "perform layout detection with only four lines of code in Python:\n",
      "1 import\n",
      "layoutparser as lp\n",
      "2 image = cv2.imread(\"image_file\") # load\n",
      "images\n",
      "3 model = lp. Detectron2LayoutModel (\n",
      "4\n",
      "\"lp:// PubLayNet/ faster_rcnn_R_50_FPN_3x /config\")\n",
      "5 layout = model.detect(image)\n",
      "LayoutParser provides a wealth of pre-trained model weights using various\n",
      "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
      "domain shift [7], the prediction performance can notably drop when models are ap-\n",
      "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
      "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
      "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
      "is used for initializing the model weights in LayoutParser, using both the dataset\n",
      "name and model name lp://<dataset-name>/<model-architecture-name>.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pages[4].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5c841",
   "metadata": {},
   "source": [
    "#### UnstructuredPDFLoader\n",
    "\n",
    "[Unstructured.io](https://unstructured-io.github.io/unstructured/) supports a common interface for working with unstructured or semi-structured file formats, such as Markdown or PDF. LangChain's [`UnstructuredPDFLoader`](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html) integrates with Unstructured to parse PDF documents into LangChain [`Document`](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6110a839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured==0.10.25\n",
      "  Downloading unstructured-0.10.25-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pdfminer.six==20221105\n",
      "  Using cached pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: chardet in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (5.2.0)\n",
      "Requirement already satisfied: filetype in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (0.4.27)\n",
      "Requirement already satisfied: lxml in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (6.0.0)\n",
      "Requirement already satisfied: nltk in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (3.9.1)\n",
      "Requirement already satisfied: tabulate in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (0.9.0)\n",
      "Requirement already satisfied: requests in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (4.13.4)\n",
      "Requirement already satisfied: emoji in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (1.0.9)\n",
      "Requirement already satisfied: numpy in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (2.2.6)\n",
      "Requirement already satisfied: rapidfuzz in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (3.13.0)\n",
      "Requirement already satisfied: backoff in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from unstructured==0.10.25) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from pdfminer.six==20221105) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from pdfminer.six==20221105) (45.0.5)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20221105) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from beautifulsoup4->unstructured==0.10.25) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from beautifulsoup4->unstructured==0.10.25) (4.14.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from dataclasses-json->unstructured==0.10.25) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from dataclasses-json->unstructured==0.10.25) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured==0.10.25) (24.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured==0.10.25) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from langdetect->unstructured==0.10.25) (1.17.0)\n",
      "Requirement already satisfied: click in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from nltk->unstructured==0.10.25) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from nltk->unstructured==0.10.25) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from nltk->unstructured==0.10.25) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from nltk->unstructured==0.10.25) (4.67.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from requests->unstructured==0.10.25) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from requests->unstructured==0.10.25) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages (from requests->unstructured==0.10.25) (2025.6.15)\n",
      "Downloading unstructured-0.10.25-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "Installing collected packages: unstructured, pdfminer.six\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pdfminer.six][0m [pdfminer.six]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pdfplumber 0.11.7 requires pdfminer.six==20250506, but you have pdfminer-six 20221105 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pdfminer.six-20221105 unstructured-0.10.25\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"unstructured==0.10.25\" \"pdfminer.six==20221105\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c29afd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Advanced UnstructuredPDFLoader with comprehensive error handling\n",
    "# import importlib\n",
    "# import sys\n",
    "\n",
    "# def test_unstructured_pdf_loader():\n",
    "#     \"\"\"Test UnstructuredPDFLoader with comprehensive error handling and fallbacks.\"\"\"\n",
    "#     try:\n",
    "#         # Check if pdfminer.six is properly installed\n",
    "#         try:\n",
    "#             import pdfminer.six\n",
    "#             print(\"✅ pdfminer.six is available\")\n",
    "#         except ImportError as e:\n",
    "#             print(f\"❌ pdfminer.six import error: {e}\")\n",
    "#             return False\n",
    "            \n",
    "#         # Check if unstructured can be imported\n",
    "#         try:\n",
    "#             from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "#             print(\"✅ UnstructuredPDFLoader imported successfully\")\n",
    "#         except ImportError as e:\n",
    "#             print(f\"❌ UnstructuredPDFLoader import error: {e}\")\n",
    "#             return False\n",
    "        \n",
    "#         # Try to load the PDF\n",
    "#         print(\"🔄 Attempting to load PDF with UnstructuredPDFLoader...\")\n",
    "#         loader = UnstructuredPDFLoader('../../docs/layoutparser_paper.pdf')\n",
    "#         data = loader.load()\n",
    "        \n",
    "#         print(f\"✅ Successfully loaded PDF with {len(data)} document(s)\")\n",
    "#         print(f\"📄 First document content preview: {data[0].page_content[:200]}...\")\n",
    "#         print(f\"📊 First document metadata: {data[0].metadata}\")\n",
    "#         return True\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error loading PDF with UnstructuredPDFLoader: {e}\")\n",
    "#         print(f\"🔍 Error type: {type(e).__name__}\")\n",
    "        \n",
    "#         # Provide specific solutions based on error type\n",
    "#         error_str = str(e).lower()\n",
    "#         if \"psparser\" in error_str or \"pdfminer\" in error_str:\n",
    "#             print(\"\\n🔧 This is a pdfminer dependency conflict. Solutions:\")\n",
    "#             print(\"1. Restart your kernel after running the previous cell\")\n",
    "#             print(\"2. Clear Python import cache\")\n",
    "#             print(\"3. Use alternative PDF loaders (PyPDFLoader, PyMuPDFLoader)\")\n",
    "#         elif \"ssl\" in error_str or \"certificate\" in error_str:\n",
    "#             print(\"\\n🔧 This is an SSL/certificate issue. Try the NLTK SSL fix above.\")\n",
    "#         elif \"permission\" in error_str:\n",
    "#             print(\"\\n🔧 This is a file permission issue. Check file accessibility.\")\n",
    "        \n",
    "#         return False\n",
    "\n",
    "# def use_alternative_pdf_loader():\n",
    "#     \"\"\"Fallback to reliable PDF loaders if UnstructuredPDFLoader fails.\"\"\"\n",
    "#     print(\"\\n🔄 Using reliable alternative: PyPDFLoader\")\n",
    "#     try:\n",
    "#         from langchain_community.document_loaders import PyPDFLoader\n",
    "#         loader = PyPDFLoader(\"../../docs/layoutparser_paper.pdf\")\n",
    "#         pages = loader.load()\n",
    "#         print(f\"✅ Successfully loaded {len(pages)} pages with PyPDFLoader\")\n",
    "#         print(f\"📄 First page preview: {pages[0].page_content[:200]}...\")\n",
    "#         return pages\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ PyPDFLoader also failed: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Try UnstructuredPDFLoader first\n",
    "# success = test_unstructured_pdf_loader()\n",
    "\n",
    "# # If it fails, use alternative\n",
    "# if not success:\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"🔄 FALLBACK: Using alternative PDF loader...\")\n",
    "#     print(\"=\"*60)\n",
    "#     alternative_data = use_alternative_pdf_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa8ca4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 🔧 Troubleshooting PDF Loading Issues\n",
    "\n",
    "If you encounter import errors with `UnstructuredPDFLoader`, here are the most common solutions:\n",
    "\n",
    "#### **Root Cause:**\n",
    "The errors `cannot import name 'PSSyntaxError'` or `cannot import name 'psparser'` occur because:\n",
    "- Conflicting `pdfminer` packages (old vs new versions)\n",
    "- Python import cache holding onto old module references\n",
    "- Incomplete package installations\n",
    "\n",
    "#### **Complete Fix Process:**\n",
    "\n",
    "1. **First, run the dependency cleanup cell above** ☝️\n",
    "2. **Restart your Jupyter kernel** (Important!)\n",
    "   - In Jupyter: `Kernel` → `Restart`\n",
    "   - In VS Code: `Ctrl+Shift+P` → `Python: Restart Extension`\n",
    "3. **Re-run the UnstructuredPDFLoader test cell**\n",
    "\n",
    "#### **If Still Failing - Manual Environment Reset:**\n",
    "\n",
    "```bash\n",
    "# Complete environment cleanup (run in terminal)\n",
    "pip uninstall -y pdfminer pdfminer.six pdfminer3k pycryptodome cryptography unstructured\n",
    "pip cache purge\n",
    "pip install pdfminer.six==20231228 cryptography>=3.1\n",
    "pip install unstructured[pdf]==0.14.0\n",
    "```\n",
    "\n",
    "#### **Alternative PDF Loaders (Reliable Options):**\n",
    "\n",
    "If `UnstructuredPDFLoader` continues to fail, use these proven alternatives:\n",
    "\n",
    "```python\n",
    "# Option 1: PyPDFLoader (Fast, Simple)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('../../docs/layoutparser_paper.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "# Option 2: PyMuPDFLoader (Fastest, Rich Metadata)\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader('../../docs/layoutparser_paper.pdf')\n",
    "pages = loader.load()\n",
    "\n",
    "# Option 3: PDFPlumberLoader (Great for Tables)\n",
    "# pip install pdfplumber\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "loader = PDFPlumberLoader('../../docs/layoutparser_paper.pdf')\n",
    "pages = loader.load()\n",
    "```\n",
    "\n",
    "#### **Why These Errors Happen:**\n",
    "- `unstructured` library has complex dependencies\n",
    "- Different Python environments may have conflicting packages\n",
    "- Import caching can hold onto old references even after package updates\n",
    "- The solution is to clean everything and restart fresh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93958a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 🔄 Advanced Fix: Clear Python Import Cache (Alternative to Kernel Restart)\n",
    "# # Run this cell if you can't restart your kernel but still have import issues\n",
    "\n",
    "# import sys\n",
    "# import importlib\n",
    "\n",
    "# def clear_import_cache():\n",
    "#     \"\"\"Clear Python import cache for pdfminer-related modules.\"\"\"\n",
    "#     modules_to_clear = []\n",
    "    \n",
    "#     # Find all pdfminer-related modules in sys.modules\n",
    "#     for module_name in list(sys.modules.keys()):\n",
    "#         if any(pkg in module_name.lower() for pkg in ['pdfminer', 'unstructured']):\n",
    "#             modules_to_clear.append(module_name)\n",
    "    \n",
    "#     # Remove them from cache\n",
    "#     for module_name in modules_to_clear:\n",
    "#         if module_name in sys.modules:\n",
    "#             del sys.modules[module_name]\n",
    "#             print(f\"🗑️  Cleared cache for: {module_name}\")\n",
    "    \n",
    "#     # Clear import caches\n",
    "#     importlib.invalidate_caches()\n",
    "#     print(f\"✅ Cleared {len(modules_to_clear)} cached modules\")\n",
    "#     print(\"🔄 Now try importing UnstructuredPDFLoader again\")\n",
    "\n",
    "# print(\"🧹 Clearing Python import cache...\")\n",
    "# clear_import_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18d70eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'add_chunking_strategy' from 'unstructured.chunking.title' (/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured/chunking/title.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredPDFLoader\n\u001b[32m      3\u001b[39m loader = UnstructuredPDFLoader(\u001b[33m'\u001b[39m\u001b[33m../../docs/layoutparser_paper.pdf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(data[\u001b[32m0\u001b[39m].page_content)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(data[\u001b[32m0\u001b[39m].metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.lazy_load())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:72\u001b[39m, in \u001b[36mUnstructuredPDFLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_pdf(filename=\u001b[38;5;28mself\u001b[39m.file_path, **\u001b[38;5;28mself\u001b[39m.unstructured_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured/partition/pdf.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdftypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PDFObjRef\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m open_filename\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunking\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtitle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_chunking_strategy\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcleaners\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     clean_extra_whitespace_with_index_run,\n\u001b[32m     29\u001b[39m     index_adjustment_after_clean_extra_whitespace,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoordinates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PixelSpace, PointSpace\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'add_chunking_strategy' from 'unstructured.chunking.title' (/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured/chunking/title.py)"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader('../../docs/layoutparser_paper.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0].page_content)\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5628d",
   "metadata": {},
   "source": [
    "Load PDF with complex parsing, table detection and chunking by sections\n",
    "\n",
    "Refer to https://community.databricks.com/t5/data-engineering/trying-to-use-pdf2image-on-databricks/td-p/12914\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70918327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install poppler on the cluster (should be done by init scripts)\n",
    "def install_ocr_on_nodes():\n",
    "    \"\"\"\n",
    "    install poppler on the cluster (should be done by init scripts)\n",
    "    \"\"\"\n",
    "    # from pyspark.sql import SparkSession\n",
    "    import subprocess\n",
    "    num_workers = max(1,int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkers\")))\n",
    "    command = \"sudo rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/* && sudo apt-get clean && sudo apt-get update && sudo apt-get install poppler-utils tesseract-ocr -y\" \n",
    "    def run_subprocess(command):\n",
    "        try:\n",
    "            output = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True)\n",
    "            return output.decode()\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise Exception(\"An error occurred installing OCR libs:\"+ e.output.decode())\n",
    "    #install on the driver\n",
    "    run_subprocess(command)\n",
    "    def run_command(iterator):\n",
    "        for x in iterator:\n",
    "            yield run_subprocess(command)\n",
    "    # spark = SparkSession.builder.getOrCreate()\n",
    "    data = spark.sparkContext.parallelize(range(num_workers), num_workers) \n",
    "    # Use mapPartitions to run command in each partition (worker)\n",
    "    output = data.mapPartitions(run_command)\n",
    "    try:\n",
    "        output.collect();\n",
    "        print(\"OCR libraries installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't install on all node: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df7f0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sourav.banerjee/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# takes 3-4 mins on Colab\u001b[39;00m\n\u001b[32m      2\u001b[39m loader = UnstructuredPDFLoader(\u001b[33m'\u001b[39m\u001b[33m../../docs/layoutparser_paper.pdf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                                strategy=\u001b[33m'\u001b[39m\u001b[33mhi_res\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m                                extract_images_in_pdf=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m                                combine_text_under_n_chars=\u001b[32m2000\u001b[39m, \u001b[38;5;66;03m# smaller chunks < 2000 chars will be combined into a larger chunk\u001b[39;00m\n\u001b[32m     10\u001b[39m                                mode=\u001b[33m'\u001b[39m\u001b[33melements\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m data = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.lazy_load())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:107\u001b[39m, in \u001b[36mUnstructuredBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Document]:\n\u001b[32m    106\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     elements = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mself\u001b[39m._post_process_elements(elements)\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py:72\u001b[39m, in \u001b[36mUnstructuredPDFLoader._get_elements\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_pdf(filename=\u001b[38;5;28mself\u001b[39m.file_path, **\u001b[38;5;28mself\u001b[39m.unstructured_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured/partition/pdf.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m PILImage\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpypdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocumentLayout\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayoutelement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutElement\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_chunking_strategy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured_inference/inference/layout.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mordering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m order_layout\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchipper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredChipperModel\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munstructuredmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     UnstructuredElementExtractionModel,\n\u001b[32m     24\u001b[39m     UnstructuredObjectDetectionModel,\n\u001b[32m     25\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured_inference/models/base.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Tuple, Type\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchipper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPES \u001b[38;5;28;01mas\u001b[39;00m CHIPPER_MODEL_TYPES\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchipper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UnstructuredChipperModel\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetectron2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MODEL_TYPES \u001b[38;5;28;01mas\u001b[39;00m DETECTRON2_MODEL_TYPES\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured_inference/models/chipper.py:24\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munstructuredmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     UnstructuredElementExtractionModel,\n\u001b[32m     23\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyDict, download_if_needed_and_get_local_path, strip_tags\n\u001b[32m     26\u001b[39m MODEL_TYPES: Dict[\u001b[38;5;28mstr\u001b[39m, Union[LazyDict, \u001b[38;5;28mdict\u001b[39m]] = {\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mchipperv1\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpre_trained_model_repo\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33munstructuredio/ved-fine-tuning\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     },\n\u001b[32m     59\u001b[39m }\n\u001b[32m     61\u001b[39m MODEL_TYPES[\u001b[33m\"\u001b[39m\u001b[33mchipper\u001b[39m\u001b[33m\"\u001b[39m] = MODEL_TYPES[\u001b[33m\"\u001b[39m\u001b[33mchipperv3\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured_inference/utils.py:14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnnotationResult\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayoutelement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutElement\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_plot\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured_inference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocumentLayout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/My Codebases/GenerativAI_Demystified/.venv/lib/python3.11/site-packages/unstructured_inference/visualize.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageFont\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# takes 3-4 mins on Colab\n",
    "loader = UnstructuredPDFLoader('../../docs/layoutparser_paper.pdf',\n",
    "                               strategy='hi_res',\n",
    "                               extract_images_in_pdf=False,\n",
    "                               infer_table_structure=True,\n",
    "                               chunking_strategy=\"by_title\",\n",
    "                               max_characters=4000, # max size of chunks\n",
    "                               new_after_n_chars=3800, # preferred size of chunks\n",
    "                               combine_text_under_n_chars=2000, # smaller chunks < 2000 chars will be combined into a larger chunk\n",
    "                               mode='elements')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
