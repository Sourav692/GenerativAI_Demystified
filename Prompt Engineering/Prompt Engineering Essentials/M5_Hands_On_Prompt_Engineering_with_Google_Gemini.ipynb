{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7eaf42-5e66-4def-9d2a-f4c3c472c96a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fb6rdwlCsCGt"
   },
   "source": [
    "# Using Gemini 1.5 Flash with Python for diverse real-world tasks using Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99914ba-102f-4eef-8179-0fcf9f971cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XTzBUFWQ-OWj"
   },
   "source": [
    "In this notebook you will use the Google's Gemini to solve:\n",
    "\n",
    "- Task 1: Zero-shot Classification\n",
    "- Task 2: Few-shot Classification\n",
    "- Task 3: Coding Tasks - Python\n",
    "- Task 4: Coding Tasks - SQL\n",
    "- Task 5: Information Extraction\n",
    "- Task 6: Closed-Domain Question Answering\n",
    "- Task 7: Open-Domain Question Answering\n",
    "- Task 8: Document Summarization\n",
    "- Task 9: Transformation\n",
    "- Task 10: Translation\n",
    "\n",
    "\n",
    "\n",
    "___Created By: Dipanjan (DJ)___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "796179d3-e669-4bfa-b0a4-47fed9a165ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install Google GenAI dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf1c95a3-ed4c-4df9-bf8e-36ff8bc484b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8439,
     "status": "ok",
     "timestamp": 1734090996436,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "2evPp14fy258",
    "outputId": "fa19270e-a3ce-4902-880f-7518ea162a53"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\nRequirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\nRequirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\nRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.151.0)\nRequirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.25.5)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.10.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.6)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.66.0)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.27.1)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.68.1)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.62.3)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai==0.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f6d19d5-5465-4ddd-bff0-4b8511159f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CiwGjVWK4q6F"
   },
   "source": [
    "## Load Google GenAI API Credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048b66fc-ed30-4a01-b695-927cc9336de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11843,
     "status": "ok",
     "timestamp": 1734091021050,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "ryheOZuXxa41",
    "outputId": "ffc3b7db-1e8d-4ec4-ba44-ae797c9559ed"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Google Gemini API Key: ··········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "gemini_key = getpass(\"Enter your Google Gemini API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d5d1b30-7ed1-434c-99cb-083205380be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1734091024434,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "kDe44J0N0NcC"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5cc6626-3e4d-4b56-b12d-ebedfcc5030c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VDWhgxCy5bA6"
   },
   "source": [
    "## Create Google Gemini Chat Completion Access Function\n",
    "\n",
    "This function will use the [Google Gemini API](https://ai.google.dev/tutorials/python_quickstart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d44f8928-6179-4827-a934-3464259b8dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1734091024775,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "kA9gVCwK0WKd"
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gemini-1.5-flash\"):\n",
    "    model = genai.GenerativeModel(model,\n",
    "                                  generation_config=genai.GenerationConfig(\n",
    "                                      temperature=0,\n",
    "    ))\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aba973e4-697d-4118-9ba5-62475a020f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1TFZjzuGjCOw"
   },
   "source": [
    "## Let's try out the Gemini API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "183c5718-b5d9-42d8-8914-e0f591d359cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 5134,
     "status": "ok",
     "timestamp": 1734091082806,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "KK-kjmMoi5rO",
    "outputId": "e4b5aefe-6516-4c16-c390-656ffb84ce33"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "* **Creates new content:** Generative AI uses algorithms to produce various forms of content, including text, images, audio, and video, rather than simply analyzing or classifying existing data.\n",
       "\n",
       "* **Learns from input data:**  It's trained on massive datasets to learn patterns and structures, allowing it to generate outputs that resemble the training data but are novel and original.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "response = get_completion(prompt='Explain Generative AI in 2 bullet points',\n",
    "                          model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26577d06-09da-44d6-a7d2-fef972460c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AeDkpvGDhMGV"
   },
   "source": [
    "## Task 1: Zero-shot Classification\n",
    "\n",
    "This prompt tests an LLM's text classification capabilities by prompting it to classify a piece of text without providing any examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0c501e5-c797-4df3-9248-95c00e17afd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hRbBZB57hT0G"
   },
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    f\"\"\"\n",
    "    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n",
    "    The sound quality is impressively clear with just the right amount of bass.\n",
    "    It's also waterproof, which tested true during a recent splashing incident.\n",
    "    Though it's compact, the volume can really fill the space.\n",
    "    The price was a bargain for such high-quality sound.\n",
    "    Shipping was also on point, arriving two days early in secure packaging.\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    Needed a new kitchen blender, but this model has been a nightmare.\n",
    "    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n",
    "    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n",
    "    I thought the brand meant quality, but this product has proven me wrong.\n",
    "    Plus, it arrived three days late. Definitely not worth the expense.\n",
    "    \"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02264b46-4aad-4346-8815-cf22d1e09fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "jZwPaViatl7f"
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for review in reviews:\n",
    "  prompt = f\"\"\"\n",
    "              Act as a product review analyst.\n",
    "              Given the following review,\n",
    "              Display the overall sentiment for the review as only one of the following:\n",
    "              Positive, Negative OR Neutral\n",
    "\n",
    "              ```{review}```\n",
    "              \"\"\"\n",
    "  response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "  responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d17a8bef-d316-403b-9486-4e97a037445f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdUFkKAmtmBj",
    "outputId": "ab2eff1a-cbdd-4047-9b98-3beec72f5156"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n    The sound quality is impressively clear with just the right amount of bass.\n    It's also waterproof, which tested true during a recent splashing incident.\n    Though it's compact, the volume can really fill the space.\n    The price was a bargain for such high-quality sound.\n    Shipping was also on point, arriving two days early in secure packaging.\n    \nSentiment: Positive\n\n------\n\n\nReview: \n    Needed a new kitchen blender, but this model has been a nightmare.\n    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n    I thought the brand meant quality, but this product has proven me wrong.\n    Plus, it arrived three days late. Definitely not worth the expense.\n    \nSentiment: Negative\n\n------\n\n\n"
     ]
    }
   ],
   "source": [
    "for review, response in zip(reviews, responses):\n",
    "  print('Review:', review)\n",
    "  print('Sentiment:', response)\n",
    "  print('------')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5299ba66-e169-4c85-81dc-1d2eac09dcbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6WC6aycZ9Qe3"
   },
   "source": [
    "## Task 2: Few-shot Classification\n",
    "\n",
    "This prompt tests an LLM's text classification capabilities by prompting it to classify a piece of text by providing a few examples of inputs and outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b6fe4b1-4d9a-40be-a661-48f3613fe073",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kQZdygfUoXGT"
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for review in reviews:\n",
    "  prompt = f\"\"\"\n",
    "              Act as a product review analyst.\n",
    "              Given the following review,\n",
    "              Display only the overall sentiment for the review:\n",
    "\n",
    "              Try to classify it by using the following examples as a reference:\n",
    "\n",
    "              Review: Just received the Laptop I ordered for work, and it's amazing.\n",
    "              Sentiment: 😊\n",
    "\n",
    "              Review: Needed a new mechanical keyboard, but this model has been totally disappointing.\n",
    "              Sentiment: 😡\n",
    "\n",
    "              Review: ```{review}```\n",
    "              \"\"\"\n",
    "  response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "  responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a362aec-5307-4211-851c-59c18823d53b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCsMzs3QodqR",
    "outputId": "0fd58f14-67e7-4c27-b290-fd41ce6ca76b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n    Just received the Bluetooth speaker I ordered for beach outings, and it's fantastic.\n    The sound quality is impressively clear with just the right amount of bass.\n    It's also waterproof, which tested true during a recent splashing incident.\n    Though it's compact, the volume can really fill the space.\n    The price was a bargain for such high-quality sound.\n    Shipping was also on point, arriving two days early in secure packaging.\n    \nSentiment: 😊\n\n------\n\n\nReview: \n    Needed a new kitchen blender, but this model has been a nightmare.\n    It's supposed to handle various foods, but it struggles with anything tougher than cooked vegetables.\n    It's also incredibly noisy, and the 'easy-clean' feature is a joke; food gets stuck under the blades constantly.\n    I thought the brand meant quality, but this product has proven me wrong.\n    Plus, it arrived three days late. Definitely not worth the expense.\n    \nSentiment: 😡\n\n------\n\n\n"
     ]
    }
   ],
   "source": [
    "for review, response in zip(reviews, responses):\n",
    "  print('Review:', review)\n",
    "  print('Sentiment:', response)\n",
    "  print('------')\n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1640ebbf-e4d0-45af-ab44-f76d939286a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HEKTyfWf9cxY"
   },
   "source": [
    "## Task 3: Coding Tasks - Python\n",
    "\n",
    "This prompt tests an LLM's capabilities for generating python code based on various tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2120885f-0502-4aeb-af9f-b72d632b670c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6xawr-Co9b0t"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in generating python code\n",
    "\n",
    "Your task is to generate python code\n",
    "to build a Chain of Though prompt pattern using\n",
    "the openai python library\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c9faa55-8b17-46fd-88ce-7602842f9121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2Cenat0G9-Bw",
    "outputId": "db0cdcaa-964b-407a-fbcc-b7b7c2586531"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "This code demonstrates building Chain of Thought (CoT) prompts using the OpenAI Python library.  It handles several scenarios and best practices for robust prompt engineering.\n",
       "\n",
       "```python\n",
       "import openai\n",
       "\n",
       "# Set your OpenAI API key\n",
       "openai.api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with your actual key\n",
       "\n",
       "def generate_cot_prompt(question, examples=None, num_examples=2, model=\"gpt-3.5-turbo\"):\n",
       "    \"\"\"\n",
       "    Generates a Chain of Thought prompt.\n",
       "\n",
       "    Args:\n",
       "        question: The question to answer.\n",
       "        examples: A list of (question, answer) tuples for examples.  If None, defaults are used.\n",
       "        num_examples: The number of examples to include (if examples is None, this is ignored).\n",
       "        model: The OpenAI model to use.\n",
       "\n",
       "    Returns:\n",
       "        A string containing the Chain of Thought prompt.  Returns None if there's an error.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        if examples is None:\n",
       "            # Default examples if none are provided.  Customize these as needed.\n",
       "            examples = [\n",
       "                (\"What is 2 + 2 * 3?\", \"2 + 2 * 3 = 2 + 6 = 8\"),\n",
       "                (\"If a train leaves Chicago at 8 am traveling at 60 mph and another train leaves at 9 am traveling at 75 mph, when will they meet?\", \"Let's think step by step. The first train has a one-hour head start.  In that hour, it travels 60 miles. The second train is 60 miles behind. The relative speed is 75 - 60 = 15 mph.  It will take 60 miles / 15 mph = 4 hours for the second train to catch up.  Therefore, they will meet at 9 am + 4 hours = 1 pm.\")\n",
       "            ]\n",
       "            \n",
       "            #Select a subset of examples if needed\n",
       "            examples = examples[:num_examples]\n",
       "\n",
       "        prompt = f\"\"\"Let's think step by step.\n",
       "\n",
       "{''.join([f\"Q: {ex[0]}\\nA: {ex[1]}\\n\" for ex in examples])}\n",
       "\n",
       "Q: {question}\\nA:\"\"\"\n",
       "\n",
       "        return prompt\n",
       "\n",
       "    except Exception as e:\n",
       "        print(f\"Error generating prompt: {e}\")\n",
       "        return None\n",
       "\n",
       "\n",
       "def get_cot_answer(prompt, model=\"gpt-3.5-turbo\"):\n",
       "    \"\"\"\n",
       "    Gets the answer using the OpenAI API.\n",
       "\n",
       "    Args:\n",
       "        prompt: The Chain of Thought prompt.\n",
       "        model: The OpenAI model to use.\n",
       "\n",
       "    Returns:\n",
       "        The answer from the model. Returns None if there's an error.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        response = openai.Completion.create(\n",
       "            engine=model,\n",
       "            prompt=prompt,\n",
       "            max_tokens=150,  # Adjust as needed\n",
       "            n=1,\n",
       "            stop=None,\n",
       "            temperature=0.0, # Lower temperature for more deterministic answers\n",
       "        )\n",
       "        answer = response.choices[0].text.strip()\n",
       "        return answer\n",
       "    except Exception as e:\n",
       "        print(f\"Error getting answer: {e}\")\n",
       "        return None\n",
       "\n",
       "\n",
       "# Example usage:\n",
       "question = \"What is 15 + 23 * 2 - 10?\"\n",
       "cot_prompt = generate_cot_prompt(question)\n",
       "\n",
       "if cot_prompt:\n",
       "    answer = get_cot_answer(cot_prompt)\n",
       "    print(f\"Question: {question}\")\n",
       "    print(f\"Answer: {answer}\")\n",
       "\n",
       "\n",
       "question2 = \"If apples cost $1.50 per pound and I buy 3 pounds, how much do I owe?\"\n",
       "cot_prompt2 = generate_cot_prompt(question2, num_examples=1) #Using only one example\n",
       "\n",
       "if cot_prompt2:\n",
       "    answer2 = get_cot_answer(cot_prompt2)\n",
       "    print(f\"\\nQuestion: {question2}\")\n",
       "    print(f\"Answer: {answer2}\")\n",
       "\n",
       "#Example with custom examples\n",
       "custom_examples = [(\"What is the capital of France?\", \"The capital of France is Paris.\")]\n",
       "cot_prompt3 = generate_cot_prompt(\"What is the capital of Germany?\", examples=custom_examples)\n",
       "if cot_prompt3:\n",
       "    answer3 = get_cot_answer(cot_prompt3)\n",
       "    print(f\"\\nQuestion: What is the capital of Germany?\")\n",
       "    print(f\"Answer: {answer3}\")\n",
       "\n",
       "```\n",
       "\n",
       "Remember to replace `\"YOUR_OPENAI_API_KEY\"` with your actual API key.  This improved code provides more flexibility, error handling, and clear examples of how to use Chain of Thought prompting effectively with the OpenAI API.  Adjust `max_tokens` and `temperature` parameters as needed for your specific use case.  Experiment with different examples to fine-tune the model's performance.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b543912-4572-429e-b25b-e2f000303062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "0JoSF5wb_Imw"
   },
   "source": [
    "## Task 4: Coding Tasks - SQL\n",
    "\n",
    "This prompt tests an LLM's capabilities for generating SQL code based on various tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c47ee0-9fb4-45f0-92e3-bc8414dfeb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4XKGZIG3_Tjl"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in generating SQL code.\n",
    "\n",
    "Understand the following schema of the database tables carefully:\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table employees, columns = [EmployeeId, EmployeeName, DepartmentId]\n",
    "Table salaries, columns = [EmployeeId, Salary]\n",
    "\n",
    "Create a MySQL query for the employee with max salary in the 'IT' Department.\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7e2570-38d8-4b77-bfc3-a205939b10b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4cFqsBtSA7Wx",
    "outputId": "813d39a1-30fc-42f1-cba6-087252278cf7"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "Several approaches exist to find the employee with the maximum salary in the 'IT' department.  Here are two, with explanations highlighting their strengths and weaknesses:\n",
       "\n",
       "**Method 1: Using a subquery**\n",
       "\n",
       "This method is generally considered more readable and easier to understand, especially for those less familiar with window functions.\n",
       "\n",
       "```sql\n",
       "SELECT\n",
       "    e.EmployeeName,\n",
       "    s.Salary\n",
       "FROM\n",
       "    employees e\n",
       "JOIN\n",
       "    salaries s ON e.EmployeeId = s.EmployeeId\n",
       "JOIN\n",
       "    departments d ON e.DepartmentId = d.DepartmentId\n",
       "WHERE\n",
       "    d.DepartmentName = 'IT'\n",
       "ORDER BY\n",
       "    s.Salary DESC\n",
       "LIMIT 1;\n",
       "```\n",
       "\n",
       "* **Explanation:** This query first joins the three tables to bring together employee information, salary, and department.  It then filters for employees in the 'IT' department. Finally, it orders the results by salary in descending order and limits the output to the top row (the highest salary).\n",
       "\n",
       "* **Strengths:** Clear, easy to understand, works well even with potential ties in salary (it returns one of the employees with the highest salary).\n",
       "\n",
       "* **Weaknesses:** Can be slightly less efficient than methods using window functions, especially on very large datasets.\n",
       "\n",
       "\n",
       "**Method 2: Using a window function (MySQL 8.0 and later)**\n",
       "\n",
       "This method leverages the power of window functions for potentially better performance on large datasets.\n",
       "\n",
       "```sql\n",
       "SELECT\n",
       "    EmployeeName,\n",
       "    Salary\n",
       "FROM (\n",
       "    SELECT\n",
       "        e.EmployeeName,\n",
       "        s.Salary,\n",
       "        ROW_NUMBER() OVER (ORDER BY s.Salary DESC) as rn\n",
       "    FROM\n",
       "        employees e\n",
       "    JOIN\n",
       "        salaries s ON e.EmployeeId = s.EmployeeId\n",
       "    JOIN\n",
       "        departments d ON e.DepartmentId = d.DepartmentId\n",
       "    WHERE\n",
       "        d.DepartmentName = 'IT'\n",
       ") ranked_salaries\n",
       "WHERE rn = 1;\n",
       "```\n",
       "\n",
       "* **Explanation:**  This query uses a subquery to assign a rank to each employee within the 'IT' department based on their salary (highest salary gets rank 1). The outer query then selects only the employee with rank 1.  `ROW_NUMBER()` handles ties by assigning unique ranks.\n",
       "\n",
       "* **Strengths:** Can be more efficient than the subquery approach, especially for large datasets.  Handles ties consistently.\n",
       "\n",
       "* **Weaknesses:** Requires MySQL 8.0 or later (which supports window functions).  Slightly more complex to understand for those unfamiliar with window functions.\n",
       "\n",
       "\n",
       "**Choosing the best method:**\n",
       "\n",
       "For most cases, especially if readability is prioritized or if you're using an older MySQL version, **Method 1 (the subquery approach)** is perfectly adequate.  If you're working with a very large dataset and using MySQL 8.0 or later, **Method 2 (the window function approach)** might offer better performance.  Always test both methods on your specific data to determine which performs best.  Remember to handle potential `NULL` values in the `Salary` column appropriately if they exist in your data, perhaps by adding `WHERE s.Salary IS NOT NULL` to both queries.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5059f328-845d-4712-8ad7-d4aea46b3718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-mck_SlcM3bG"
   },
   "source": [
    "## Task 5: Information Extraction\n",
    "\n",
    "This prompt tests an LLM's capabilities for extracting and analyzing key entities from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e0098f9-9929-4a4c-8cf1-44c1b977a89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "UxciK2wsIw3U"
   },
   "outputs": [],
   "source": [
    "clinical_note = \"\"\"\n",
    "60-year-old man in NAD with a h/o CAD, DM2, asthma, pharyngitis, SBP,\n",
    "and HTN on altace for 8 years awoke from sleep around 1:00 am this morning\n",
    "with a sore throat and swelling of the tongue.\n",
    "He came immediately to the ED because he was having difficulty swallowing and\n",
    "some trouble breathing due to obstruction caused by the swelling.\n",
    "He did not have any associated SOB, chest pain, itching, or nausea.\n",
    "He has not noticed any rashes.\n",
    "He says that he feels like it is swollen down in his esophagus as well.\n",
    "He does not recall vomiting but says he might have retched a bit.\n",
    "In the ED he was given 25mg benadryl IV, 125 mg solumedrol IV,\n",
    "and pepcid 20 mg IV.\n",
    "Family history of CHF and esophageal cancer (father).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ebf2913-b3f5-4358-8cdb-41eaf9dd20cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "GcazKzm0JGKc"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Act as an expert in analyzing and understanding clinical doctor notes in healthcare.\n",
    "Extract all symptoms only from the clinical note information below.\n",
    "Differentiate between symptoms that are present vs. absent.\n",
    "Give me the probability (high/ medium/ low) of how sure you are about the result.\n",
    "Add a note on the probabilities and why you think so.\n",
    "\n",
    "Output as a markdown table with the following columns,\n",
    "all symptoms should be expanded and no acronyms unless you don't know:\n",
    "\n",
    "Symptoms | Present/Denies | Probability.\n",
    "\n",
    "Also expand all acronyms.\n",
    "Output that also as a separate appendix table in Markdown.\n",
    "\n",
    "Clinical Note:\n",
    "```{clinical_note}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fffca90-2a15-4188-b38e-d5de2b2380f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "pqXCxmtEJl4h",
    "outputId": "7a4eac75-f860-4d76-9507-237482430279"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "| Symptoms | Present/Denies | Probability |\n",
       "|---|---|---|\n",
       "| Sore throat | Present | High |\n",
       "| Tongue swelling | Present | High |\n",
       "| Difficulty swallowing (dysphagia) | Present | High |\n",
       "| Difficulty breathing (dyspnea) due to upper airway obstruction | Present | High |\n",
       "| Shortness of breath (SOB) | Denies | High |\n",
       "| Chest pain | Denies | High |\n",
       "| Itching | Denies | High |\n",
       "| Nausea | Denies | High |\n",
       "| Rash | Denies | High |\n",
       "| Esophageal swelling | Present | Medium |\n",
       "| Vomiting | Denies/Uncertain | Medium |\n",
       "| Retching | Present | Medium |\n",
       "\n",
       "\n",
       "**Probability Note:**\n",
       "\n",
       "The probabilities are assigned based on the explicit statements within the clinical note.  The high probability symptoms are directly mentioned and clearly stated as present or absent by the patient.  The medium probability symptoms require some interpretation.  \"Esophageal swelling\" is inferred from the patient's subjective feeling of swelling \"down in his esophagus,\" which is not a direct observation.  Similarly, vomiting is denied, but the patient mentions possible retching, creating uncertainty.\n",
       "\n",
       "\n",
       "**Appendix: Acronym Expansion**\n",
       "\n",
       "| Acronym | Expansion |\n",
       "|---|---|\n",
       "| NAD | No acute distress |\n",
       "| h/o | history of |\n",
       "| CAD | Coronary artery disease |\n",
       "| DM2 | Diabetes Mellitus type 2 |\n",
       "| SBP | Systolic blood pressure (likely, contextually) |\n",
       "| HTN | Hypertension |\n",
       "| SOB | Shortness of breath |\n",
       "| ED | Emergency Department |\n",
       "| IV | Intravenous |\n",
       "| CHF | Congestive heart failure |\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a11fda1-bb12-4799-ba38-3c6c82ab7247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_jucY6L9w14Z"
   },
   "source": [
    "## Task 6: Closed-Domain Question Answering\n",
    "\n",
    "Question Answering (QA) is a natural language processing task which involves generating the desired answer for the given question. Question Answering can be open-domain QA or closed-domain QA depending on whether the LLM is provided with the relevant context or not.\n",
    "\n",
    "In the case of closed-domain QA, a question along with relevant context is given. Here the context is nothing but the relevant text which ideally should have the answer. Just like a RAG workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f17854-735f-4f27-b8bc-0bf4ac8c964d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8I7ZsP3OxL-n"
   },
   "outputs": [],
   "source": [
    "report = \"\"\"\n",
    "Three quarters (77%) of the population saw an increase in their regular outgoings over the past year,\n",
    "according to findings from our recent consumer survey. In contrast, just over half (54%) of respondents\n",
    "had an increase in their salary, which suggests that the burden of costs outweighing income remains for\n",
    "most. In total, across the 2,500 people surveyed, the increase in outgoings was 18%, three times higher\n",
    "than the 6% increase in income.\n",
    "\n",
    "Despite this, the findings of our survey suggest we have reached a plateau. Looking at savings,\n",
    "for example, the share of people who expect to make regular savings this year is just over 70%,\n",
    "broadly similar to last year. Over half of those saving plan to use some of the funds for residential\n",
    "property. A third are saving for a deposit, and a further 20% for an investment property or second home.\n",
    "\n",
    "But for some, their plans are being pushed back. 9% of respondents stated they had planned to purchase\n",
    "a new home this year but have now changed their mind. While for many the deposit may be an issue,\n",
    "the other driving factor remains the cost of the mortgage, which has been steadily rising the last\n",
    "few years. For those that currently own a property, the survey showed that in the last year,\n",
    "the average mortgage payment has increased from £668.51 to £748.94, or 12%.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37bd5bd6-ca9c-4234-987e-6f9c2cf14e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dAPf8hD2xATM"
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "How much has the average mortage payment increased in the last year?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Using the following context information below please answer the following question\n",
    "to the best of your ability\n",
    "Context:\n",
    "{report}\n",
    "Question:\n",
    "{question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3175fe76-f719-4f6e-9d4c-38c1dabf087f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "whY2mBoXxUPM",
    "outputId": "b2c1d975-2226-49b2-ca51-0310774b327c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "The average mortgage payment has increased by £80.43 in the last year.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f24fd78-21f4-4dd0-be74-4c791c75786e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "A-Dxn9d_xibD",
    "outputId": "8c8005e8-349a-42da-d70a-870c0ecfa232"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "54\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "What percentage of people had an increase in salary last year? Show the answer just as a number.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Using the following context information below please answer the following question\n",
    "to the best of your ability\n",
    "Context:\n",
    "{report}\n",
    "Question:\n",
    "{question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "066a0f70-d238-4242-b74c-6a565126a711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EuQlsSmWxyCd"
   },
   "source": [
    "## Task 7: Open-Domain Question Answering\n",
    "\n",
    "Question Answering (QA) is a natural language processing task which involves generating the desired answer for the given question.\n",
    "\n",
    "In the case of open-domain QA, only the question is asked without providing any context or information. Here, the LLM answers the question using the knowledge gained from large volumes of text data during its training. This is basically Zero-Shot QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efa79205-c04a-4196-bd40-71c8b606e6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fV2MP4OdyETp"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please answer the following question to the best of your ability\n",
    "Question:\n",
    "What is LangChain?\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dea4f48-f6e0-4d53-bbcc-9696a409a981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "OSC-IhDmyRYg",
    "outputId": "e46f288a-8dec-459a-d129-e07d3080bfce"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "LangChain is a framework for developing applications powered by large language models (LLMs).  It's designed to make it easier to build applications that combine the capabilities of LLMs with other sources of computation and knowledge.  Instead of just using an LLM as a standalone tool, LangChain allows you to integrate it with other components, such as:\n",
       "\n",
       "* **Different LLMs:**  Easily switch between various LLMs (OpenAI, Hugging Face, etc.) without changing your core application logic.\n",
       "* **Memory:**  Give your application memory so it can remember previous interactions and context, leading to more coherent and relevant responses.\n",
       "* **Chains:**  Combine multiple LLMs or other components into sequences to perform complex tasks.  For example, you might chain together an LLM for summarization, another for question answering, and a database lookup.\n",
       "* **Indexes:**  Connect LLMs to your own data (documents, databases, etc.) so they can access and process information relevant to your specific needs.  This allows for more informed and accurate responses.\n",
       "* **Agents:**  Allow LLMs to decide which tools (like LLMs, databases, calculators, etc.) to use to answer a question or complete a task.  This enables more autonomous and flexible applications.\n",
       "\n",
       "In essence, LangChain provides a structured and modular way to build more sophisticated and powerful applications that leverage the strengths of LLMs while mitigating their limitations.  It simplifies the process of connecting LLMs to external resources and managing the flow of information between them.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95267732-ab94-4d8d-bd1c-975289d72fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xMX1CcsdMg_C"
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Please answer the following question to the best of your ability\n",
    "Question:\n",
    "What is LangGraph?\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb09d98a-3a02-4cbe-8250-bc8be3982d0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "8-AxYD5AMjk6",
    "outputId": "37876f1e-8896-4a96-b1ba-7fa588edeee1"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "LangGraph is a large-scale multilingual knowledge graph constructed by connecting entities from different languages.  It aims to represent knowledge across multiple languages in a unified and interconnected way, going beyond simple translation by capturing the nuances and relationships between concepts as they exist in different linguistic and cultural contexts.  Essentially, it's a knowledge graph that transcends language barriers.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f372d742-6fed-4866-96f9-9ce69666095a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "REutW_DWauKF",
    "outputId": "a7298877-9b23-47a5-d356-8eb44559c131"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "I don't know.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Please answer the following question to the best of your ability\n",
    "Question:\n",
    "What is LangGraph?\n",
    "If you are not able to find the answer in your trained knowledge just say you don't know\n",
    "Answer:\n",
    "\"\"\"\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a637d1af-6e91-4751-92f1-c7c8a8777710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "tnVmXCZAzyux"
   },
   "source": [
    "## Task 8: Document Summarization\n",
    "\n",
    "Document summarization is a natural language processing task which involves creating a concise summary of the given text, while still capturing all the important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d7fe870-b101-4880-810c-fcb9daccf521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dY53sXf_ymwE"
   },
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "Coronaviruses are a large family of viruses which may cause illness in animals or humans.\n",
    "In humans, several coronaviruses are known to cause respiratory infections ranging from the\n",
    "common cold to more severe diseases such as Middle East Respiratory Syndrome (MERS) and Severe Acute Respiratory Syndrome (SARS).\n",
    "The most recently discovered coronavirus causes coronavirus disease COVID-19.\n",
    "COVID-19 is the infectious disease caused by the most recently discovered coronavirus.\n",
    "This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019.\n",
    "COVID-19 is now a pandemic affecting many countries globally.\n",
    "The most common symptoms of COVID-19 are fever, dry cough, and tiredness.\n",
    "Other symptoms that are less common and may affect some patients include aches\n",
    "and pains, nasal congestion, headache, conjunctivitis, sore throat, diarrhea,\n",
    "loss of taste or smell or a rash on skin or discoloration of fingers or toes.\n",
    "These symptoms are usually mild and begin gradually.\n",
    "Some people become infected but only have very mild symptoms.\n",
    "Most people (about 80%) recover from the disease without needing hospital treatment.\n",
    "Around 1 out of every 5 people who gets COVID-19 becomes seriously ill and develops difficulty breathing.\n",
    "Older people, and those with underlying medical problems like high blood pressure, heart and lung problems,\n",
    "diabetes, or cancer, are at higher risk of developing serious illness.\n",
    "However, anyone can catch COVID-19 and become seriously ill.\n",
    "People of all ages who experience fever and/or  cough associated with difficulty breathing/shortness of breath,\n",
    "chest pain/pressure, or loss of speech or movement should seek medical attention immediately.\n",
    "If possible, it is recommended to call the health care provider or facility first,\n",
    "so the patient can be directed to the right clinic.\n",
    "People can catch COVID-19 from others who have the virus.\n",
    "The disease spreads primarily from person to person through small droplets from the nose or mouth,\n",
    "which are expelled when a person with COVID-19 coughs, sneezes, or speaks.\n",
    "These droplets are relatively heavy, do not travel far and quickly sink to the ground.\n",
    "People can catch COVID-19 if they breathe in these droplets from a person infected with the virus.\n",
    "This is why it is important to stay at least 1 meter) away from others.\n",
    "These droplets can land on objects and surfaces around the person such as tables, doorknobs and handrails.\n",
    "People can become infected by touching these objects or surfaces, then touching their eyes, nose or mouth.\n",
    "This is why it is important to wash your hands regularly with soap and water or clean with alcohol-based hand rub.\n",
    "Practicing hand and respiratory hygiene is important at ALL times and is the best way to protect others and yourself.\n",
    "When possible maintain at least a 1 meter distance between yourself and others.\n",
    "This is especially important if you are standing by someone who is coughing or sneezing.\n",
    "Since some infected persons may not yet be exhibiting symptoms or their symptoms may be mild,\n",
    "maintaining a physical distance with everyone is a good idea if you are in an area where COVID-19 is circulating.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f723358-e842-4b15-9396-d74dac3ca0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "7upxVwh5y8te",
    "outputId": "1677043a-dfb7-49f9-bfa6-35ea20e3e6de"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "Summary: Coronaviruses cause respiratory illnesses, ranging from the common cold to severe diseases like SARS and MERS.  COVID-19, a novel coronavirus, is a global pandemic with symptoms including fever, cough, and tiredness, though many experience mild or no symptoms.  Transmission occurs through respiratory droplets produced by coughs and sneezes, and contact with contaminated surfaces.  Serious illness is more likely in older adults and those with underlying health conditions.  Prevention involves maintaining physical distance, practicing hand hygiene, and seeking medical attention if symptoms worsen.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an expert in generating accurate document summaries.\n",
    "Generate a summary of the given document.\n",
    "\n",
    "Document:\n",
    "{doc}\n",
    "\n",
    "\n",
    "Constraints: Please start the summary with the delimiter 'Summary'\n",
    "and limit the summary to 5 lines\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a390b1f0-0803-40f6-b2fa-361a4f133206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "keC5k5S-18cw"
   },
   "source": [
    "## Task 9: Transformation\n",
    "\n",
    "You can use LLMs to take an existing document and transform it into other formats of content and even generate training data for fine-tuning or training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0622efe8-7047-42c8-9040-e7bbc8981003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ta182I2t2FD0"
   },
   "outputs": [],
   "source": [
    "fact_sheet_mobile = \"\"\"\n",
    "PRODUCT NAME\n",
    "Samsung Galaxy Z Fold4 5G Black\n",
    "​\n",
    "PRODUCT OVERVIEW\n",
    "Stands out. Stands up. Unfolds.\n",
    "The Galaxy Z Fold4 does a lot in one hand with its 15.73 cm(6.2-inch) Cover Screen.\n",
    "Unfolded, the 19.21 cm(7.6-inch) Main Screen lets you really get into the zone.\n",
    "Pushed-back bezels and the Under Display Camera means there's more screen\n",
    "and no black dot getting between you and the breathtaking Infinity Flex Display.\n",
    "Do more than more with Multi View. Whether toggling between texts or catching up\n",
    "on emails, take full advantage of the expansive Main Screen with Multi View.\n",
    "PC-like power thanks to Qualcomm Snapdragon 8+ Gen 1 processor in your pocket,\n",
    "transforms apps optimized with One UI to give you menus and more in a glance\n",
    "New Taskbar for PC-like multitasking. Wipe out tasks in fewer taps. Add\n",
    "apps to the Taskbar for quick navigation and bouncing between windows when\n",
    "you're in the groove.4 And with App Pair, one tap launches up to three apps,\n",
    "all sharing one super-productive screen\n",
    "Our toughest Samsung Galaxy foldables ever. From the inside out,\n",
    "Galaxy Z Fold4 is made with materials that are not only stunning,\n",
    "but stand up to life's bumps and fumbles. The front and rear panels,\n",
    "made with exclusive Corning Gorilla Glass Victus+, are ready to resist\n",
    "sneaky scrapes and scratches. With our toughest aluminum frame made with\n",
    "Armor Aluminum, this is one durable smartphone.\n",
    "World’s first water resistant foldable smartphones. Be adventurous, rain\n",
    "or shine. You don't have to sweat the forecast when you've got one of the\n",
    "world's first water-resistant foldable smartphones.\n",
    "​\n",
    "PRODUCT SPECS\n",
    "OS - Android 12.0\n",
    "RAM - 12 GB\n",
    "Product Dimensions - 15.5 x 13 x 0.6 cm; 263 Grams\n",
    "Batteries - 2 Lithium Ion batteries required. (included)\n",
    "Item model number - SM-F936BZKDINU_5\n",
    "Wireless communication technologies - Cellular\n",
    "Connectivity technologies - Bluetooth, Wi-Fi, USB, NFC\n",
    "GPS - True\n",
    "Special features - Fast Charging Support, Dual SIM, Wireless Charging, Built-In GPS, Water Resistant\n",
    "Other display features - Wireless\n",
    "Device interface - primary - Touchscreen\n",
    "Resolution - 2176x1812\n",
    "Other camera features - Rear, Front\n",
    "Form factor - Foldable Screen\n",
    "Colour - Phantom Black\n",
    "Battery Power Rating - 4400\n",
    "Whats in the box - SIM Tray Ejector, USB Cable\n",
    "Manufacturer - Samsung India pvt Ltd\n",
    "Country of Origin - China\n",
    "Item Weight - 263 g\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9030b718-e3cc-4c82-8b2c-c8967403a1e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "zko53u-N2KmD",
    "outputId": "f94cbb6b-b1cc-4848-8516-85f0d0a2cf7c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "**FAQ: Samsung Galaxy Z Fold4 5G Black**\n",
       "\n",
       "**Q1: What are the screen sizes of the Samsung Galaxy Z Fold4?**\n",
       "\n",
       "A1: The Galaxy Z Fold4 features a 6.2-inch Cover Screen and a 7.6-inch Main Screen when unfolded.\n",
       "\n",
       "**Q2: What makes the Galaxy Z Fold4's display special?**\n",
       "\n",
       "A2:  It boasts an Infinity Flex Display with pushed-back bezels and an Under Display Camera, minimizing bezels and eliminating a distracting black dot.\n",
       "\n",
       "**Q3: How does the Galaxy Z Fold4 handle multitasking?**\n",
       "\n",
       "A3: Multitasking is enhanced with Multi View, allowing you to use multiple apps simultaneously on the large Main Screen.  A new Taskbar and App Pair features further improve multitasking capabilities, similar to a PC experience.\n",
       "\n",
       "**Q4: What is the processor and operating system of the Galaxy Z Fold4?**\n",
       "\n",
       "A4: It's powered by a Qualcomm Snapdragon 8+ Gen 1 processor and runs on Android 12.0.\n",
       "\n",
       "**Q5: How durable is the Samsung Galaxy Z Fold4?**\n",
       "\n",
       "A5: It's Samsung's toughest foldable yet, featuring Corning Gorilla Glass Victus+ on the front and rear panels and an Armor Aluminum frame for enhanced scratch and impact resistance.\n",
       "\n",
       "**Q6: Is the Galaxy Z Fold4 water resistant?**\n",
       "\n",
       "A6: Yes, it's one of the world's first water-resistant foldable smartphones.\n",
       "\n",
       "**Q7: What are the key specifications of the Galaxy Z Fold4?**\n",
       "\n",
       "A7:  It has 12GB of RAM, a 4400mAh battery (dual Lithium-ion), and supports fast and wireless charging.  Connectivity includes Bluetooth, Wi-Fi, USB, NFC, and GPS.\n",
       "\n",
       "**Q8: What's included in the box?**\n",
       "\n",
       "A8: The box contains the phone, a SIM tray ejector, and a USB cable.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt =f\"\"\"Turn the following product description into a list of frequently asked questions (FAQ).\n",
    "Show both the question and it's corresponding answer.\n",
    "Create at the max 8 FAQs\n",
    "\n",
    "Product description:\n",
    "```{fact_sheet_mobile}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbe09944-04b1-4320-b1f7-fce829c06479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "53QdnJ0M3Vee"
   },
   "source": [
    "## Task 10: Translation\n",
    "\n",
    "You can use LLMs to take an existing document and translate it from a source to target language. You can also translate to multiple languages at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15d625f-acef-46c0-abf9-caf8ccd92ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "g-jdFJqJ2cWu",
    "outputId": "8045ef23-46a0-4c74-8520-1acc2e589b4a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "The English phrase \"Hello, how are you today?\" requires nuanced translation to accurately capture the level of formality and the implied meaning, which is often more of a polite greeting than a genuine inquiry about one's well-being.  A literal translation wouldn't always be appropriate.\n",
       "\n",
       "**German:**\n",
       "\n",
       "* **Formal:** \"Guten Tag, wie geht es Ihnen heute?\" (This is the most formal option, using the polite \"Sie\" form of \"you.\")\n",
       "* **Informal:** \"Hallo, wie geht es dir heute?\" (This uses the informal \"du\" form of \"you,\" appropriate for friends and family.)\n",
       "* **Very informal/colloquial:** \"Hi, wie läuft's?\" or \"Moin, wie geht's?\" (\"Moin\" is a common Northern German greeting).  These options are less direct about asking how one's day is going.\n",
       "\n",
       "\n",
       "**Spanish:**\n",
       "\n",
       "* **Formal:** \"Buenos días, ¿cómo está usted hoy?\" (Uses the formal \"usted\" form of \"you.\")\n",
       "* **Informal:** \"Hola, ¿cómo estás hoy?\" (Uses the informal \"tú\" form of \"you.\")\n",
       "* **Very informal/colloquial:** \"¿Qué tal?\" or \"¿Cómo va todo?\"  These are common greetings that don't directly ask \"how are you today,\" but convey a similar sentiment.\n",
       "\n",
       "\n",
       "**Choosing the right translation:**  The best translation depends heavily on the context.  Who is being addressed? What is the relationship between the speaker and the listener?  A simple \"Hello, how are you?\" in English often doesn't require a detailed answer, and the same is true in German and Spanish.  The more informal options reflect this better.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"You are an expert translator.\n",
    "Translate the given text from English to German and Spanish.\n",
    "\n",
    "Text: 'Hello, how are you today?'\n",
    "Translation:\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt, model='gemini-1.5-flash')\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "M5_Hands_On_Prompt_Engineering_with_Google_Gemini",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
