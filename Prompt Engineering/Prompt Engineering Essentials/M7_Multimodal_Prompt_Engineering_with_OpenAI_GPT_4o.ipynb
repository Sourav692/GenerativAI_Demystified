{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2dd811d-6b55-477f-9b06-4e6169661f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iclH0trXX1cS"
   },
   "source": [
    "# Multimodal Prompt Engineering with OpenAI GPT-4o\n",
    "\n",
    "GPT-4o (\"o\" for \"omni\") is designed to handle a combination of text, audio, and video inputs, and can generate outputs in text, audio, and image formats.\n",
    "\n",
    "\n",
    "### Background\n",
    "\n",
    "Before GPT-4o, users could interact with ChatGPT using Voice Mode, which operated with three separate models. GPT-4o will integrate these capabilities into a single model that's trained across text, vision, and audio. This unified approach ensures that all inputs—whether text, visual, or auditory—are processed cohesively by the same neural network.\n",
    "\n",
    "\n",
    "### Current API Capabilities\n",
    "\n",
    "Currently, the API supports `{text, image}` inputs only, with `{text}` outputs, the same modalities as `gpt-4-turbo`. Additional modalities, including audio, will be introduced soon. This guide will help you get started with using GPT-4o for text, image, and video understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab478a7-fa16-4c4c-99c5-e7567e094389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bzxzvs7mX1cU"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cffdea7-9872-44d7-98cf-d110db4748b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "o6VHu_v1X1cU"
   },
   "source": [
    "### Install OpenAI SDK for Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb251c2c-b6a4-4812-b251-41d89ae78c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10627,
     "status": "ok",
     "timestamp": 1734092212542,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "cV7yu4wAX1cU",
    "outputId": "857fde79-9d52-47c3-a61e-32be8fa1bd99"
   },
   "outputs": [],
   "source": [
    "!pip install openai==1.55.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12434ee7-db97-4252-ac58-2f0bc6b59664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PtBa7rlWJWH3"
   },
   "source": [
    "## Enter API Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe583207-b2ae-4814-815b-4bb661e6f83e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9582,
     "status": "ok",
     "timestamp": 1734092227342,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "Av1UpSgXZUsI",
    "outputId": "5981f492-b848-4526-fe11-713a04334001"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cf6c80b-88c4-4a09-aec4-2af896731f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1734092233956,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "1PIStD04Zp9p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b29fbed5-7a5e-4e90-bd4c-8fb9ef2ddb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "executionInfo": {
     "elapsed": 1792,
     "status": "ok",
     "timestamp": 1734092237070,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "ODMrKo1iX1cV"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0582b8a-30af-4c70-9edc-56e41fe320ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1734092238118,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "RUzd-sY4X1cW",
    "outputId": "daf9416d-f162-4717-a120-b9c9a82a5f1c"
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"}, # <-- This is the system message that provides context to the model\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Could you solve 2+2?\"}  # <-- This is the user message for which the model will generate a response\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82cd7fc3-4355-4e21-961c-460b98e7f6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1734092242671,
     "user": {
      "displayName": "Dipanjan “DJ” Sarkar",
      "userId": "05135987707016476934"
     },
     "user_tz": -330
    },
    "id": "ROUOPIP4YfGE",
    "outputId": "128a3b02-4f3d-40b9-b124-23fc1baa0ef0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b9960e8-f29f-4a0b-94a6-9601bd446324",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vL-8dH57X1cX"
   },
   "source": [
    "## Image Processing\n",
    "GPT-4o can directly process images and take intelligent actions based on the image. We can provide images in two formats:\n",
    "1. Base64 Encoded\n",
    "2. URL\n",
    "\n",
    "Let's first view the image we'll use, then try sending this image as both Base64 and as a URL link to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822a7fd4-6831-4d89-aeed-1409dc793b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAk_I9K3YyvX",
    "outputId": "a1cf87e5-9409-4091-92d4-2df18602f156"
   },
   "outputs": [],
   "source": [
    "!curl -o triangle.png https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c99c7d-bada-4ca6-bec6-8917f42df256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "id": "76rlfwc_X1cY",
    "outputId": "243280d9-8131-427e-f634-6a307a2051a0"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, Audio, Markdown\n",
    "import base64\n",
    "\n",
    "IMAGE_PATH = \"./triangle.png\"\n",
    "\n",
    "# Preview image for context\n",
    "display(Image(IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a89186ed-18a9-46df-a6dd-c78e96a54add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5qXtA4P-X1cY"
   },
   "source": [
    "#### Base64 Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ebe1374-5596-4f46-9ec3-1af5c0260aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "8zVIe9nxX1cY",
    "outputId": "af82da96-4c18-47d2-fbec-134cebb3e8f5"
   },
   "outputs": [],
   "source": [
    "# Open the image file and encode it as a base64 string\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's the area of the triangle?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "252124f4-3bee-4258-9a1b-98840aaeff23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1nf38twSdo0c"
   },
   "source": [
    "To find the area of the triangle, we can use Heron's formula.\n",
    "\n",
    "First, we need to find the semi-perimeter of the triangle.\n",
    "\n",
    "The sides of the triangle are 6, 5, and 9.\n",
    "\n",
    "Calculate the semi-perimeter $( s ): [ s = \\frac{a + b + c}{2} = \\frac{6 + 5 + 9}{2} = 10 ]$\n",
    "\n",
    "Use Heron's formula to find the area $( A )$\n",
    "\n",
    "$: [ A = \\sqrt{s(s-a)(s-b)(s-c)} ]$\n",
    "\n",
    "$[ A = \\sqrt{10(10-6)(10-5)(10-9)} ]$\n",
    "\n",
    "$[ A = \\sqrt{10 \\cdot 4 \\cdot 5 \\cdot 1} ]$\n",
    "\n",
    "$[ A = \\sqrt{200} ]$\n",
    "\n",
    "$[ A = 10\\sqrt{2} ]$\n",
    "\n",
    "So, the area of the triangle is $( 10\\sqrt{2} )$ square units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "986884b4-fd9c-4490-adf0-22fa08df08ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "i7YrcUwSX1cY"
   },
   "source": [
    "#### URL Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aea5d857-c77a-44e4-8e8c-98e036dd9fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "oBKTVG8xX1cY",
    "outputId": "4b1ee9e9-e256-4e3f-cf7d-0c1d91732e6a"
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown. Help me with my math homework!\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's the area of the triangle?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/e/e2/The_Algebra_of_Mohammed_Ben_Musa_-_page_82b.png\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d6c980-df95-4340-92ed-ae6e1af6fbda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAJj1QQuxPm6",
    "outputId": "68a130d9-bb74-45ca-f47b-a0631125452b"
   },
   "outputs": [],
   "source": [
    "! curl -o clinical_note.png https://i.imgur.com/AJwKUEb.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc6e5436-9419-4375-b5c0-a802a19b1265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "id": "wxIgv4CrxhJB",
    "outputId": "44f9b032-41e2-4f21-f530-34dd72399047"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./clinical_note.png\"\n",
    "\n",
    "# Preview image for context\n",
    "display(Image(IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f270eed-44a7-4900-ae3c-35d32f185bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "id": "2X-kuRo-xmLL",
    "outputId": "525204b9-6c6c-4bf2-ee5b-5005e35a85b1"
   },
   "outputs": [],
   "source": [
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Act as an expert in analyzing and understanding handwritten clinical notes.\n",
    "                                         Detect the handwriting in the clinical note and perform tasks as per the user\n",
    "                                      \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\",\n",
    "             \"text\": \"\"\"Extract all symptoms from the given clinical note image.\n",
    "                        Differentiate between symptoms that are present vs. absent.\n",
    "                        Give me the probability (high/ medium/ low) of how sure you are about the result.\n",
    "                        Add a note on the probabilities and why you think so.\n",
    "\n",
    "                        Output as a markdown table with the following columns,\n",
    "                        all symptoms should be expanded and no acronyms unless you don't know:\n",
    "\n",
    "                        Symptoms | Present/Denies | Probability.\n",
    "\n",
    "                        Also expand all acronyms.\n",
    "                        Output that also as a separate appendix table in Markdown.\n",
    "                        Do not make up terms, if something is not detectable leave it out.\n",
    "                     \"\"\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89cabfdd-e306-40f2-87e2-2aff862ef2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7akthobyZD_T",
    "outputId": "ed4e427b-4722-4f0b-a6a8-3f84175c3d67"
   },
   "outputs": [],
   "source": [
    "! curl -o hwrite.png https://i.imgur.com/XWeRd8a.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a388003b-bf06-4cd2-8778-461fab7773a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VjHIS4IGaAZw",
    "outputId": "d8b8eea6-5e3a-4b40-80dc-48db4104e952"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./hwrite.png\"\n",
    "\n",
    "# Preview image for context\n",
    "display(Image(IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "258f4f43-14ba-47dc-b35f-51f2ee626a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "kjOl2b27_Z6z",
    "outputId": "328da50b-bd61-41c1-8f77-f839c91106bf"
   },
   "outputs": [],
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93a0609e-dcd7-44e7-87b9-5535d13c4ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "id": "qffoMLHAZpAq",
    "outputId": "643acb1f-1a48-4c34-8834-57f62b335c75"
   },
   "outputs": [],
   "source": [
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Act as handwriting expert, detect the handwriting in the documents and perform tasks as per the user\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\",\n",
    "             \"text\": \"\"\"Convert the handwritten document into text exactly as in the image,\n",
    "                        do not make up words, if something is not detectable just put [NOT_EXTRACTED]\n",
    "                     \"\"\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec62e5a-e4da-4584-9579-489d6ff19669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cusFSttfbdmh",
    "outputId": "732a212b-6c58-4f54-85ee-f7f6b36c6b4b"
   },
   "outputs": [],
   "source": [
    "!curl -o sales.png https://i.imgur.com/jH3MNNP.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8051a4-cb85-42f0-89c8-7bff69ef4aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "CJluBDnDbnGE",
    "outputId": "016e5480-598a-429f-92b5-30ea30910079"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"./sales.png\"\n",
    "\n",
    "# Preview image for context\n",
    "display(Image(IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71951521-ffd6-4c3a-aebb-8715aeb72a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "4Hk2E4pcbpKC",
    "outputId": "713b58cc-5c56-4fb8-8e71-d289ed94eceb"
   },
   "outputs": [],
   "source": [
    "base64_image = encode_image(IMAGE_PATH)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Act as a data analyst, your job is to analyze visuals and give insights\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\",\n",
    "             \"text\": \"\"\"Given this sales report visualization, summarize it briefly,\n",
    "                        give detailed statistics about the top 3 best performing salesmen\n",
    "                     \"\"\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d733b4-61ae-49bd-8ad0-2edb7a650f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gZtJpxPGOQn",
    "outputId": "fbabda55-50fe-4346-aaff-02aab0c54f8c"
   },
   "outputs": [],
   "source": [
    "# download images using curl\n",
    "!curl https://i.imgur.com/6b9jwkk.png -o image1.png\n",
    "!curl https://i.imgur.com/9CWuU2q.png -o image2.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40b4a930-3f0b-41ef-9980-53ed33585562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "V8Ok3Z1EHMYG",
    "outputId": "a374e149-f055-4fc5-e181-9ebf0317c9e1"
   },
   "outputs": [],
   "source": [
    "display(Image('image1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c050abf-b6ca-4456-8e8c-47ebd9b2d799",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tupxtH9kHTvf",
    "outputId": "69d61a12-5f73-4de0-aa3f-6e27e62bcffd"
   },
   "outputs": [],
   "source": [
    "display(Image('image2.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d377056f-9001-4113-a15b-c2879b501f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "cRu7C0USGMuS",
    "outputId": "1644aff5-815a-4d5e-925d-83fa17c88dff"
   },
   "outputs": [],
   "source": [
    "base64_image1 = encode_image('./image1.png')\n",
    "base64_image2 = encode_image('./image2.png')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Act as an analyst, your job is to analyze document scans and give insights\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\",\n",
    "             \"text\": \"\"\"Given the following images which can contain graphs, tables and text,\n",
    "                        analyze all of them to answer the following questions:\n",
    "\n",
    "                        - Tell me about the top 5 years with largest Wildfires\n",
    "                        - Tell me about trend of wildfires in terms of acreage burned by region and ownership\n",
    "                     \"\"\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image1}\"}\n",
    "            },\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{base64_image2}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0706a043-fb95-48e1-83ca-794e6cb402ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "z2KkBf9SX1cZ"
   },
   "source": [
    "## Video Processing\n",
    "While it's not possible to directly send a video to the API, GPT-4o can understand videos if you sample frames and then provide them as images. It performs better at this task than GPT-4 Turbo.\n",
    "\n",
    "Since GPT-4o in the API does not yet support video directly and audio is in beta (as of November-December 2024), we'll use a combination of GPT-4o and Whisper to process both the audio and we will manually convert the video into a list of image frames, and showcase two usecases:\n",
    "1. Summarization\n",
    "2. Question and Answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "240c72b9-c102-4ca1-b963-3b29941fab6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "m8_C8D-VX1cZ"
   },
   "source": [
    "### Setup for Video Processing\n",
    "We'll use two python packages for video processing - opencv-python and moviepy.\n",
    "\n",
    "These require [ffmpeg](https://ffmpeg.org/about.html), so make sure to install this beforehand. Depending on your OS, you may need to run `brew install ffmpeg` or `sudo apt install ffmpeg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6b264d-4e8d-416f-b684-6d5788222c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "5dYwLuuWX1cZ"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python --quiet\n",
    "!pip install moviepy --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9921f9d-8bdd-4dd7-822b-6f09132df787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LP9874_nX1cZ"
   },
   "source": [
    "### Process the video into two components: frames and audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca63b4a4-0acc-4cae-a350-848dffb06957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjNIQw59H4-Z",
    "outputId": "b89f9b1b-121c-4dcd-9ab8-804675f97df0"
   },
   "outputs": [],
   "source": [
    "!gdown -O 'keynote_recap.mp4' '1s6WOK3w1hJxcxE7T_WWZFioFGE81uKLb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beace337-9613-4ae1-8ef5-fa6c98a636a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WqTkBfrX1cZ",
    "outputId": "f66ff143-d7e0-405c-de53-65afc6e07d40"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import time\n",
    "import base64\n",
    "\n",
    "# We'll be using the OpenAI DevDay Keynote Recap video. You can review the video here: https://www.youtube.com/watch?v=h02ti0Bl6zk\n",
    "VIDEO_PATH = \"./keynote_recap.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a6b0c1-8bfb-48b0-be11-1aee1c69ddc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kz8RkGUbX1cZ",
    "outputId": "e4af6dde-9b2c-411f-ec51-d3cd56729b4d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import base64\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def process_video(video_path, seconds_per_frame=2):\n",
    "    # Initialize a list to store base64 encoded frames\n",
    "    base64Frames = []\n",
    "    # Extract the base name of the video file without extension\n",
    "    base_video_path, _ = os.path.splitext(video_path)\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames and the frames per second (fps) of the video\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    # Calculate the number of frames to skip between samples\n",
    "    frames_to_skip = int(fps * seconds_per_frame)\n",
    "\n",
    "    # Start from the first frame\n",
    "    curr_frame = 0\n",
    "    # Loop through the video to extract frames at the specified interval\n",
    "    while curr_frame < total_frames - 1:\n",
    "        # Set the current frame position in the video\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, curr_frame)\n",
    "        # Read the frame\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        # Encode the frame as a JPEG image and convert it to base64\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        # Move to the next frame based on the sampling interval\n",
    "        curr_frame += frames_to_skip\n",
    "    # Release the video object\n",
    "    video.release()\n",
    "\n",
    "    # Extract audio from the video\n",
    "    audio_path = f\"{base_video_path}.mp3\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clip.audio.write_audiofile(audio_path, bitrate=\"32k\")  # Save audio with reduced bitrate\n",
    "    clip.audio.close()\n",
    "    clip.close()\n",
    "\n",
    "    print(f\"Extracted {len(base64Frames)} frames\")\n",
    "    print(f\"Extracted audio to {audio_path}\")\n",
    "    # Return the frames as base64 strings and the path to the audio file\n",
    "    return base64Frames, audio_path\n",
    "\n",
    "# Example usage\n",
    "# Extract 1 frame for every 3 seconds from the video\n",
    "base64Frames, audio_path = process_video(VIDEO_PATH, seconds_per_frame=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e4f463a-e8e1-42b6-a26f-6441099cfaa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "wMJypRnsX1cZ",
    "outputId": "6874c8d3-6adc-401e-a138-aa42c673cf92"
   },
   "outputs": [],
   "source": [
    "## Display the frames and audio for context\n",
    "display_handle = display(None, display_id=True)\n",
    "for img in base64Frames:\n",
    "    display_handle.update(Image(data=base64.b64decode(img.encode(\"utf-8\")), width=600))\n",
    "    time.sleep(0.5)\n",
    "\n",
    "Audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b79b5ec-bf69-4bfe-bb9d-bb106421c45b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXWDuKI3c5Jf",
    "outputId": "2df97975-828d-46d7-afbe-2f3fb988e2c2"
   },
   "outputs": [],
   "source": [
    "len(base64Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d969f5e-847d-4df7-ba77-4c94b3453e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "XxLfkoE_IsH-",
    "outputId": "e7dcf21e-6c43-4f57-bf38-39d7bca8de46"
   },
   "outputs": [],
   "source": [
    "base64Frames[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b21bd049-4eca-44c0-b501-15af576cfc4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "Z_-y3fJuIv6o",
    "outputId": "a666752b-b7d7-4901-afa0-857aab26dc0e"
   },
   "outputs": [],
   "source": [
    "Image(data=base64.b64decode(base64Frames[55].encode(\"utf-8\")), width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73908791-2563-4612-947d-6e2312369af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XVb-POX7X1ca"
   },
   "source": [
    "### Example 1: Summarization\n",
    "Now that we have both the video frames and the audio, let's run a few different tests to generate a video summary to compare the results of using the models with different modalities. We should expect to see that the summary generated with context from both visual and audio inputs will be the most accurate, as the model is able to use the entire context from the video.\n",
    "\n",
    "1. Visual Summary\n",
    "2. Audio Summary\n",
    "3. Visual + Audio Summary\n",
    "\n",
    "#### Visual Summary\n",
    "The visual summary is generated by sending the model only the frames from the video. With just the frames, the model is likely to capture the visual aspects, but will miss any details discussed by the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08ecf4b5-3a3d-4ee5-801b-11dcb9e8d465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "yLZa96q9X1ca",
    "outputId": "fd6b468a-69c5-4730-d7f4-5d87cc8de5e6"
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\": \"\"\"You are generating a video summary.\n",
    "                   Create a detailed summary of the provided video with key bullet points.\n",
    "                   Respond in Markdown.\n",
    "                \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        \"These are the frames from the video.\",\n",
    "        *map(lambda x: {\"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames)\n",
    "        ],\n",
    "    }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67e1f638-db4f-4095-9a49-0d06e906b9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gsdL8cyvX1ca"
   },
   "source": [
    "The results are as expected - the model is able to capture the high level aspects of the video visuals, but misses the details provided in the speech.\n",
    "\n",
    "#### Audio Summary\n",
    "The audio summary is generated by sending the model the audio transcript. With just the audio, the model is likely to bias towards the audio content, and will miss the context provided by the presentations and visuals.\n",
    "\n",
    "`{audio}` input for GPT-4o is in beta access via its realtime API but hopefully we see it in a stable release in 2025! For now, we use our existing `whisper-1` model to process the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be87735-71fb-4a77-b30a-e5853896176d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drBkj-S1I97r",
    "outputId": "24c78b57-be0d-4219-ddd8-856580177b64"
   },
   "outputs": [],
   "source": [
    "# Transcribe the audio\n",
    "transcription = client.audio.transcriptions.create(\n",
    "    model=\"whisper-1\",\n",
    "    file=open(audio_path, \"rb\"),\n",
    ")\n",
    "## OPTIONAL: Uncomment the line below to print the transcription\n",
    "print(\"Transcript: \", transcription.text[:1000] + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e24cd5-f0b3-4301-9f73-22426bf605ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "10qjasilX1ca",
    "outputId": "c4fbbfd3-e3bc-49f1-f6bb-ca0e39889a1e"
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\":\"\"\"You are generating a transcript summary.\n",
    "                  Create a detailed summary of the provided transcription with key bullet points.\n",
    "                  Respond in Markdown.\n",
    "               \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription.text}\"}\n",
    "        ],\n",
    "    }\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8162ccf-9d9a-4cc1-9876-4387a9002b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "c1U-QR2uX1ca"
   },
   "source": [
    "The audio summary is biased towards the content discussed during the speech, but comes out with much less structure than the video summary.\n",
    "\n",
    "#### Audio + Visual Summary\n",
    "The Audio + Visual summary is generated by sending the model both the visual and the audio from the video at once. When sending both of these, the model is expected to better summarize since it can perceive the entire video at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "016fee34-d3fe-4e64-b2d8-1c2d4795e0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OFc9r-NNX1ca",
    "outputId": "3d7cbec9-f9a9-4949-fa91-29a7bc380ce5"
   },
   "outputs": [],
   "source": [
    "## Generate a summary with visual and audio\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "    {\"role\": \"system\",\n",
    "     \"content\":\"\"\"You are generating a video summary.\n",
    "                  Create a detailed summary of the provided video and its transcript with key bullet points.\n",
    "                  Respond in Markdown\n",
    "               \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        \"These are the frames from the video.\",\n",
    "        *map(lambda x: {\"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, base64Frames),\n",
    "        {\"type\": \"text\", \"text\": f\"The audio transcription is: {transcription.text}\"}\n",
    "        ],\n",
    "    }\n",
    "],\n",
    "    temperature=0,\n",
    ")\n",
    "Markdown(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bdaec8d-2857-4628-8767-680c58435923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wreLODCuX1ca"
   },
   "source": [
    "After combining both the video and audio, we're able to get a much more detailed and comprehensive summary for the event which uses information from both the visual and audio elements from the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be55201-c348-40fc-9e66-e8f24cd61343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6WD4J5E7X1cb"
   },
   "source": [
    "Comparing the three answers, the most accurate answer is generated by using both the audio and visual from the video. Sam Altman did not discuss the raising windows or radio on during the Keynote, but referenced an improved capability for the model to execute multiple functions in a single request while the examples were shown behind him.\n",
    "\n",
    "## Conclusion\n",
    "Integrating many input modalities such as audio, visual, and textual, significantly enhances the performance of the model on a diverse range of tasks. This multimodal approach allows for more comprehensive understanding and interaction, mirroring more closely how humans perceive and process information.\n",
    "\n",
    "Currently, GPT-4o in the API supports text and image inputs, with audio capabilities in beta (late 2024)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "M7_Multimodal_Prompt_Engineering_with_OpenAI_GPT_4o",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
