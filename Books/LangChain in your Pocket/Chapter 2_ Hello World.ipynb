{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from apikey import apikey\n",
    "os.environ[\"OPENAI_API_KEY\"] = apikey # set the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI() # Create a new instance of the OpenAI class.\n",
    "chat_llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2135,
     "status": "ok",
     "timestamp": 1701188212776,
     "user": {
      "displayName": "mehul gupta",
      "userId": "02075325736316345622"
     },
     "user_tz": -330
    },
    "id": "FHSIuL2irECg",
    "outputId": "7e329353-546e-43c1-861d-ae2f42006c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Sizzle & Spice\"\n",
      "2. \"Flavor Fusion\"\n",
      "3. \"The Culinary Co.\"\n",
      "4. \"Taste Haven\"\n",
      "5. \"Kitchen Creations\"\n",
      "\n",
      "1. MindForge\n",
      "2. Neural Nexus\n",
      "\n",
      "\n",
      "1. LearnLab\n",
      "2. EduConnect\n",
      "3. BrainBoosters\n"
     ]
    }
   ],
   "source": [
    "#2.2 Name Generator\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "prompt = PromptTemplate.from_template(\" Suggest {number} names for a {domain} startup?\") # Create a new instance of the PromptTemplate class.\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt) # Create a new instance of the LLMChain class.\n",
    "chain1 = prompt | llm # Create a new instance of the LLMChain class.\n",
    "chain2 = RunnableSequence(prompt, llm) # Create a new instance of the RunnableSequence class.\n",
    " \n",
    "print(chain.run({'number':'5','domain':'cooking'})) # Run the chain with the input dictionary.\n",
    "print(chain1.invoke({'number':'2','domain':'AI'}))  # Run the chain with the input dictionary.\n",
    "print(chain2.invoke({'number':'3','domain':'EdTech'}))  # Run the chain with the input dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The capital of France is Paris.\n",
      "--------------------\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Name\": \"Ford Models\",\n",
      "        \"Description\": \"Ford Models offers Legal Liability Models (LLMs) for various insurance needs.\",\n",
      "        \"URL\": \"https://www.fordmodels.com/llm\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"XYZ Agency\",\n",
      "        \"Description\": \"XYZ Agency provides customized Legal Liability Models (LLMs) tailored to meet specific client requirements.\",\n",
      "        \"URL\": \"https://www.xyzagency.com/llm\"\n",
      "    },\n",
      "    {\n",
      "        \"Name\": \"ABC Model Agency\",\n",
      "        \"Description\": \"ABC Model Agency specializes in offering Legal Liability Models (LLMs) for businesses and individuals.\",\n",
      "        \"URL\": \"https://www.abcmodelagency.com/llm\"\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# the building blocks\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "# Initialize the ChatOpenAI model with the API key\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of the model\n",
    "response = chat_llm.invoke(template.format(question=\"What is the capital of France?\"))\n",
    "print(response.content)\n",
    "\n",
    "print(\"--------------------\")\n",
    "chatbot = template | chat_llm\n",
    "# use it\n",
    "result = chatbot.invoke({\n",
    "    \"question\": \"Which model providers offer LLMs? Provide the output in Json Format with keys as Name, Description, and URL.\"\n",
    "})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20898,
     "status": "ok",
     "timestamp": 1701320183056,
     "user": {
      "displayName": "mehul gupta",
      "userId": "02075325736316345622"
     },
     "user_tz": -330
    },
    "id": "_jPkuNvfsrI5",
    "outputId": "8a3f1cf5-fe70-4ec5-d467-4711e8759fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "\n",
      "Hey I got out of in Swimming\n",
      "\n",
      "\n",
      "22 13B is my flat no. Rohit will be joining us for the party\n",
      "\n",
      "22 13B flat no rohit join us parti\n"
     ]
    }
   ],
   "source": [
    "#2.3 Text Pre-processing\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Preprocess the given text by following the given steps in sequence. Follow only those steps which have a yes. Remove Number:{number},Remove punctuations:{punc}, Word stemming :{stem}. Output just the preprocessed text. Text:{text}\")\n",
    "\n",
    "\n",
    "chain = prompt | llm # Create a new instance of the LLMChain class.\n",
    "\n",
    "print(chain.invoke({'text':'Hey !! I got 12 out of 20 in Swimming','number':'yes','punc':'yes','stem':'no'}))\n",
    "print(chain.invoke({'text':'22 13B is my flat no. Rohit will be joining us for the party','number':'yes','punc':'no','stem':'yes'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10708,
     "status": "ok",
     "timestamp": 1701320513241,
     "user": {
      "displayName": "mehul gupta",
      "userId": "02075325736316345622"
     },
     "user_tz": -330
    },
    "id": "h6aPmPHgwEnN",
    "outputId": "b4bdd7e4-ba5c-4b23-fbe1-c1ffae3523da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Once there was a coder named Max who was known for his brilliant coding skills\n",
      " He worked alone in his small apartment, spending long hours in front of his computer, completely immersed in his work\n",
      "\n",
      "\n",
      "One night, Max received an email from an unknown sender\n",
      " The subject line simply read \"Help me\"\n",
      " Curiosity getting the best of him, Max opened the email and found a single line of code attached\n",
      " Without hesitation, he copied and pasted the code into his program\n",
      "\n",
      "\n",
      "As soon as he hit enter, Max's computer screen turned black and a chilling voice echoed through his speakers, \"You shouldn't have meddled in things you don't understand, Max\n",
      "\" Fear gripped Max as he realized the code was a virus that had taken over his system\n",
      "\n",
      "\n",
      "Suddenly, a figure appeared on his screen - a ghostly woman with a twisted face, her eyes burning with rage\n",
      " She explained that she was a former coder who had died trying to create the ultimate program\n",
      " Now, she wanted Max to complete her work so that she could finally rest in peace\n",
      "\n",
      "\n",
      "Terrified, Max tried to shut down his computer but it was too late\n",
      " The ghost had taken control and Max was trapped in a virtual nightmare\n",
      " He spent hours trying to find a way out, but the\n",
      "\n",
      "\n",
      "And the Queen died, leaving behind a kingdom in mourning\n",
      " As the people gathered to pay their respects, a young prince named William found himself drawn to a beautiful woman in the crowd\n",
      " Her name was Sophia, and she had tears streaming down her face as she clutched a bouquet of white roses\n",
      "\n",
      "\n",
      "William couldn't take his eyes off of her, and he found himself wanting to comfort her\n",
      " He approached her and offered his condolences, and they began to talk\n",
      " As they talked, they discovered they had a lot in common and a strong connection\n",
      "\n",
      "\n",
      "As the days went by, William and Sophia spent more and more time together, and their bond grew stronger\n",
      " They laughed, they cried, and they fell in love\n",
      " It was as if the Queen's passing brought them together\n",
      "\n",
      "\n",
      "On the one year anniversary of the Queen's death, William took Sophia to the palace gardens, where he had set up a beautiful picnic\n",
      " As they sat and enjoyed the view, William turned to Sophia and said, \"I know this might seem sudden, but I can't imagine spending my life without you\n",
      " Will you do me the honor of becoming my Queen?\"\n",
      "\n",
      "Tears welled up in Sophia's eyes as she nodded, and they shared a passionate kiss\n",
      " The kingdom rejoiced as\n"
     ]
    }
   ],
   "source": [
    "#2.4 Storyteller\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\" Complete a {length} story in using the given beginning. The genre should be {genre} and the story should have an apt ending. Beginning: {text}\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "response1 = chain.invoke({'length': 'very short', 'genre': 'horror', 'text': 'Once there was a coder'})\n",
    "response2 = chain.invoke({'length': 'very short', 'genre': 'rom-com', 'text': 'And the Queen died'})\n",
    "\n",
    "print('\\n'.join(response1['text'].replace('\\n', '.').split('.')))\n",
    "print('\\n'.join(response2['text'].replace('\\n', '.').split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40616,
     "status": "ok",
     "timestamp": 1701321529593,
     "user": {
      "displayName": "mehul gupta",
      "userId": "02075325736316345622"
     },
     "user_tz": -330
    },
    "id": "-DrT7-610Xb0",
    "outputId": "b06ce683-691f-4a69-f850-a82eac980946"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ff7e2e5fa84e76a55f04595070a14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6890d8262628471f9b80dd4984942575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffeda6769f3441793a8714329259a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755896bb36fe46bc9792f0331da58875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f25027a1e342ddb0eb06df14ad00b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7caee9c9ade743238e67084266f961ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45657e2fba646068d0c43ea4fa7df04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e7084142ce4b5ebfb16dff67344ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/var/folders/8v/xkrl1q210t5_4t4hvbx286800000gp/T/ipykernel_9011/1476570883.py:32: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me something about humans that's not so funny\n",
      " I have some weird, weird, weird stuff on my phone\n",
      "\n",
      "\n",
      " I want you to know I love you! You're my best friend! Please don't take me wrong, I am so proud of you! I have been through a lot of things and I love you, too, you're my biggest friend\n",
      "\n",
      "\n",
      "I just got back from a holiday with my family and we got together for a little dinner\n",
      " This is the most amazing meal of the day\n",
      " I think it's a great meal but you should try it anyway\n",
      " I'm sure that you'll be enjoying it more when you eat\n",
      "\n",
      "\n",
      "Here's a few of my favorite foods you can try:\n",
      "\n",
      "Chicken & Rice\n",
      "\n",
      "I know I should have included all the rice I'm making and I think that this is going too fast to be the most delicious dish in my home, I really like that\n",
      " I think it's a great meal that's easy\n"
     ]
    }
   ],
   "source": [
    "#2.5 LangChain using Local LLMs\n",
    "\n",
    "# !pip install transformers==4.35.2 torch==2.1.0+cu121\n",
    "# !pip install einops==0.7.0 accelerate==0.26.1\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "\n",
    "model = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me something about {entity}\")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "chain = LLMChain(llm=llm,prompt=prompt)\n",
    "\n",
    "print('\\n'.join(chain.run('humans').replace('\\n','.').split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 3220,
     "status": "ok",
     "timestamp": 1702965618127,
     "user": {
      "displayName": "mehul gupta",
      "userId": "02075325736316345622"
     },
     "user_tz": -330
    },
    "id": "T0KcO_hlpL9e",
    "outputId": "4460b57b-a944-4728-9977-1137d8179141"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import OpenAIModerationChain\n",
    "\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''\n",
    "\n",
    "huggingface_llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\": 0})\n",
    "huggingface_llm('What is Earth?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11847,
     "status": "ok",
     "timestamp": 1702965722155,
     "user": {
      "displayName": "mehul gupta",
      "userId": "02075325736316345622"
     },
     "user_tz": -330
    },
    "id": "zOyzxvWQpmjn",
    "outputId": "3034a1b4-0508-4636-92e5-3f8264dd7b7c"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"name an {entity}. Output just the name\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "huggingface_llm = HuggingFaceHub(repo_id=\"google/flan-t5-small\", model_kwargs={\"temperature\": 0})\n",
    "\n",
    "chain = LLMChain(llm=huggingface_llm,prompt=prompt)\n",
    "print(chain.run({'entity':'country'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9d3d6AZqF2h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP9gD9TNnnGwcqfmxtFI7cL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
