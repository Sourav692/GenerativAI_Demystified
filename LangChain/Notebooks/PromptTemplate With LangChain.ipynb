{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.3.11 requires langchain<0.4.0,>=0.3.11, but you have langchain 0.3.10 which is incompatible.\n",
      "langchain-huggingface 0.3.0 requires langchain-core<1.0.0,>=0.3.65, but you have langchain-core 0.3.63 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain==0.3.10\n",
    "!pip install -q langchain-openai==0.2.12\n",
    "!pip install -q langchain-community==0.3.11\n",
    "!pip -q install openai==1.55.3\n",
    "!pip -q install faiss-cpu\n",
    "!pip -q install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Basic Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch.\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n",
      "You are an expert in deep learning and PyTorch.\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"You are an expert in deep learning and PyTorch.\n",
    "You answer queries by being brief, bright, and concise.\n",
    "\n",
    "Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "# instantiate using the initializer\n",
    "prompt_template = PromptTemplate(input_variables = ['query'],template = template)\n",
    "prompt_template.pretty_print()\n",
    "\n",
    "# Another way to creating PromptTemplate is to use PromptTemplate.from_template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch.\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: What is the best way to learn PyTorch?\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt\n",
    "formatted_prompt = prompt_template.format(query=\"What is the best way to learn PyTorch?\")\n",
    "print(formatted_prompt)\n",
    "print(type(formatted_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the Prompt to LLM Model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to learn PyTorch is to follow these steps:\n",
      "\n",
      "1. **Official Documentation**: Start with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for foundational concepts and tutorials.\n",
      "\n",
      "2. **Online Courses**: Enroll in courses like \"Deep Learning with PyTorch\" on platforms like Coursera, Udacity, or Fast.ai.\n",
      "\n",
      "3. **Hands-On Projects**: Build small projects or replicate existing ones from GitHub to apply what you learn.\n",
      "\n",
      "4. **Books**: Read books like \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann.\n",
      "\n",
      "5. **Community Engagement**: Join forums like PyTorch Discuss, Stack Overflow, or Reddit to ask questions and share knowledge.\n",
      "\n",
      "6. **Practice Regularly**: Consistently practice coding in PyTorch to reinforce your understanding and skills.\n",
      "\n",
      "By combining these resources, you'll gain a solid grasp of PyTorch effectively.\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChatOpenAI model for text generation\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.0)\n",
    "\n",
    "# Invoke the model with the formatted prompt and get response.Pass the Prompt Value directly to the LLM model\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the Prompt to a Chain - Use PromptTemplate instead of PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to learn PyTorch is to follow these steps:\n",
      "\n",
      "1. **Official Documentation**: Start with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for foundational concepts and tutorials.\n",
      "\n",
      "2. **Online Courses**: Enroll in courses like \"Deep Learning with PyTorch\" on platforms like Coursera, Udacity, or Fast.ai.\n",
      "\n",
      "3. **Hands-On Projects**: Build small projects or replicate existing ones from GitHub to apply what you learn.\n",
      "\n",
      "4. **Books**: Read books like \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann.\n",
      "\n",
      "5. **Community Engagement**: Join forums like PyTorch Discuss, Stack Overflow, or Reddit to ask questions and share knowledge.\n",
      "\n",
      "6. **Practice Regularly**: Consistent coding practice is key. Use Kaggle for datasets and challenges.\n",
      "\n",
      "By combining these resources, you'll gain a solid understanding of PyTorch.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"query\": \"What is the best way to learn PyTorch?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Input Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Movie Title:** The Dark Knight: Shadows of Gotham\n",
      "\n",
      "**Genre:** Action/Thriller\n",
      "\n",
      "**Synopsis:**\n",
      "In the heart of Gotham City, a new wave of crime threatens to engulf the streets as a mysterious figure known only as \"The Puppeteer\" emerges, manipulating the city's underworld with a sinister agenda. With the Joker locked away and the city still reeling from past chaos, Batman (Christian Bale) finds himself facing a foe unlike any he has encountered beforeâ€”one who knows how to exploit the fears and weaknesses of both the criminals and the citizens of Gotham.\n",
      "\n",
      "As The Puppeteer orchestrates a series of high-stakes heists and public spectacles, Batman must navigate a web of deception that tests his resolve and moral code. With the help of Commissioner Gordon and a new ally, the tech-savvy vigilante known as Oracle, Batman races against time to uncover the identity of The Puppeteer before the city descends into total anarchy.\n",
      "\n",
      "As the stakes rise, Batman is forced to confront his own demons and the consequences of his dual life. The line between hero and villain blurs as he grapples with the question: how far is he willing to go to save Gotham? In a thrilling climax that pits mind against might, Batman must outsmart The Puppeteer in a battle that will determine the fate of the city he has sworn to protect.\n",
      "\n",
      "With stunning visuals, heart-pounding action, and a gripping narrative, \"The Dark Knight: Shadows of Gotham\" explores the depths of fear, the nature of justice, and the enduring struggle between light and darkness.\n"
     ]
    }
   ],
   "source": [
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template = \"\"\"Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\"\"\",\n",
    "        input_variables = ['movie_title', 'main_actor']\n",
    "        \n",
    "    )\n",
    "    \n",
    "    formatted_prompt = prompt.format(movie_title = movie_title, main_actor = main_actor)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "movie_info = get_movie_information(\"The Dark Knight\", \"Christian Bale\")\n",
    "print(movie_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the same function usingwith_template and using LCEL | operato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Movie Title:** Amritsar: 1984\n",
      "\n",
      "**Genre:** Historical Drama / Thriller\n",
      "\n",
      "**Synopsis:**\n",
      "\n",
      "Set against the backdrop of the turbulent events of 1984 in India, \"Amritsar: 1984\" follows the gripping journey of Harpreet Singh (played by Gurdaas Mann), a devoted schoolteacher and a loving father living in the heart of Amritsar. As the city becomes a focal point of political unrest and communal tension, Harpreet finds himself torn between his love for his family and his commitment to his community.\n",
      "\n",
      "When the Indian government launches Operation Blue Star to remove militant leader Jarnail Singh Bhindranwale from the Golden Temple, Harpreet's life is turned upside down. As violence erupts and the sacred site becomes a battleground, he must navigate the chaos while trying to protect his family and uphold his values. \n",
      "\n",
      "Haunted by the memories of his childhood spent in the temple, Harpreet embarks on a perilous quest to rescue his estranged brother, who has been drawn into the conflict. Along the way, he encounters a diverse cast of characters, including a courageous journalist seeking the truth, a soldier grappling with his orders, and a young activist determined to fight for justice.\n",
      "\n",
      "As the situation escalates, Harpreet's resolve is tested, forcing him to confront his beliefs about faith, loyalty, and sacrifice. The film culminates in a heart-wrenching climax that challenges the very fabric of humanity, leaving audiences to ponder the cost of conflict and the power of forgiveness.\n",
      "\n",
      "\"Amritsar: 1984\" is a poignant exploration of love, loss, and resilience, showcasing Gurdaas Mann's powerful performance as he brings to life a character caught in the crossfire of history. The film serves as a reminder of the enduring spirit of those who strive for peace amidst turmoil.**Movie Title:** Amritsar: 1984\n",
      "\n",
      "**Genre:** Historical Drama / Thriller\n",
      "\n",
      "**Synopsis:**\n",
      "Set against the backdrop of the turbulent events of 1984 in India, \"Amritsar: 1984\" follows the gripping journey of Harpreet Singh (played by Gurdaas Mann), a devoted schoolteacher and a loving father living in the heart of Amritsar. As the city becomes a focal point of political unrest and communal tension, Harpreet finds himself torn between his love for his family and his commitment to his community.\n",
      "\n",
      "When the Indian government launches Operation Blue Star to remove militant leader Jarnail Singh Bhindranwale from the Golden Temple, Harpreet's life is turned upside down. As violence erupts and the sacred site becomes a battleground, he must navigate the chaos while trying to protect his family and uphold his values. \n",
      "\n",
      "Haunted by the memories of his childhood spent in the temple, Harpreet embarks on a perilous journey to rescue his estranged brother, who has been drawn into the conflict. Along the way, he encounters a diverse cast of characters, including a courageous journalist seeking the truth, a soldier grappling with his orders, and a young activist determined to fight for justice.\n",
      "\n",
      "As the situation escalates, Harpreet must confront his own beliefs and the harsh realities of a divided nation. \"Amritsar: 1984\" is a poignant exploration of love, sacrifice, and the quest for peace amidst the storm of history, showcasing Gurdaas Mann's powerful performance as a man caught in the crossfire of fate and duty. \n",
      "\n",
      "With stunning cinematography and a haunting score, the film captures the essence of a pivotal moment in Indian history, leaving audiences with a profound reflection on the cost of conflict and the enduring spirit of humanity.\n"
     ]
    }
   ],
   "source": [
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
    "\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    for chunk in llm_chain.stream({\"movie_title\":movie_title,\"main_actor\":main_actor}):\n",
    "      print(chunk, end=\"\", flush=True)\n",
    "    response = llm_chain.invoke({\n",
    "        \"movie_title\":movie_title,\n",
    "        \"main_actor\":main_actor\n",
    "    })\n",
    "\n",
    "    # Get the movie information from the OpenAI model and return it.\n",
    "    return response\n",
    "    \n",
    "print(get_movie_information(movie_title=\"Amritsar:1984\", main_actor=\"Gurdaas Mann\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ§± 1. PromptTemplate (for Text Completion LLMs)**\n",
    "\n",
    "### **âœ… What it does:**\n",
    "\n",
    "Builds a **formatted prompt** with placeholders.\n",
    "\n",
    "### **âœ… When to use:**\n",
    "\n",
    "When using text-davinci, gpt-3, Claude, or any **non-chat LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate this sentence to French: I love programming.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt with variables\n",
    "prompt = PromptTemplate.from_template(\"Translate this sentence to French: {sentence}\")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt.format(sentence=\"I love programming.\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ’¬ 2. ChatPromptTemplate (for Chat Models like GPT-3.5, GPT-4)**\n",
    "\n",
    "### **âœ… What it does:**\n",
    "\n",
    "Creates **multi-turn prompts** in a chat format using **roles** like system, user, and ai.\n",
    "\n",
    "### **âœ… When to use:**\n",
    "\n",
    "When using **OpenAIâ€™s chat models**, like GPT-3.5/4 (gpt-4), **Anthropic Claude**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant that translates text to French.\n",
      "Human: Translate this sentence: I love programming.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat prompt template with system and user messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates text to French.\"),\n",
    "    (\"user\", \"Translate this sentence: {sentence}\")\n",
    "])\n",
    "\n",
    "# Format the chat prompt\n",
    "formatted_chat_prompt = chat_prompt.format(sentence=\"I love programming.\")\n",
    "print(formatted_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful assistant that translates text to French.' additional_kwargs={} response_metadata={}\n",
      "content='Translate this sentence: I love programming.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "# Format the chat prompt\n",
    "formatted_chat_prompt = chat_prompt.invoke({\"sentence\":\"I love programming.\"})\n",
    "for msg in formatted_chat_prompt.messages:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates text to French.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Translate this sentence: I love programming.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_chat_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompt_values.ChatPromptValue"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format_prompt(sentence=\"I love programming.\")\n",
    "type(formatted_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example of ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Yo! Wassup nephew.\"),\n",
    "    (\"ai\", \"As an AI language model, I am incapable of being your nephew.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(template)\n",
    "# langchain_core.prompts.chat.ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'user_input']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name', 'user_input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Yo! Wassup nephew.'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='As an AI language model, I am incapable of being your nephew.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Yo! Wassup nephew.'), additional_kwargs={}),\n",
       " AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='As an AI language model, I am incapable of being your nephew.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is Robotalker.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yo! Wassup nephew.', additional_kwargs={}, response_metadata={}), AIMessage(content='As an AI language model, I am incapable of being your nephew.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Talk robo to me!', additional_kwargs={}, response_metadata={})]\n",
      "Beep boop! Initiating RoboTalk mode! ðŸ¤– Whatâ€™s on your mind, human? Need some data, a joke, or maybe a fun fact? Just say the word!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Beep boop! Initiating RoboTalk mode! ðŸ¤– Whatâ€™s on your mind, human? Need some data, a joke, or maybe a fun fact? Just say the word!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = template.format_messages(\n",
    "    name=\"Robotalker\",\n",
    "    user_input=\"Talk robo to me!\"\n",
    ")\n",
    "print(messages)\n",
    "\n",
    "# [SystemMessage(content='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is Robotalker.'),\n",
    "# HumanMessage(content='Yo! Wassup nephew.'),\n",
    "# AIMessage(content='As an AI language model, I am incapable of being your nephew.'),\n",
    "#HumanMessage(content='Talk robo to me!')]\n",
    "\n",
    "print(llm.invoke(messages).content)\n",
    "# Beep boop! Let's chat about all things robotic and techy. Got any burning questions about robots or artificial intelligence?\n",
    "\n",
    "# use LCEL\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"})\n",
    "# \"Beep boop! What's shakin', human friend?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\", additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Another way to create Template using SystemMessage and HumanMessagePromptTemplate\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "template = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables\n",
    "# ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, thatâ€™s your opinion, isnâ€™t it? But honestly, who cares? If you donâ€™t like Sam I Am, just move on. There are plenty of other characters out there.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(template.format_messages(text=\"That Sam I Am, I do not like that Sam I Am...\"))\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well, tough luck! Dr. Seuss isn't going to write itself, you know. If you don't like Sam I Am, maybe you should just find something else to read. What do you want, a medal for your opinion?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | llm | StrOutputParser()\n",
    "chain.invoke({\"text\":\"That Sam I Am, I do not like that Sam I Am...\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate vs PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PromptTemplate ---\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='Write a joke about {topic}'\n",
      "\n",
      "--- PromptValue ---\n",
      "text='Write a joke about penguins'\n",
      "\n",
      "--- Final Prompt String ---\n",
      "Write a joke about penguins\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Write a joke about {topic}\")\n",
    "\n",
    "# Step 2: Use format_prompt() to create a PromptValue\n",
    "prompt_value = prompt_template.format_prompt(topic=\"penguins\")\n",
    "\n",
    "print(\"--- PromptTemplate ---\")\n",
    "print(prompt_template)\n",
    "print(\"\\n--- PromptValue ---\")\n",
    "print(prompt_value)\n",
    "print(\"\\n--- Final Prompt String ---\")\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ§  When to Use Each**\n",
    "\n",
    "- **Use PromptTemplate**:\n",
    "    - When designing **reusable and parameterized prompts**.\n",
    "    - For building **dynamic chains** with variables.\n",
    "- **Use PromptValue**:\n",
    "    - When you want to **pass a fully formatted prompt** to an LLM manually.\n",
    "    - When chaining LLMs directly: llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ§ª BONUS: Use PromptValue directly with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do ducks have feathers?\n",
      "\n",
      "To cover their butt quacks!\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {animal}\")\n",
    "])\n",
    "\n",
    "# Create a PromptValue (chat prompt formatted)\n",
    "prompt_value = chat_prompt.format_prompt(animal=\"ducks\")\n",
    "\n",
    "# Send PromptValue to LLM directly\n",
    "response = llm.invoke(prompt_value)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using format_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "\n",
    "# Convert to PromptValue\n",
    "prompt_value = template.format_prompt(country=\"France\")\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompt_values.StringPromptValue"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using format() â†’ Return Str not a PromptValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Germany?\n"
     ]
    }
   ],
   "source": [
    "formatted_string = template.format(country=\"Germany\")\n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(formatted_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ChatPromptTemplate:Â .format_prompt() â†’ returns ChatPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You're a helpful assistant.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about penguins', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "chat_prompt_value = chat_template.format_prompt(topic=\"penguins\")\n",
    "\n",
    "print(chat_prompt_value.to_messages())  # List of ChatMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompt_values.ChatPromptValue"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You're a helpful assistant.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about penguins', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt_value.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Invoke()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "Write a poem about stars\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write a poem about {topic}\")\n",
    "\n",
    "# Using invoke to get PromptValue\n",
    "prompt_value = prompt.invoke({\"topic\": \"stars\"})\n",
    "print(type(prompt_value))\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You're a helpful assistant.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about ducks', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {animal}\")\n",
    "])\n",
    "\n",
    "# Using invoke\n",
    "chat_prompt_value = chat_prompt.invoke({\"animal\": \"ducks\"})\n",
    "print(chat_prompt_value.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use Inside LCEL Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a cat joke for you:\n",
      "\n",
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = chat_template | llm | parser\n",
    "print(chain.invoke({\"topic\": \"cats\"}))  # `format_prompt()` happens internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
