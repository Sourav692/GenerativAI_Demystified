{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd\n"
     ]
    }
   ],
   "source": [
    "chain = RunnablePassthrough()\n",
    "print(chain.invoke(\"abcd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def output_length(input: str):\n",
    "    output = len(input)\n",
    "    return output\n",
    "\n",
    "chain = RunnableLambda(output_length)\n",
    "print(chain.invoke(\"input to output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum(item: dict):\n",
    "    return item[\"a\"]+item[\"b\"]\n",
    "\n",
    "chain = RunnableLambda(sum)\n",
    "chain.invoke({\"a\":1,\"b\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def sum_values(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "# Wrap the function to accept a dict and unpack it\n",
    "chain = RunnableLambda(lambda item: sum_values(item[\"a\"], item[\"b\"]))\n",
    "\n",
    "# Invoke with dictionary input\n",
    "result = chain.invoke({\"a\": 1, \"b\": 2})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text1': 'start-tech academy', 'length': 18}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel(text1 = RunnablePassthrough(), length = RunnableLambda(output_length))\n",
    "chain.invoke(\"start-tech academy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# create a prompt template to accept user queries\n",
    "prompt_txt = \"{query}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_txt)\n",
    "\n",
    "# the chain has been formatted for better readability\n",
    "# you could also write this as llmchain = prompt_template | chatgpt\n",
    "llmchain = (prompt_template\n",
    "              |\n",
    "           chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to algorithms that can create new content, such as text, images, or music, by learning patterns from existing data.\n"
     ]
    }
   ],
   "source": [
    "response = llmchain.invoke({'query' : 'Explain Generative AI in 1 line'})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Sourav | Role: Solution Architect | Location: Bengaluru\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def format_user_data(user_info):\n",
    "    \"\"\"Process user information for downstream components\"\"\"\n",
    "    return f\"User: {user_info['name']} | Role: {user_info['role']} | Location: {user_info['location']}\"\n",
    "\n",
    "# Convert function to Runnable\n",
    "formatter = RunnableLambda(format_user_data)\n",
    "\n",
    "# Execute the runnable\n",
    "user_data = {\"name\": \"Sourav\", \"role\": \"Solution Architect\", \"location\": \"Bengaluru\"}\n",
    "result = formatter.invoke(user_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassthrough - Preserving Input Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text \"Building scalable data platforms requires careful architecture planning\" emphasizes the importance of strategic design in the development of data platforms that can grow and adapt to increasing demands. \n",
      "\n",
      "### Analysis:\n",
      "\n",
      "1. **Keywords**:\n",
      "   - **Building**: This suggests an active process of creation and development, indicating that constructing data platforms is not a passive task but requires effort and expertise.\n",
      "   - **Scalable**: This is a critical term in technology and data management, referring to the ability of a system to handle growth, whether in terms of data volume, user load, or functionality. Scalability is essential for ensuring that a platform can evolve without requiring a complete redesign.\n",
      "   - **Platforms**: This term indicates a foundational technology or framework that supports various applications or services. In the context of data, it implies a comprehensive system that manages data storage, processing, and analysis.\n",
      "\n",
      "2. **Key Themes**:\n",
      "   - **Architecture Planning**: The phrase highlights the necessity of thoughtful and strategic planning in the architecture of data platforms. This involves considering various factors such as data flow, storage solutions, processing capabilities, and user access.\n",
      "   - **Scalability Challenges**: The text implies that without careful planning, a data platform may struggle to scale effectively, leading to performance issues or the need for costly overhauls in the future.\n",
      "\n",
      "3. **Implications**:\n",
      "   - Organizations looking to build data platforms must invest time and resources into the architectural design phase to ensure long-term success and adaptability.\n",
      "   - The focus on scalability suggests that businesses should anticipate future growth and design their platforms accordingly, rather than merely addressing current needs.\n",
      "\n",
      "### Conclusion:\n",
      "Overall, the text serves as a reminder of the complexities involved in creating data platforms and the critical role of architecture in ensuring that these platforms can scale effectively to meet future demands.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Simple keyword extraction\"\"\"\n",
    "    words = text.lower().split()\n",
    "    keywords = [word for word in words if len(word) > 4]\n",
    "    return keywords[:3]  # Top 3 keywords\n",
    "\n",
    "# Create components\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze this text: {original_text}\\nKeywords found: {keywords}\"\n",
    ")\n",
    "\n",
    "# Build chain that preserves original input and adds keywords\n",
    "chain = {\n",
    "    \"original_text\": RunnablePassthrough(),\n",
    "    \"keywords\": RunnableLambda(extract_keywords)\n",
    "} | prompt | chatgpt\n",
    "\n",
    "result = chain.invoke(\"Building scalable data platforms requires careful architecture planning\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chaining with Pipe Operator (|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Analysis Summary:\n",
      "- Characters: 119\n",
      "- Words: 18  \n",
      "- Sentences: 3\n",
      "- Avg words per sentence: 6.0\n",
      "Original: \"Data engineering is crucial for ML success. It inv...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def validate_input(data):\n",
    "    \"\"\"Validate and clean input data\"\"\"\n",
    "    if not isinstance(data, str) or len(data.strip()) == 0:\n",
    "        raise ValueError(\"Input must be a non-empty string\")\n",
    "    return data.strip()\n",
    "\n",
    "def extract_metrics(text):\n",
    "    \"\"\"Extract text metrics\"\"\"\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"character_count\": len(text),\n",
    "        \"word_count\": len(text.split()),\n",
    "        \"sentence_count\": text.count('.') + text.count('!') + text.count('?')\n",
    "    }\n",
    "\n",
    "def generate_summary(metrics):\n",
    "    \"\"\"Generate a summary report\"\"\"\n",
    "    return f\"\"\"\n",
    "Text Analysis Summary:\n",
    "- Characters: {metrics['character_count']}\n",
    "- Words: {metrics['word_count']}  \n",
    "- Sentences: {metrics['sentence_count']}\n",
    "- Avg words per sentence: {metrics['word_count'] / max(metrics['sentence_count'], 1):.1f}\n",
    "Original: \"{metrics['original_text'][:50]}...\"\n",
    "\"\"\"\n",
    "\n",
    "# Create sequential chain\n",
    "analysis_chain = (\n",
    "    RunnableLambda(validate_input) |\n",
    "    RunnableLambda(extract_metrics) |\n",
    "    RunnableLambda(generate_summary)\n",
    ")\n",
    "\n",
    "# Execute the chain\n",
    "sample_text = \"Data engineering is crucial for ML success. It involves building robust pipelines. Quality data leads to better models.\"\n",
    "result = analysis_chain.invoke(sample_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution with RunnableParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Analysis Results:\n",
      "  technical_analysis: {'technical_terms': ['data', 'platform', 'architecture', 'pipeline', 'model'], 'technical_density': 27.77777777777778}\n",
      "  sentiment: Positive\n",
      "  entities: ['The', 'Databricks']\n",
      "  word_count: 18\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "import re\n",
    "\n",
    "def analyze_technical_content(text):\n",
    "    \"\"\"Analyze technical aspects of text\"\"\"\n",
    "    tech_terms = ['data', 'platform', 'architecture', 'pipeline', 'model', 'algorithm']\n",
    "    found_terms = [term for term in tech_terms if term.lower() in text.lower()]\n",
    "    return {\n",
    "        \"technical_terms\": found_terms,\n",
    "        \"technical_density\": len(found_terms) / len(text.split()) * 100\n",
    "    }\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Simple sentiment analysis\"\"\"\n",
    "    positive_words = ['good', 'great', 'excellent', 'success', 'efficient', 'robust']\n",
    "    negative_words = ['bad', 'poor', 'failed', 'problem', 'issue', 'difficult']\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_words if word in text.lower())\n",
    "    neg_count = sum(1 for word in negative_words if word in text.lower())\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return \"Positive\"\n",
    "    elif neg_count > pos_count:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract potential entities (simplified)\"\"\"\n",
    "    # Simple pattern matching for demonstration\n",
    "    entities = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "    return list(set(entities))\n",
    "\n",
    "# Create parallel analysis pipeline\n",
    "parallel_analyzer = RunnableParallel(\n",
    "    technical_analysis=RunnableLambda(analyze_technical_content),\n",
    "    sentiment=RunnableLambda(analyze_sentiment),\n",
    "    entities=RunnableLambda(extract_entities),\n",
    "    word_count=RunnableLambda(lambda x: len(x.split()))\n",
    ")\n",
    "\n",
    "# Test the parallel execution\n",
    "sample_text = \"Databricks provides an excellent platform for building robust data pipelines. The architecture supports efficient model training and deployment.\"\n",
    "\n",
    "result = parallel_analyzer.invoke(sample_text)\n",
    "print(\"Parallel Analysis Results:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Logic with RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I optimize Spark pipelines for better performance?\n",
      "Response: üîß Data Engineering Response: This query about 'How do I optimize Spark pipelines for better performance?' relates to building and maintaining data pipelines and infrastructure.\n",
      "\n",
      "Query: What's the best algorithm for classification problems?\n",
      "Response: ü§ñ ML Response: This query about 'What's the best algorithm for classification problems?' involves machine learning models and algorithms.\n",
      "\n",
      "Query: How to deploy models on Azure ML?\n",
      "Response: ü§ñ ML Response: This query about 'How to deploy models on Azure ML?' involves machine learning models and algorithms.\n",
      "\n",
      "Query: What's the weather like today?\n",
      "Response: üí° General Response: This is a general query about 'What's the weather like today?'. Please provide more specific context.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "\n",
    "def is_data_engineering_query(text):\n",
    "    \"\"\"Check if query is about data engineering\"\"\"\n",
    "    de_keywords = ['pipeline', 'etl', 'data warehouse', 'spark', 'kafka', 'airflow']\n",
    "    return any(keyword in text.lower() for keyword in de_keywords)\n",
    "\n",
    "def is_ml_query(text):\n",
    "    \"\"\"Check if query is about machine learning\"\"\"\n",
    "    ml_keywords = ['model', 'algorithm', 'training', 'prediction', 'classification', 'regression']\n",
    "    return any(keyword in text.lower() for keyword in ml_keywords)\n",
    "\n",
    "def is_cloud_query(text):\n",
    "    \"\"\"Check if query is about cloud platforms\"\"\"\n",
    "    cloud_keywords = ['aws', 'azure', 'gcp', 'cloud', 'kubernetes', 'docker']\n",
    "    return any(keyword in text.lower() for keyword in cloud_keywords)\n",
    "\n",
    "def handle_data_engineering(text):\n",
    "    \"\"\"Specialized handler for data engineering queries\"\"\"\n",
    "    return f\"üîß Data Engineering Response: This query about '{text}' relates to building and maintaining data pipelines and infrastructure.\"\n",
    "\n",
    "def handle_ml_query(text):\n",
    "    \"\"\"Specialized handler for ML queries\"\"\"\n",
    "    return f\"ü§ñ ML Response: This query about '{text}' involves machine learning models and algorithms.\"\n",
    "\n",
    "def handle_cloud_query(text):\n",
    "    \"\"\"Specialized handler for cloud queries\"\"\"\n",
    "    return f\"‚òÅÔ∏è Cloud Response: This query about '{text}' concerns cloud platforms and services.\"\n",
    "\n",
    "def handle_general_query(text):\n",
    "    \"\"\"Default handler for general queries\"\"\"\n",
    "    return f\"üí° General Response: This is a general query about '{text}'. Please provide more specific context.\"\n",
    "\n",
    "# Create conditional routing\n",
    "query_router = RunnableBranch(\n",
    "    (RunnableLambda(is_data_engineering_query), RunnableLambda(handle_data_engineering)),\n",
    "    (RunnableLambda(is_ml_query), RunnableLambda(handle_ml_query)),\n",
    "    (RunnableLambda(is_cloud_query), RunnableLambda(handle_cloud_query)),\n",
    "    RunnableLambda(handle_general_query)  # Default case\n",
    ")\n",
    "\n",
    "# Test different types of queries\n",
    "test_queries = [\n",
    "    \"How do I optimize Spark pipelines for better performance?\",\n",
    "    \"What's the best algorithm for classification problems?\",\n",
    "    \"How to deploy models on Azure ML?\",\n",
    "    \"What's the weather like today?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = query_router.invoke(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming processing:\n",
      "  Processed: BUILDING\n",
      "  Processed: BUILDING SCALABLE\n",
      "  Processed: BUILDING SCALABLE DATA\n",
      "  Processed: BUILDING SCALABLE DATA SOLUTIONS\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "import time\n",
    "\n",
    "def streaming_processor(text):\n",
    "    \"\"\"Process text and yield results incrementally\"\"\"\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        processed_words.append(word.upper())\n",
    "        # Simulate processing time\n",
    "        time.sleep(0.1)\n",
    "        # Yield intermediate result\n",
    "        yield f\"Processed: {' '.join(processed_words)}\"\n",
    "\n",
    "# Create streaming runnable\n",
    "streaming_runnable = RunnableLambda(streaming_processor)\n",
    "\n",
    "# Stream the results\n",
    "print(\"Streaming processing:\")\n",
    "for chunk in streaming_runnable.stream(\"building scalable data solutions\"):\n",
    "    print(f\"  {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Analysis Results:\n",
      "  Query 1: {'query': 'Hello', 'word_count': 1, 'complexity': 'Simple'}\n",
      "  Query 2: {'query': 'How do I setup Databricks?', 'word_count': 5, 'complexity': 'Medium'}\n",
      "  Query 3: {'query': 'What are the best practices fo...', 'word_count': 21, 'complexity': 'Complex'}\n",
      "  Query 4: {'query': 'Spark optimization tips', 'word_count': 3, 'complexity': 'Simple'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def analyze_query_complexity(query):\n",
    "    \"\"\"Analyze the complexity of a user query\"\"\"\n",
    "    word_count = len(query.split())\n",
    "    char_count = len(query)\n",
    "    \n",
    "    if word_count <= 3:\n",
    "        complexity = \"Simple\"\n",
    "    elif word_count <= 10:\n",
    "        complexity = \"Medium\"\n",
    "    else:\n",
    "        complexity = \"Complex\"\n",
    "    \n",
    "    return {\n",
    "        \"query\": query[:30] + \"...\" if len(query) > 30 else query,\n",
    "        \"word_count\": word_count,\n",
    "        \"complexity\": complexity\n",
    "    }\n",
    "\n",
    "# Create analyzer\n",
    "complexity_analyzer = RunnableLambda(analyze_query_complexity)\n",
    "\n",
    "# Process multiple queries at once\n",
    "queries = [\n",
    "    \"Hello\",\n",
    "    \"How do I setup Databricks?\",\n",
    "    \"What are the best practices for designing a data lake architecture that can handle both batch and streaming data processing workloads?\",\n",
    "    \"Spark optimization tips\"\n",
    "]\n",
    "\n",
    "# Batch process all queries\n",
    "results = complexity_analyzer.batch(queries)\n",
    "\n",
    "print(\"Batch Analysis Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"  Query {i}: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnableLambda\n",
    "RunnableLambda wraps any Python function to make it compatible with LangChain chains. This is perfect for integrating custom business logic with AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In below code clean_user_input() take str as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'apache spark'}\n",
      "AI Response: Apache Spark is an open-source, distributed computing system that is designed for big data processing and analytics. It provides a fast and general-purpose cluster computing framework for large-scale data processing tasks. Spark is known for its speed and ease of use, as well as its ability to handle a wide range of workloads, including batch processing, real-time streaming, machine learning, and graph processing.\n",
      "\n",
      "Spark uses a concept called Resilient Distributed Datasets (RDDs) to store and process data across multiple nodes in a cluster. RDDs are fault-tolerant, immutable collections of objects that can be operated on in parallel. Spark also provides a rich set of APIs in multiple programming languages, such as Scala, Java, Python, and R, making it accessible to a wide range of developers.\n",
      "\n",
      "One of the key features of Apache Spark is its ability to perform in-memory processing, which allows it to achieve high performance by caching data in memory and reusing it across multiple computations. This makes Spark well-suited for iterative algorithms and interactive data analysis.\n",
      "\n",
      "Overall, Apache Spark is a powerful and versatile tool for processing large volumes of data efficiently and quickly, making it a popular choice for organizations looking to harness the power of big data analytics.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "def clean_user_input(text):\n",
    "    \"\"\"Clean and format user input for better LLM processing\"\"\"\n",
    "    clean_text = text[\"topic\"].strip().lower()\n",
    "    return {\"topic\": clean_text}\n",
    "\n",
    "# Convert function to Runnable\n",
    "input_cleaner = RunnableLambda(clean_user_input)\n",
    "print(input_cleaner.invoke({\"topic\": \"  APACHE SPARK  \"}))\n",
    "\n",
    "# Create a simple AI chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain this concept clearly: {topic}\")\n",
    "simple_chain = input_cleaner | prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "user_input = \"  APACHE SPARK  \"\n",
    "result = simple_chain.invoke({\"topic\": user_input})\n",
    "print(f\"AI Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In below code clean_user_input() take str as Input Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache spark\n",
      "AI Response: Apache Spark is an open-source, distributed computing system that is designed for big data processing and analytics. It provides a fast and general-purpose cluster computing framework for large-scale data processing. Spark allows users to write applications in Java, Scala, Python, and R, and provides high-level APIs in these languages for ease of use.\n",
      "\n",
      "One of the key features of Apache Spark is its ability to perform in-memory processing, which allows it to process data much faster than traditional disk-based systems. Spark also supports a wide range of data processing tasks, including batch processing, real-time streaming, machine learning, and graph processing.\n",
      "\n",
      "Spark is built around the concept of Resilient Distributed Datasets (RDDs), which are fault-tolerant collections of data that can be operated on in parallel across a cluster of machines. Spark automatically distributes the data and computation across the cluster, making it easy to scale up and down as needed.\n",
      "\n",
      "Overall, Apache Spark is a powerful and flexible tool for processing and analyzing large volumes of data, making it a popular choice for organizations looking to harness the power of big data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "def clean_user_input(text):\n",
    "    \"\"\"Clean and format user input for better LLM processing\"\"\"\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Convert function to Runnable\n",
    "input_cleaner = RunnableLambda(clean_user_input)\n",
    "print(input_cleaner.invoke( \" APACHE SPARK  \"))\n",
    "\n",
    "# Create a simple AI chain\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain this concept clearly: {topic}\")\n",
    "simple_chain = {\"topic\": input_cleaner\n",
    "                } | prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "user_input = \"  APACHE SPARK  \"\n",
    "result = simple_chain.invoke(user_input)\n",
    "print(f\"AI Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunnablePassthrough - Preserving Original Context\n",
    "RunnablePassthrough keeps the original input available while simultaneously processing it through other functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: How do I build data pipelines using Spark for cloud architecture?\n",
      "Keywords found: ['data', 'spark', 'pipeline', 'cloud', 'architecture']\n",
      "AI Analysis: To build data pipelines using Spark for cloud architecture, you can leverage the capabilities of Spark's distributed processing framework to efficiently process and analyze large volumes of data in a cloud environment. \n",
      "\n",
      "First, you would need to design a pipeline that outlines the flow of data from source to destination, including any transformations or processing steps along the way. Spark provides a high-level API for building data pipelines, allowing you to easily define and execute complex data processing workflows.\n",
      "\n",
      "Next, you can deploy your Spark application on a cloud platform such as A...\n"
     ]
    }
   ],
   "source": [
    "def extract_keywords(text):\n",
    "    \"\"\"Extract key technical terms from text\"\"\"\n",
    "    technical_terms = ['data', 'spark', 'pipeline', 'model', 'cloud', 'architecture']\n",
    "    words = text.lower().split()\n",
    "    found_terms = [term for term in technical_terms if term in ' '.join(words)]\n",
    "    return found_terms\n",
    "\n",
    "# Create prompt that uses both original text and extracted keywords\n",
    "analysis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Original query: \"{original_text}\"\n",
    "Technical terms found: {keywords}\n",
    "\n",
    "Provide a focused technical explanation based on these key terms.\n",
    "\"\"\")\n",
    "\n",
    "# Chain that preserves original input and adds keyword analysis\n",
    "context_chain = {\n",
    "    \"original_text\": RunnablePassthrough(),\n",
    "    \"keywords\": RunnableLambda(extract_keywords)\n",
    "} | analysis_prompt | chatgpt | StrOutputParser()\n",
    "\n",
    "# Test with technical query\n",
    "tech_query = \"How do I build data pipelines using Spark for cloud architecture?\"\n",
    "result = context_chain.invoke(tech_query)\n",
    "print(f\"Original: {tech_query}\")\n",
    "print(f\"Keywords found: {extract_keywords(tech_query)}\")\n",
    "print(f\"AI Analysis: {result[:600]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chaining with Pipe Operator (|)\n",
    "The pipe operator creates sequential workflows where data flows from left to right through multiple processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: 1. Partitioning: Ensure that your data is properly partitioned to distribute the workload evenly across the cluster. Use the `repartition()` or `coale...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: The best algorithm for classification can vary depending on the specific characteristics of the dataset and the problem at hand. Some commonly used al...\n",
      "\n",
      "Question: How to deploy on Azure Kubernetes Service?\n",
      "Category: Cloud Platform\n",
      "Expert Response: To deploy on Azure Kubernetes Service (AKS) as a cloud architect, you would follow these steps:\n",
      "\n",
      "1. Create an Azure account: If you don't already have...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def categorize_query(text):\n",
    "    \"\"\"Categorize technical queries\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    if any(word in text_lower for word in ['spark', 'hadoop', 'etl', 'pipeline']):\n",
    "        return \"Data Engineering\"\n",
    "    elif any(word in text_lower for word in ['model', 'algorithm', 'training']):\n",
    "        return \"Machine Learning\"\n",
    "    elif any(word in text_lower for word in ['azure', 'aws', 'cloud']):\n",
    "        return \"Cloud Platform\"\n",
    "    return \"General\"\n",
    "\n",
    "def create_expert_prompt(data):\n",
    "    \"\"\"Create specialized prompts based on category\"\"\"\n",
    "    category = data['category']\n",
    "    question = data['question']\n",
    "    \n",
    "    expert_prompts = {\n",
    "        \"Data Engineering\": f\"As a data engineering expert, provide technical guidance for: {question}\",\n",
    "        \"Machine Learning\": f\"As an ML specialist, explain the concepts related to: {question}\",\n",
    "        \"Cloud Platform\": f\"As a cloud architect, describe the solution for: {question}\",\n",
    "        \"General\": f\"Provide a comprehensive answer to: {question}\"\n",
    "    }\n",
    "    \n",
    "    return expert_prompts[category]\n",
    "\n",
    "# Build sequential expert chain\n",
    "expert_chain = (\n",
    "    RunnableLambda(lambda x: {\"question\": x, \"category\": categorize_query(x)}) |\n",
    "    RunnableLambda(create_expert_prompt) |\n",
    "    chatgpt |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to optimize Spark job performance?\n",
      "Category: Data Engineering\n",
      "Expert Response: 1. Partitioning: Ensure that your data is properly partitioned before running a Spark job. This helps in distributing the workload evenly across the c...\n",
      "\n",
      "Question: What's the best algorithm for classification?\n",
      "Category: Machine Learning\n",
      "Expert Response: The best algorithm for classification depends on various factors such as the nature of the data, the size of the dataset, the complexity of the proble...\n",
      "\n",
      "Question: How to deploy on Azure Kubernetes Service?\n",
      "Category: Cloud Platform\n",
      "Expert Response: To deploy on Azure Kubernetes Service (AKS) as a cloud architect, you would follow these steps:\n",
      "\n",
      "1. Create an Azure account: If you don't already have...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another way to create the chain\n",
    "\n",
    "# Build sequential expert chain\n",
    "expert_chain = (\n",
    "    {\"question\": RunnablePassthrough(),\"category\": RunnableLambda(categorize_query)} |\n",
    "    RunnableLambda(create_expert_prompt) |\n",
    "    chatgpt |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test different question types\n",
    "test_questions = [\n",
    "    \"How to optimize Spark job performance?\",\n",
    "    \"What's the best algorithm for classification?\",\n",
    "    \"How to deploy on Azure Kubernetes Service?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    category = categorize_query(question)\n",
    "    result = expert_chain.invoke(question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Expert Response: {result[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Processing with RunnableParallel\n",
    "RunnableParallel executes multiple operations simultaneously on the same input, perfect for extracting different types of information concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"Simple word count function\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "# Create different analysis prompts\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the sentiment of this text (positive/negative/neutral): {text}\"\n",
    ")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Provide a one-sentence summary of: {text}\"\n",
    ")\n",
    "\n",
    "# Create parallel analysis chains\n",
    "sentiment_chain = sentiment_prompt | chatgpt | StrOutputParser()\n",
    "summary_chain = summary_prompt | chatgpt | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"Databricks provides excellent tools for building scalable data pipelines and machine learning solutions efficiently.\"\n",
    "# RunnableLambda(count_words).invoke({\"text\": sample_text})\n",
    "\n",
    "RunnableLambda(count_words).invoke( sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into parallel processing\n",
    "parallel_analyzer = RunnableParallel(\n",
    "    word_count=RunnableLambda(count_words),\n",
    "    sentiment=sentiment_chain,\n",
    "    summary=summary_chain\n",
    ")\n",
    "\n",
    "# Test parallel processing\n",
    "sample_text = \"Databricks provides excellent tools for building scalable data pipelines and machine learning solutions efficiently.\"\n",
    "results = parallel_analyzer.invoke({\"text\": sample_text})\n",
    "\n",
    "print(\"=== PARALLEL ANALYSIS RESULTS ===\")\n",
    "for analysis_type, result in results.items():\n",
    "    print(f\"{analysis_type.title()}: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
