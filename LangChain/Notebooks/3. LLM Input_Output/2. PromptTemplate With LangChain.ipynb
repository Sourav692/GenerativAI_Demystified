{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain==0.3.20\n",
    "!pip install -qq langchain-openai==0.2.12\n",
    "!pip install -qq langchain-community==0.3.11\n",
    "!pip install -qq openai==1.55.3\n",
    "!pip install -qq faiss-cpu\n",
    "!pip install -qq tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Basic Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch.\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n",
      "You are an expert in deep learning and PyTorch.\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = \"\"\"You are an expert in deep learning and PyTorch.\n",
    "You answer queries by being brief, bright, and concise.\n",
    "\n",
    "Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "# instantiate using the initializer\n",
    "prompt_template = PromptTemplate(input_variables = ['query'],template = template)\n",
    "prompt_template.pretty_print()\n",
    "\n",
    "# Another way to creating PromptTemplate is to use PromptTemplate.from_template\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch.\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: What is the best way to learn PyTorch?\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt\n",
    "formatted_prompt = prompt_template.format(query=\"What is the best way to learn PyTorch?\")\n",
    "print(formatted_prompt)\n",
    "print(type(formatted_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='You are an expert in deep learning and PyTorch.\\nYou answer queries by being brief, bright, and concise.\\n\\nQuery: What is the best way to learn PyTorch?\\n'\n",
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt1 = prompt_template.format_prompt(query=\"What is the best way to learn PyTorch?\")\n",
    "print(formatted_prompt1)\n",
    "print(type(formatted_prompt1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the Prompt to LLM Model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to learn PyTorch is to follow these steps:\n",
      "\n",
      "1. **Official Documentation**: Start with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for foundational concepts and tutorials.\n",
      "\n",
      "2. **Online Courses**: Enroll in courses like \"Deep Learning with PyTorch\" on platforms like Coursera, Udacity, or Fast.ai.\n",
      "\n",
      "3. **Hands-On Projects**: Build small projects or replicate existing ones from GitHub to apply what you learn.\n",
      "\n",
      "4. **Books**: Read books like \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann.\n",
      "\n",
      "5. **Community Engagement**: Join forums like PyTorch Discuss, Stack Overflow, or Reddit to ask questions and share knowledge.\n",
      "\n",
      "6. **Practice**: Regularly solve problems on platforms like Kaggle to reinforce your skills.\n",
      "\n",
      "Consistency and practice are key!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChatOpenAI model for text generation\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.0)\n",
    "\n",
    "# Invoke the model with the formatted prompt and get response.Pass the Prompt Value directly to the LLM model\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to learn PyTorch is to follow these steps:\n",
      "\n",
      "1. **Official Documentation**: Start with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for foundational concepts and tutorials.\n",
      "\n",
      "2. **Online Courses**: Enroll in courses like \"Deep Learning with PyTorch\" on platforms like Coursera, Udacity, or Fast.ai.\n",
      "\n",
      "3. **Hands-On Projects**: Build small projects or replicate existing ones from GitHub to apply what you learn.\n",
      "\n",
      "4. **Books**: Read books like \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann.\n",
      "\n",
      "5. **Community Engagement**: Join forums like PyTorch Discuss, Stack Overflow, or Reddit to ask questions and share knowledge.\n",
      "\n",
      "6. **Practice**: Regularly solve problems on platforms like Kaggle to reinforce your skills.\n",
      "\n",
      "Consistency and practice are key!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(formatted_prompt1)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the Prompt to a Chain - Use PromptTemplate instead of PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to learn PyTorch is to follow these steps:\n",
      "\n",
      "1. **Official Documentation**: Start with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for foundational concepts and tutorials.\n",
      "\n",
      "2. **Online Courses**: Enroll in courses like \"Deep Learning with PyTorch\" on platforms like Coursera, Udacity, or Fast.ai.\n",
      "\n",
      "3. **Hands-On Projects**: Build small projects or replicate existing ones from GitHub to apply what you learn.\n",
      "\n",
      "4. **Books**: Read books like \"Deep Learning with PyTorch\" by Eli Stevens, Luca Antiga, and Thomas Viehmann.\n",
      "\n",
      "5. **Community Engagement**: Join forums like PyTorch Discuss, Stack Overflow, or Reddit to ask questions and share knowledge.\n",
      "\n",
      "6. **Practice**: Regularly solve problems on platforms like Kaggle to reinforce your skills.\n",
      "\n",
      "Consistency and practice are key!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"query\": \"What is the best way to learn PyTorch?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Input Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Movie Title:** The Dark Knight: Shadows of Gotham\n",
      "\n",
      "**Genre:** Action/Thriller\n",
      "\n",
      "**Synopsis:**\n",
      "In the heart of Gotham City, a new wave of crime threatens to engulf the streets as a mysterious figure known only as \"The Puppeteer\" emerges, manipulating the city's underworld with a sinister agenda. With the Joker locked away and the city still reeling from past chaos, Batman (Christian Bale) finds himself facing a foe unlike any he has encountered beforeâ€”one who knows how to exploit the fears and weaknesses of both the criminals and the citizens of Gotham.\n",
      "\n",
      "As The Puppeteer orchestrates a series of high-stakes heists and public spectacles, Batman must navigate a web of deception that tests his resolve and moral code. With the help of Commissioner Gordon and the resourceful but conflicted Catwoman, he delves deeper into the shadows, uncovering a conspiracy that reaches the highest echelons of power.\n",
      "\n",
      "As the city spirals into chaos, Batman must confront not only the external threats but also his own inner demons. The line between hero and vigilante blurs as he grapples with the consequences of his actions and the impact they have on those he loves. In a race against time, Batman must unmask The Puppeteer before Gotham is plunged into darkness forever.\n",
      "\n",
      "With breathtaking action sequences, psychological tension, and a gripping narrative, \"The Dark Knight: Shadows of Gotham\" explores the complexities of justice, sacrifice, and the enduring battle between light and darkness.\n"
     ]
    }
   ],
   "source": [
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template = \"\"\"Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\"\"\",\n",
    "        input_variables = ['movie_title', 'main_actor']\n",
    "        \n",
    "    )\n",
    "    \n",
    "    formatted_prompt = prompt.format(movie_title = movie_title, main_actor = main_actor)\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "movie_info = get_movie_information(\"The Dark Knight\", \"Christian Bale\")\n",
    "print(movie_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat the same function usingwith_template and using LCEL | operato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Movie Title:** Amritsar: 1984\n",
      "\n",
      "**Genre:** Historical Drama / Thriller\n",
      "\n",
      "**Synopsis:**\n",
      "\n",
      "Set against the backdrop of the turbulent events of 1984 in India, \"Amritsar: 1984\" follows the gripping journey of Harpreet Singh (played by Gurdaas Mann), a devoted schoolteacher and a loving father living in the heart of Amritsar. As the city becomes a focal point of political unrest and communal tension, Harpreet finds himself torn between his love for his family and his duty to his community.\n",
      "\n",
      "When the Indian government launches Operation Blue Star to remove militants from the Golden Temple, Harpreet's life is turned upside down. His younger brother, a passionate activist, becomes embroiled in the conflict, leading to a rift between the siblings. As violence erupts and the city descends into chaos, Harpreet must navigate the treacherous waters of loyalty, faith, and survival.\n",
      "\n",
      "With the backdrop of the Golden Temple's sacredness and the haunting memories of a divided nation, Harpreet embarks on a desperate quest to save his brother and protect his family. Along the way, he encounters a diverse cast of characters, including a courageous journalist seeking the truth, a conflicted soldier grappling with his orders, and a wise elder who offers guidance amidst the turmoil.\n",
      "\n",
      "As the situation escalates, Harpreet's resolve is tested, forcing him to confront his beliefs and the harsh realities of a world in conflict. \"Amritsar: 1984\" is a poignant exploration of love, sacrifice, and the human spirit's resilience in the face of adversity, culminating in a heart-wrenching climax that will leave audiences reflecting on the cost of war and the power of hope. \n",
      "\n",
      "With Gurdaas Mann's powerful performance, the film captures the essence of a pivotal moment in history, blending personal stories with the larger narrative of a nation in crisis.**Movie Title:** Amritsar: 1984\n",
      "\n",
      "**Genre:** Historical Drama / Thriller\n",
      "\n",
      "**Synopsis:**\n",
      "\n",
      "Set against the backdrop of the turbulent events of 1984 in India, \"Amritsar: 1984\" follows the gripping journey of Harpreet Singh (played by Gurdaas Mann), a devoted schoolteacher and a loving father living in the heart of Amritsar. As the city becomes a focal point of political unrest and communal tension, Harpreet finds himself torn between his love for his family and his commitment to his community.\n",
      "\n",
      "When the Indian government launches Operation Blue Star to remove militants from the Golden Temple, Harpreet's life is turned upside down. His son, a passionate young activist, becomes embroiled in the conflict, leading to a heartbreaking rift between father and son. As violence erupts and the city descends into chaos, Harpreet must navigate the treacherous waters of loyalty, faith, and survival.\n",
      "\n",
      "With the backdrop of the Golden Temple's sacredness and the haunting memories of loss, Harpreet embarks on a desperate quest to save his son and protect his family. Along the way, he encounters a diverse cast of characters, including a journalist seeking the truth, a soldier grappling with his orders, and a local shopkeeper trying to maintain peace amidst the turmoil.\n",
      "\n",
      "\"Amritsar: 1984\" is a poignant exploration of the human spirit in the face of adversity, showcasing the resilience of a community and the bonds of family that can withstand even the darkest of times. As Harpreet fights to reclaim his family's future, the film serves as a powerful reminder of the cost of conflict and the enduring hope for reconciliation. \n",
      "\n",
      "With Gurdaas Mann's compelling performance, the film captures the emotional depth and cultural richness of a pivotal moment in history, leaving audiences reflecting on the importance of understanding and compassion in a divided world.\n"
     ]
    }
   ],
   "source": [
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
    "\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    for chunk in llm_chain.stream({\"movie_title\":movie_title,\"main_actor\":main_actor}):\n",
    "      print(chunk, end=\"\", flush=True)\n",
    "    response = llm_chain.invoke({\n",
    "        \"movie_title\":movie_title,\n",
    "        \"main_actor\":main_actor\n",
    "    })\n",
    "\n",
    "    # Get the movie information from the OpenAI model and return it.\n",
    "    return response\n",
    "    \n",
    "print(get_movie_information(movie_title=\"Amritsar:1984\", main_actor=\"Gurdaas Mann\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ§± 1. PromptTemplate (for Text Completion LLMs)**\n",
    "\n",
    "### **âœ… What it does:**\n",
    "\n",
    "Builds a **formatted prompt** with placeholders.\n",
    "\n",
    "### **âœ… When to use:**\n",
    "\n",
    "When using text-davinci, gpt-3, Claude, or any **non-chat LLM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate this sentence to French: I love programming.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt with variables\n",
    "prompt = PromptTemplate.from_template(\"Translate this sentence to French: {sentence}\")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt.format(sentence=\"I love programming.\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"What is the best way to learn PyTorch?\" in French is: \"Quelle est la meilleure faÃ§on d'apprendre PyTorch ?\"\n"
     ]
    }
   ],
   "source": [
    "fmt_prompt = prompt.invoke({\"sentence\":\"What is the best way to learn PyTorch?\"})\n",
    "response = llm.invoke(fmt_prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ðŸ’¬ 2. ChatPromptTemplate (for Chat Models like GPT-3.5, GPT-4)**\n",
    "\n",
    "### **âœ… What it does:**\n",
    "\n",
    "Creates **multi-turn prompts** in a chat format using **roles** like system, user, and ai.\n",
    "\n",
    "### **âœ… When to use:**\n",
    "\n",
    "When using **OpenAIâ€™s chat models**, like GPT-3.5/4 (gpt-4), **Anthropic Claude**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful assistant that translates text to French.\n",
      "Human: Translate this sentence: I love programming.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat prompt template with system and user messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that translates text to French.\"),\n",
    "    (\"user\", \"Translate this sentence: {sentence}\")\n",
    "])\n",
    "\n",
    "# Format the chat prompt\n",
    "formatted_chat_prompt = chat_prompt.format(sentence=\"I love programming.\")\n",
    "print(formatted_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful assistant that translates text to French.' additional_kwargs={} response_metadata={}\n",
      "content='Translate this sentence: I love programming.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "# Format the chat prompt\n",
    "formatted_chat_prompt = chat_prompt.invoke({\"sentence\":\"I love programming.\"})\n",
    "for msg in formatted_chat_prompt.messages:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates text to French.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Translate this sentence: I love programming.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_chat_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompt_values.ChatPromptValue"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format_prompt(sentence=\"I love programming.\")\n",
    "type(formatted_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example of ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Yo! Wassup nephew.\"),\n",
    "    (\"ai\", \"As an AI language model, I am incapable of being your nephew.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(template)\n",
    "# langchain_core.prompts.chat.ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'user_input']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['name', 'user_input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Yo! Wassup nephew.'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='As an AI language model, I am incapable of being your nephew.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Yo! Wassup nephew.'), additional_kwargs={}),\n",
       " AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='As an AI language model, I am incapable of being your nephew.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is Robotalker.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yo! Wassup nephew.', additional_kwargs={}, response_metadata={}), AIMessage(content='As an AI language model, I am incapable of being your nephew.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Talk robo to me!', additional_kwargs={}, response_metadata={})]\n",
      "Beep boop! Initiating RoboTalk mode! ðŸ¤– Whatâ€™s on your mind, human? Need some data, a joke, or maybe a fun fact? Just say the word!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Beep boop! Initiating RoboTalk mode! ðŸ¤– Whatâ€™s on your mind, human? Need some data, a joke, or maybe a fun fact? Just say the word!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = template.format_messages(\n",
    "    name=\"Robotalker\",\n",
    "    user_input=\"Talk robo to me!\"\n",
    ")\n",
    "print(messages)\n",
    "\n",
    "# [SystemMessage(content='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is Robotalker.'),\n",
    "# HumanMessage(content='Yo! Wassup nephew.'),\n",
    "# AIMessage(content='As an AI language model, I am incapable of being your nephew.'),\n",
    "#HumanMessage(content='Talk robo to me!')]\n",
    "\n",
    "print(llm.invoke(messages).content)\n",
    "# Beep boop! Let's chat about all things robotic and techy. Got any burning questions about robots or artificial intelligence?\n",
    "\n",
    "# use LCEL\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"})\n",
    "# \"Beep boop! What's shakin', human friend?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\", additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Another way to create Template using SystemMessage and HumanMessagePromptTemplate\n",
    "\n",
    "system_message = SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "template = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables\n",
    "# ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, thatâ€™s your opinion, isnâ€™t it? But honestly, who cares? If you donâ€™t like Sam I Am, just move on. There are plenty of other characters out there.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(template.format_messages(text=\"That Sam I Am, I do not like that Sam I Am...\"))\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well, tough luck! Dr. Seuss isn't going to write itself, you know. If you don't like Sam I Am, maybe you should just find something else to read. What do you want, a medal for your opinion?\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | llm | StrOutputParser()\n",
    "chain.invoke({\"text\":\"That Sam I Am, I do not like that Sam I Am...\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PromptTemplate vs PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PromptTemplate ---\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='Write a joke about {topic}'\n",
      "\n",
      "--- PromptValue ---\n",
      "text='Write a joke about penguins'\n",
      "\n",
      "--- Final Prompt String ---\n",
      "Write a joke about penguins\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(\"Write a joke about {topic}\")\n",
    "\n",
    "# Step 2: Use format_prompt() to create a PromptValue\n",
    "prompt_value = prompt_template.format_prompt(topic=\"penguins\")\n",
    "\n",
    "print(\"--- PromptTemplate ---\")\n",
    "print(prompt_template)\n",
    "print(\"\\n--- PromptValue ---\")\n",
    "print(prompt_value)\n",
    "print(\"\\n--- Final Prompt String ---\")\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ðŸ§  When to Use Each**\n",
    "\n",
    "- **Use PromptTemplate**:\n",
    "    - When designing **reusable and parameterized prompts**.\n",
    "    - For building **dynamic chains** with variables.\n",
    "- **Use PromptValue**:\n",
    "    - When you want to **pass a fully formatted prompt** to an LLM manually.\n",
    "    - When chaining LLMs directly: llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ§ª BONUS: Use PromptValue directly with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do ducks have feathers?\n",
      "\n",
      "To cover their butt quacks!\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {animal}\")\n",
    "])\n",
    "\n",
    "# Create a PromptValue (chat prompt formatted)\n",
    "prompt_value = chat_prompt.format_prompt(animal=\"ducks\")\n",
    "\n",
    "# Send PromptValue to LLM directly\n",
    "response = llm.invoke(prompt_value)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using format_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "\n",
    "# Convert to PromptValue\n",
    "prompt_value = template.format_prompt(country=\"France\")\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompt_values.StringPromptValue"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using format() â†’ Return Str not a PromptValue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of Germany?\n"
     ]
    }
   ],
   "source": [
    "formatted_string = template.format(country=\"Germany\")\n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(formatted_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ChatPromptTemplate:Â .format_prompt() â†’ returns ChatPromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You're a helpful assistant.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about penguins', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "chat_prompt_value = chat_template.format_prompt(topic=\"penguins\")\n",
    "\n",
    "print(chat_prompt_value.to_messages())  # List of ChatMessages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompt_values.ChatPromptValue"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You're a helpful assistant.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about penguins', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt_value.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Invoke()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "Write a poem about stars\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write a poem about {topic}\")\n",
    "\n",
    "# Using invoke to get PromptValue\n",
    "prompt_value = prompt.invoke({\"topic\": \"stars\"})\n",
    "print(type(prompt_value))\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You're a helpful assistant.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about ducks', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You're a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me a joke about {animal}\")\n",
    "])\n",
    "\n",
    "# Using invoke\n",
    "chat_prompt_value = chat_prompt.invoke({\"animal\": \"ducks\"})\n",
    "print(chat_prompt_value.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use Inside LCEL Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a cat joke for you:\n",
      "\n",
      "Why was the cat sitting on the computer?\n",
      "\n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = chat_template | llm | parser\n",
    "print(chain.invoke({\"topic\": \"cats\"}))  # `format_prompt()` happens internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
