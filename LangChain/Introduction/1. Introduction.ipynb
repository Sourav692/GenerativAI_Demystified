{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc651ec5-a7f0-4cfe-b4f2-18d9eb5c488c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff6bfc09-d93d-407f-98c9-65dbff528232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f6f9013-b58d-4e55-b563-1e35747e2d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e3f4601-f77c-4535-81a2-2970248c1bd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Introduction of LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1f07b6-e0a3-4efb-9b1a-b87eed7e3aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the LLM\n",
    "\n",
    "    llm = OpenAI(model = 'gpt-3.5-turbo-instruct')\n",
    "\n",
    "    # Get the response\n",
    "    response = llm.invoke('What is the capital of France?')\n",
    "    print(response)\n",
    "    \n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    # Use llm.generate() moethod to generate response\n",
    "    alt_response = llm.generate([{\"What is LangChain?\"}])\n",
    "    print(alt_response.generations[0][0].text)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c46bc7a5-9933-4dfb-b124-5c80b8877673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### First LangChain Project \n",
    "\n",
    "A LangChain pipeline is a sequence of steps where inputs are processed to produce outputs. For your first project, you’ll create a basic pipeline that uses an LLM to generate text responses based on user input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39546b1c-5818-4969-ad53-abc8f3c6a85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Build a pipeline that: \n",
    "* Accepts a user’s question as input.  \n",
    "* Uses a pre-trained LLM to generate a response.   \n",
    "* Outputs the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15eb1c3-a4d3-4daa-9a6d-6fe6ef583bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3c1421-93bf-40cc-a040-b387c7aeb86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    \n",
    "    # Step 1: Initialize the LLM\n",
    "    llm = OpenAI(model = 'gpt-3.5-turbo-instruct')\n",
    "\n",
    "    # Step 2 : Create a Prompt Template\n",
    "    prompts = PromptTemplate(\n",
    "        input_variables= ['topic'],\n",
    "        template= \"Provide me details about {topic}?\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Create a LLMChain\n",
    "    chain = LLMChain(llm = llm, prompt = prompts)\n",
    "\n",
    "    # Step 4: Generate the response\n",
    "    response = chain.invoke('Generative AI')\n",
    "\n",
    "    resp = chain.generate([{'topic': 'Deep Learning'}])\n",
    "    resp1 = chain.run([{'topic': 'Deep Learning'}])\n",
    "    \n",
    "    print(response[\"text\"])\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(resp.generations[0][0].text)\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(resp1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bc02a62-48bd-449b-82fe-8e97663f527a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modify the above project\n",
    "* After generating the response, summarize it into a shorter version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f4690bf-6a59-4ef5-9215-d965a4714d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain,SequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "try:\n",
    "    \n",
    "    # Step 1: Initialize the LLM\n",
    "    llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "\n",
    "    # Step 2: Create Prompt Templates\n",
    "    detail_prompt = PromptTemplate(\n",
    "        input_variables=['topic'],\n",
    "        template=\"Provide me details about {topic}?\"\n",
    "    )\n",
    "\n",
    "    summary_prompt = PromptTemplate(\n",
    "        input_variables=['text'],\n",
    "        template=\"Summarize the following text into a single concise sentence: {text}\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Create LLM Chains\n",
    "    answer_chain = LLMChain(llm=llm, prompt=detail_prompt,verbose=True)\n",
    "    summary_chain = LLMChain(llm=llm, prompt=summary_prompt,verbose=True)\n",
    "\n",
    "    # Step 4: Combine the Chains\n",
    "    pipeline = SimpleSequentialChain(chains=[answer_chain, summary_chain])\n",
    "\n",
    "    # Step 5: Generate the Response\n",
    "    response = pipeline.run('Deep Learning')\n",
    "\n",
    "    print(response)\n",
    "        \n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd8ba57-330a-4583-9a7e-26f90244ae62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add a Sentiment Analysis Step at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d8f7865-ea5f-4a0b-8790-fefc0f03f0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=['summary'],\n",
    "    template=\"Analyze the sentiment of the following text: {summary}\"\n",
    ")\n",
    "\n",
    "sentiment_prompt = LLMChain(llm=llm, prompt=sentiment_prompt,verbose=True)\n",
    "\n",
    "pipeline = SimpleSequentialChain(chains=[answer_chain, summary_chain,sentiment_prompt],verbose=True)\n",
    "\n",
    "response = pipeline.run('Deep Learning')\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c761b6d4-cf34-4863-ab87-c5ab7041f912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add a Dynamic User Input Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15a4378-0cdb-42b9-96ea-0f3818031e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain,SequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Step 1: Initialize the LLM\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')\n",
    "\n",
    "# Step 2: Create Prompt Templates\n",
    "detail_prompt = PromptTemplate(\n",
    "    input_variables=['topic'],\n",
    "    template=\"Provide me details about {topic}?\"\n",
    ")\n",
    "\n",
    "answer_chain = LLMChain(llm=llm, prompt=detail_prompt,verbose=True)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter the topic you want to know more about(or type 'exit' to quit): \")\n",
    "    \n",
    "    # Exit the loop if the user types 'exit'\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the assistant.GoodBye!!\")\n",
    "        break\n",
    "    \n",
    "    response = answer_chain.run(user_input)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110563cf-cd75-4f6f-9f00-1fe3b6c9ff95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1. Introduction",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
