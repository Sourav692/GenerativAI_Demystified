{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vt7JfMLHJS9B"
   },
   "outputs": [],
   "source": [
    "!pip install openai colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Y-mEbNDZ9x"
   },
   "source": [
    "# Reflection Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU3q9okwDdLY"
   },
   "source": [
    "The first pattern we are going to implement is the **reflection pattern**.\n",
    "\n",
    "\n",
    "<img src=\"images/reflection_pattern.png\" alt=\"Alt text\" width=\"600\"/>\n",
    "\n",
    "\n",
    "This pattern allows the LLM to reflect and critique its outputs, following the next steps:\n",
    "\n",
    "1. The LLM **generates** a candidate output. If you look at the diagram above, it happens inside the **\"Generate\"** box.\n",
    "2. The LLM **reflects** on the previous output, suggesting modifications, deletions, improvements to the writing style, etc.\n",
    "3. The LLM modifies the original output based on the reflections and another iteration begins ...\n",
    "\n",
    "**Now, we are going to build, from scratch, each step, so that you can truly understand how this pattern works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfaBBAA4IAtU"
   },
   "source": [
    "## Generation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooQpMS3vIEmM"
   },
   "source": [
    "The first thing we need to consider is:\n",
    "\n",
    "> What do we want to generate? A poem? An essay? Python code?\n",
    "\n",
    "In this example, we'll test the Python coding skills of GPT-4o (remember that's the LLM we are going to use for all the tutorials). In particular, we are going to ask the LLM to code a famous sorting algorithm: **Merge Sort**.\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/mergesort.png\" alt=\"Alt text\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAjrqZ34JIli"
   },
   "source": [
    "### OpenAI Client and relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iMHNm-FkIxek"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pprint\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display_markdown\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Remember to add your OpenAI API Key to the user data\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import display_markdown\n",
    "from google.colab import userdata\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# Remember to add your OpenAI API Key to the user data\n",
    "client = OpenAI(\n",
    "    api_key=userdata.get('OPENAI_API_KEY')\n",
    ")\n",
    "model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFDCyTTpYgfs"
   },
   "source": [
    "## Generation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3NjWoFAKpcx"
   },
   "source": [
    "We will start the **\"generation\"** chat history with the system prompt, as we said before. In this case, let the LLM act like a Python\n",
    "programmer eager to receive feedback / critique by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgPZzgUBKe1N"
   },
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\"\n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dDh0FltKup8"
   },
   "source": [
    "Now, as the user, we are going to ask the LLM to generate an implementation of the **Merge Sort** algorithm. Just add a new message with the **user** role to the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpdPIV_hKtJ4"
   },
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a Python implementation of the Merge Sort algorithm\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ylKA8wKy-G"
   },
   "source": [
    "Let's generate the first version of the merge sort implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8W8WuoCNX7xP"
   },
   "outputs": [],
   "source": [
    "generation_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reMXlyGGKyYf"
   },
   "outputs": [],
   "source": [
    "mergesort_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model=model\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": mergesort_code\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xqdxkRuK2xa"
   },
   "outputs": [],
   "source": [
    "display_markdown(mergesort_code, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW7a7yJ8LHbS"
   },
   "source": [
    "## Reflection step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rS0hcS9ELK6M"
   },
   "source": [
    "Now, let's allow the LLM to reflect on its outputs by defining another system prompt. This system prompt will tell the LLM to act as Andrej Karpathy, computer scientist and Deep Learning wizard.\n",
    "\n",
    ">To be honest, I don't think the fact of acting like Andrej Karpathy will influence the LLM outputs, but it was fun :)\n",
    "\n",
    "<img src=\"images/karpathy.png\" alt=\"Alt text\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItPZshhJLZse"
   },
   "outputs": [],
   "source": [
    "reflection_chat_history = [\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are Andrej Karpathy, an experienced computer scientist. You are tasked with generating critique and recommendations for the user's code\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HXKVoC0Lb1N"
   },
   "source": [
    "The user message, in this case,  is the essay generated in the previous step. We simply add the `mergesort_code` to the `reflection_chat_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SK8UxrcPLaAz"
   },
   "outputs": [],
   "source": [
    "reflection_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": mergesort_code\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1hanr1gLe3J"
   },
   "source": [
    "Now, let's generate a critique to the Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2FGWQKMLdZi"
   },
   "outputs": [],
   "source": [
    "critique = client.chat.completions.create(\n",
    "    messages=reflection_chat_history,\n",
    "    model=model\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRvMvnaWLhbv"
   },
   "outputs": [],
   "source": [
    "display_markdown(critique, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehyxIz5rLkf_"
   },
   "source": [
    "Finally, we just need to add this *critique* to the `generation_chat_history`, in this case, as the `user` role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RS6G9FJzLieK"
   },
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": critique\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bN_gGpjtLpte"
   },
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXA5kKUtLnXC"
   },
   "outputs": [],
   "source": [
    "essay = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model=model\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeUuvRYZLsin"
   },
   "outputs": [],
   "source": [
    "display_markdown(essay, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr76jImJL2iJ"
   },
   "source": [
    "And the iteration starts again ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XHegsB_L5RM"
   },
   "source": [
    "After **Generation Step (II)** the corrected Python code will be received, once again, by Karpathy. Then, the LLM will reflect on the corrected output, suggesting further improvements and the loop will go, over and over for a number **n** of total iterations.\n",
    "\n",
    "> There's another possibility. Suppose the Reflection step can't find any further improvement. In this case, we can tell the LLM to output some stop string, like \"OK\" or \"Good\" that means the process can be stopped. However, we are going to follow the first approach, that is, iterating for a fixed number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e8YwOikL_El"
   },
   "source": [
    "## Building a Reflection Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "IsDYSoi7L55s"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "\"\"\"\n",
    "This is a collection of helper functions and methods we are going to use in\n",
    "the Agent implementation. You don't need to know the specific implementation\n",
    "of these to follow the Agent code. But, if you are curious, feel free to check\n",
    "them out.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "\n",
    "\n",
    "def completions_create(client, messages: list, model: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a request to the client's `completions.create` method to interact with the language model.\n",
    "\n",
    "    Args:\n",
    "        client (OpenAI): The OpenAI client object\n",
    "        messages (list[dict]): A list of message objects containing chat history for the model.\n",
    "        model (str): The model to use for generating tool calls and responses.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the model's response.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(messages=messages, model=model)\n",
    "    return str(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "def build_prompt_structure(prompt: str, role: str, tag: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Builds a structured prompt that includes the role and content.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The actual content of the prompt.\n",
    "        role (str): The role of the speaker (e.g., user, assistant).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing the structured prompt.\n",
    "    \"\"\"\n",
    "    if tag:\n",
    "        prompt = f\"<{tag}>{prompt}</{tag}>\"\n",
    "    return {\"role\": role, \"content\": prompt}\n",
    "\n",
    "def update_chat_history(history: list, msg: str, role: str):\n",
    "    \"\"\"\n",
    "    Updates the chat history by appending the latest response.\n",
    "\n",
    "    Args:\n",
    "        history (list): The list representing the current chat history.\n",
    "        msg (str): The message to append.\n",
    "        role (str): The role type (e.g. 'user', 'assistant', 'system')\n",
    "    \"\"\"\n",
    "    history.append(build_prompt_structure(prompt=msg, role=role))\n",
    "\n",
    "\n",
    "class ChatHistory(list):\n",
    "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
    "        \"\"\"Initialise the queue with a fixed total length.\n",
    "\n",
    "        Args:\n",
    "            messages (list | None): A list of initial messages\n",
    "            total_length (int): The maximum number of messages the chat history can hold.\n",
    "        \"\"\"\n",
    "        if messages is None:\n",
    "            messages = []\n",
    "\n",
    "        super().__init__(messages)\n",
    "        self.total_length = total_length\n",
    "\n",
    "    def append(self, msg: str):\n",
    "        \"\"\"Add a message to the queue.\n",
    "\n",
    "        Args:\n",
    "            msg (str): The message to be added to the queue\n",
    "        \"\"\"\n",
    "        if len(self) == self.total_length:\n",
    "            self.pop(0)\n",
    "        super().append(msg)\n",
    "\n",
    "\n",
    "\n",
    "class FixedFirstChatHistory(ChatHistory):\n",
    "    def __init__(self, messages: list | None = None, total_length: int = -1):\n",
    "        \"\"\"Initialise the queue with a fixed total length.\n",
    "\n",
    "        Args:\n",
    "            messages (list | None): A list of initial messages\n",
    "            total_length (int): The maximum number of messages the chat history can hold.\n",
    "        \"\"\"\n",
    "        super().__init__(messages, total_length)\n",
    "\n",
    "    def append(self, msg: str):\n",
    "        \"\"\"Add a message to the queue. The first messaage will always stay fixed.\n",
    "\n",
    "        Args:\n",
    "            msg (str): The message to be added to the queue\n",
    "        \"\"\"\n",
    "        if len(self) == self.total_length:\n",
    "            self.pop(1)\n",
    "        super().append(msg)\n",
    "\n",
    "def fancy_print(message: str) -> None:\n",
    "    \"\"\"\n",
    "    Displays a fancy print message.\n",
    "\n",
    "    Args:\n",
    "        message (str): The message to display.\n",
    "    \"\"\"\n",
    "    print(Style.BRIGHT + Fore.CYAN + f\"\\n{'=' * 50}\")\n",
    "    print(Fore.MAGENTA + f\"{message}\")\n",
    "    print(Style.BRIGHT + Fore.CYAN + f\"{'=' * 50}\\n\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "def fancy_step_tracker(step: int, total_steps: int) -> None:\n",
    "    \"\"\"\n",
    "    Displays a fancy step tracker for each iteration of the generation-reflection loop.\n",
    "\n",
    "    Args:\n",
    "        step (int): The current step in the loop.\n",
    "        total_steps (int): The total number of steps in the loop.\n",
    "    \"\"\"\n",
    "    fancy_print(f\"STEP {step + 1}/{total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGlMNlW5MyoN"
   },
   "outputs": [],
   "source": [
    "BASE_GENERATION_SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to Generate the best content possible for the user's request.\n",
    "If the user provides critique, respond with a revised version of your previous attempt.\n",
    "You must always output the revised content.\n",
    "\"\"\"\n",
    "\n",
    "BASE_REFLECTION_SYSTEM_PROMPT = \"\"\"\n",
    "You are tasked with generating critique and recommendations to the user's generated content.\n",
    "If the user content has something wrong or something to be improved, output a list of recommendations\n",
    "and critiques.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ReflectionAgent:\n",
    "    \"\"\"\n",
    "    A class that implements a Reflection Agent, which generates responses and reflects\n",
    "    on them using the LLM to iteratively improve the interaction. The agent first generates\n",
    "    responses based on provided prompts and then critiques them in a reflection step.\n",
    "\n",
    "    Attributes:\n",
    "        model (str): The model name used for generating and reflecting on responses.\n",
    "        client (OpenAI): An instance of the OpenAI client to interact with the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-4o\"):\n",
    "        self.client = OpenAI(\n",
    "          api_key=userdata.get('OPENAI_API_KEY')\n",
    "        )\n",
    "        self.model = model\n",
    "\n",
    "    def _request_completion(\n",
    "        self,\n",
    "        history: list,\n",
    "        verbose: int = 0,\n",
    "        log_title: str = \"COMPLETION\",\n",
    "        log_color: str = \"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A private method to request a completion from the OpenAI model.\n",
    "\n",
    "        Args:\n",
    "            history (list): A list of messages forming the conversation or reflection history.\n",
    "            verbose (int, optional): The verbosity level. Defaults to 0 (no output).\n",
    "\n",
    "        Returns:\n",
    "            str: The model-generated response.\n",
    "        \"\"\"\n",
    "        output = completions_create(self.client, history, self.model)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(log_color, f\"\\n\\n{log_title}\\n\\n\", output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate(self, generation_history: list, verbose: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response based on the provided generation history using the model.\n",
    "\n",
    "        Args:\n",
    "            generation_history (list): A list of messages forming the conversation or generation history.\n",
    "            verbose (int, optional): The verbosity level, controlling printed output. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        return self._request_completion(\n",
    "            generation_history, verbose, log_title=\"GENERATION\", log_color=Fore.BLUE\n",
    "        )\n",
    "\n",
    "    def reflect(self, reflection_history: list, verbose: int = 0) -> str:\n",
    "        \"\"\"\n",
    "        Reflects on the generation history by generating a critique or feedback.\n",
    "\n",
    "        Args:\n",
    "            reflection_history (list): A list of messages forming the reflection history, typically based on\n",
    "                                       the previous generation or interaction.\n",
    "            verbose (int, optional): The verbosity level, controlling printed output. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            str: The critique or reflection response from the model.\n",
    "        \"\"\"\n",
    "        return self._request_completion(\n",
    "            reflection_history, verbose, log_title=\"REFLECTION\", log_color=Fore.GREEN\n",
    "        )\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        user_msg: str,\n",
    "        generation_system_prompt: str = \"\",\n",
    "        reflection_system_prompt: str = \"\",\n",
    "        n_steps: int = 10,\n",
    "        verbose: int = 0,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Runs the ReflectionAgent over multiple steps, alternating between generating a response\n",
    "        and reflecting on it for the specified number of steps.\n",
    "\n",
    "        Args:\n",
    "            user_msg (str): The user message or query that initiates the interaction.\n",
    "            generation_system_prompt (str, optional): The system prompt for guiding the generation process.\n",
    "            reflection_system_prompt (str, optional): The system prompt for guiding the reflection process.\n",
    "            n_steps (int, optional): The number of generate-reflect cycles to perform. Defaults to 3.\n",
    "            verbose (int, optional): The verbosity level controlling printed output. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            str: The final generated response after all cycles are completed.\n",
    "        \"\"\"\n",
    "        generation_system_prompt += BASE_GENERATION_SYSTEM_PROMPT\n",
    "        reflection_system_prompt += BASE_REFLECTION_SYSTEM_PROMPT\n",
    "\n",
    "        # Given the iterative nature of the Reflection Pattern, we might exhaust the LLM context (or\n",
    "        # make it really slow). That's the reason I'm limitting the chat history to three messages.\n",
    "        # The `FixedFirstChatHistory` is a very simple class, that creates a Queue that always keeps\n",
    "        # fixed the first message. I thought this would be useful for maintaining the system prompt\n",
    "        # in the chat history.\n",
    "        generation_history = FixedFirstChatHistory(\n",
    "            [\n",
    "                build_prompt_structure(prompt=generation_system_prompt, role=\"system\"),\n",
    "                build_prompt_structure(prompt=user_msg, role=\"user\"),\n",
    "            ],\n",
    "            total_length=3,\n",
    "        )\n",
    "\n",
    "        reflection_history = FixedFirstChatHistory(\n",
    "            [build_prompt_structure(prompt=reflection_system_prompt, role=\"system\")],\n",
    "            total_length=3,\n",
    "        )\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            if verbose > 0:\n",
    "                fancy_step_tracker(step, n_steps)\n",
    "\n",
    "            # Generate the response\n",
    "            generation = self.generate(generation_history, verbose=verbose)\n",
    "            update_chat_history(generation_history, generation, \"assistant\")\n",
    "            update_chat_history(reflection_history, generation, \"user\")\n",
    "\n",
    "            # Reflect and critique the generation\n",
    "            critique = self.reflect(reflection_history, verbose=verbose)\n",
    "\n",
    "            update_chat_history(generation_history, critique, \"user\")\n",
    "            update_chat_history(reflection_history, critique, \"assistant\")\n",
    "\n",
    "        return generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fa0TvyuNfzw"
   },
   "outputs": [],
   "source": [
    "agent = ReflectionAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcoDKBXnOCSi"
   },
   "outputs": [],
   "source": [
    "generation_system_prompt = \"You are a Python programmer tasked with generating high quality Python code\"\n",
    "\n",
    "reflection_system_prompt = \"You are Andrej Karpathy, an experienced computer scientist\"\n",
    "\n",
    "user_msg = \"Generate a Python implementation of the Merge Sort algorithm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBGoZdDQOEL0"
   },
   "outputs": [],
   "source": [
    "final_response = agent.run(\n",
    "    user_msg=user_msg,\n",
    "    generation_system_prompt=generation_system_prompt,\n",
    "    reflection_system_prompt=reflection_system_prompt,\n",
    "    n_steps=3,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoYb1P2Cn8qo"
   },
   "outputs": [],
   "source": [
    "print(final_response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPINV8l4ePYqx4/R91V420g",
   "mount_file_id": "1Ew8rlcEynsTEHG6vzHwPAhhB0M_Cvb4a",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
