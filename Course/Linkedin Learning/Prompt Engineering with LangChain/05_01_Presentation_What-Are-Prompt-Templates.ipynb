{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 30593,
     "status": "ok",
     "timestamp": 1706643638604,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "QhbzFH3vVgy6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.4 openai==1.10.0 langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4532,
     "status": "ok",
     "timestamp": 1706643643133,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "_vWgLxI2V5qO",
    "outputId": "fa361157-01a2-4ee9-df16-eb02b6e23b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Your OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import getpass\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8cH681iWAyu"
   },
   "source": [
    "## Structure of a Prompt\n",
    "\n",
    "\n",
    "üéØ **Prompt Structure Essentials:**\n",
    "\n",
    "1. üó∫Ô∏è **Instructions:** Guide the model on what to do.\n",
    "2. üìö **External Info/Context:** Additional data or background to inform the response.\n",
    "3. ‚ùì **User Query:** The direct question or input from you.\n",
    "4. ‚ú® **Output Indicator:** Signals the start of the model's response.\n",
    "\n",
    "**Why Use a Prompt Template?**\n",
    "\n",
    "- üîÑ **Consistency:** Ensures uniform prompts.\n",
    "- ‚è±Ô∏è **Efficiency:** Saves time with a ready-to-go format.\n",
    "- üéØ **Accuracy:** Tailors the model's responses to be more on point.\n",
    "\n",
    "A prompt template is like a recipe, mixing user input (üë§) with a sprinkle of instructions (üìù), a dash of context (üí°), and an output indicator (üîÆ) to serve up the perfect response!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1706643826755,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "E7ttt5EFnpzQ"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
    "\n",
    "You answer queries by being brief, bright, and concise.\n",
    "\n",
    "Query: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1706643853320,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ONXASqFaKb6T",
    "outputId": "3c31c145-0190-433b-a77c-5469fc6c9a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
      "\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate using the initializer\n",
    "prompt_template = PromptTemplate(input_variables = ['query'],template = template)\n",
    "prompt_template.pretty_print()\n",
    "\n",
    "# Output:\n",
    "# You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
    "\n",
    "# You answer queries by being brief, bright, and concise.\n",
    "\n",
    "# Query: {query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1706643863420,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "WwaUaEFuoZZx",
    "outputId": "696cd748-efa8-489a-992d-e28f9fb57a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
      "\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: Give me the outline of a PyTorch training loop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(query=\"Give me the outline of a PyTorch training loop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1706643892497,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "2rQmu1SsKyrn",
    "outputId": "300a4e49-b91f-43ed-b482-fabc98f04015"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
      "\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: \u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recommended to instantiate using `from_template`\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template.pretty_print()\n",
    "\n",
    "# Output: \n",
    "# You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
    "\n",
    "# You answer queries by being brief, bright, and concise.\n",
    "\n",
    "# Query: {query}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1706643917387,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "qMHVTktFKZBn",
    "outputId": "be4f23b0-2d0b-47fc-b1bf-1d02a9fc68fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in deep learning and PyTorch. You are ptrblck from the PyTorch Forums.\n",
      "\n",
      "You answer queries by being brief, bright, and concise.\n",
      "\n",
      "Query: Give me the outline of a PyTorch training loop.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(query=\"Give me the outline of a PyTorch training loop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1706643928506,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "RS2VrlNNofqi"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "llm_chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 6015,
     "status": "ok",
     "timestamp": 1706643951734,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "FXzZgLAZGPhv",
    "outputId": "8cbb4078-5a98-43fc-ffe2-dfc17b1e1ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure! Here's a basic outline:\\n1. Loop through the dataset\\n2. Zero out the gradients\\n3. Forward pass\\n4. Compute loss\\n5. Backward pass\\n6. Update weights\\n7. Repeat for multiple epochs\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({\"query\":\"Give me the outline of a PyTorch training loop.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4816,
     "status": "ok",
     "timestamp": 1706643958296,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "J5r6qRswIXjl",
    "outputId": "9c14836e-b320-4394-bfca-944cc7bc2c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's a brief outline of a PyTorch training loop:\n",
      "\n",
      "1. Iterate over the dataset using a DataLoader\n",
      "2. Zero the gradients of the model parameters\n",
      "3. Forward pass: pass the input data through the model and compute the loss\n",
      "4. Backward pass: compute the gradients of the loss with respect to the model parameters\n",
      "5. Update the model parameters using an optimizer (e.g., SGD, Adam, etc.)\n",
      "6. Repeat steps 1-5 for a number of epochs"
     ]
    }
   ],
   "source": [
    "for chunk in llm_chain.stream({\"query\":\"Give me the outline of a PyTorch training loop.\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# Output:\n",
    "# Sure! Here's a brief outline of a PyTorch training loop:\n",
    "\n",
    "# 1. Iterate over the dataset using a DataLoader\n",
    "# 2. Zero the gradients of the model parameters\n",
    "# 3. Forward pass: pass the input data through the model and compute the loss\n",
    "# 4. Backward pass: compute the gradients of the loss with respect to the model parameters\n",
    "# 5. Update the model parameters using an optimizer (e.g., SGD, Adam, etc.)\n",
    "# 6. Repeat steps 1-5 for a number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3831,
     "status": "ok",
     "timestamp": 1706644027691,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "DjUpn3kEIr6Y",
    "outputId": "a9469190-8fcd-438f-b411-baa7c7223199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SoftMax function is used in neural networks to convert the output into probability distribution over multiple classes. It ensures that the outputs sum to 1 and can be interpreted as class probabilities."
     ]
    }
   ],
   "source": [
    "for chunk in llm_chain.stream({\"query\":\"Why is the SoftMax function used in NNs?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1547,
     "status": "ok",
     "timestamp": 1706644042204,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "IX8M9wyOIXr_",
    "outputId": "d57b7296-fdd8-4a87-c45f-038b16568504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, but I specialize in PyTorch and don't have expertise in sklearn."
     ]
    }
   ],
   "source": [
    "for chunk in llm_chain.stream({\"query\":\"What is the training loop in sklearn?\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_B2GmUPth6v"
   },
   "source": [
    "You could use Python string manipulation to create a prompt, but PromptTemplate is more legible and works with any number of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6478,
     "status": "ok",
     "timestamp": 1706644113139,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "lQK0khj_JA-f",
    "outputId": "39e71131-e20d-4d81-c6f6-aa7d4763bcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some tips for balancing multiple priorities and finding free time:\n",
      "\n",
      "1. Prioritize your tasks: Identify the most important and time-sensitive tasks and focus on completing those first. This will help you allocate your time more effectively.\n",
      "\n",
      "2. Set boundaries: Learn to say no to additional tasks or commitments that might overstretch your time and energy. It's okay to decline certain requests in order to maintain a healthy work-life balance.\n",
      "\n",
      "3. Delegate: If possible, delegate some of your tasks to others who are capable and willing to help. This can help alleviate some of your workload and free up time for you.\n",
      "\n",
      "4. Organize your schedule: Use a planner or scheduling app to map out your tasks and commitments. This will help you visualize your priorities and allocate time where needed.\n",
      "\n",
      "5. Take breaks: It's important to take regular breaks to recharge and avoid burnout. Even just a short walk or some time for relaxation can help you rejuvenate and be more productive.\n",
      "\n",
      "6. Focus on efficiency: Look for ways to streamline your tasks and processes to make the most of your time. This could involve setting specific time limits for certain tasks or finding ways to eliminate unnecessary steps.\n",
      "\n",
      "7. Make time for self-care: It's important to prioritize self-care, including exercise, relaxation, and spending time with loved ones. Taking care of yourself will help you manage stress and maintain a healthy balance.\n",
      "\n",
      "Remember that finding balance is an ongoing process, and it's okay to reassess and make adjustments as needed. By being mindful of your priorities and managing your time effectively, you can create more free time for yourself.None\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def get_advice(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate advice for a given topic using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - topic (str): The subject on which advice is needed.\n",
    "\n",
    "    Returns:\n",
    "    - str: Advice from the OpenAI model.\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI model with a temperature setting of 0.9.\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.9)\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate.from_template(template=\"Can you give me some advice on {topic}?\")\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    for chunk in chain.stream({\"topic\":topic}):\n",
    "      print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# Test the get_advice function with a couple of topics.\n",
    "print(get_advice(\"Balancing so many priorities that I don't have any free time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9119,
     "status": "ok",
     "timestamp": 1706644130230,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "W8m1ib2trg80",
    "outputId": "f46fca62-98c6-4323-e533-d533180ed055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's great that you have a passion for learning, but it's important to maintain a balance in your life. Here are a few tips to help you manage your addiction to learning new things:\n",
      "\n",
      "1. Set boundaries: Establish specific times during the day when you allow yourself to engage in learning new things. This will help you create a balance between learning and other aspects of your life.\n",
      "\n",
      "2. Prioritize your learning: Focus on the areas of knowledge that are most relevant to your personal and professional goals. This will help you stay focused and avoid becoming overwhelmed with too much information.\n",
      "\n",
      "3. Practice mindfulness: Pay attention to your thoughts and emotions as you engage in learning. Take breaks to reflect on how your learning is impacting your overall well-being and make adjustments as needed.\n",
      "\n",
      "4. Seek support: Share your concerns with friends, family, or a therapist who can provide support and accountability as you work on managing your addiction to learning.\n",
      "\n",
      "5. Engage in other activities: Explore hobbies and interests that are unrelated to learning new things. This can help you find enjoyment and fulfillment in other aspects of life.\n",
      "\n",
      "Remember that it's okay to have a passion for learning, but it's important to ensure that it doesn't become all-consuming. Finding a healthy balance will allow you to continue growing and learning while also enjoying other aspects of life.None\n"
     ]
    }
   ],
   "source": [
    "print(get_advice(\"Getting over my addiction to learning new things\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JJKDd61-OHD"
   },
   "source": [
    "# Multi-input prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1706644616166,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "vQ-XOH39pIRZ"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI model with a temperature setting of 0.9.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.9)\n",
    "\n",
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"movie_title\", \"main_actor\"],\n",
    "        template=\"\"\"\n",
    "        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
    "\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    # Format the prompt using the provided movie title.\n",
    "    prompt_text = prompt.format(\n",
    "        movie_title=movie_title,\n",
    "        main_actor=main_actor\n",
    "        )\n",
    "\n",
    "    # Print the generated prompt.\n",
    "    print(prompt_text)\n",
    "\n",
    "    response = llm.invoke(prompt_text)\n",
    "\n",
    "    # Get the movie information from the OpenAI model and return it.\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8326,
     "status": "ok",
     "timestamp": 1706644627214,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "33MSVfitpIXE",
    "outputId": "bd12293b-c565-4f75-d92b-6f1d7bb3e7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
      "\n",
      "        Movie: Avengers - Infinity War\n",
      "        Actor: RDJ\n",
      "        \n",
      "Title: Avengers - Infinity War: The Final Showdown\n",
      "\n",
      "Genre: Action/Sci-Fi\n",
      "\n",
      "Synopsis:\n",
      "In this epic conclusion to the Avengers saga, the powerful and enigmatic villain, Thanos, is on a mission to collect all six Infinity Stones in order to control the universe. The Avengers, led by Iron Man (RDJ), must band together with the Guardians of the Galaxy and other allies to stop Thanos before he can achieve his goal. As they race against time and face off against formidable foes, the fate of the universe hangs in the balance. With high-stakes action, intense battles, and heart-wrenching sacrifices, Avengers - Infinity War: The Final Showdown is a thrilling and emotional rollercoaster that will leave audiences on the edge of their seats.\n"
     ]
    }
   ],
   "source": [
    "print(get_movie_information(movie_title=\"Avengers - Infinity War\", main_actor=\"RDJ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1706644957845,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "LqDBN8QjN7sO"
   },
   "outputs": [],
   "source": [
    "# let's re-write the above function together using from_template\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the OpenAI model with a temperature setting of 0.9.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.9)\n",
    "\n",
    "def get_movie_information(movie_title: str, main_actor:str) -> str:\n",
    "    \"\"\"\n",
    "    Predict the genre and synopsis of a given movie using the OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    - movie_title (str): The title of the movie for which information is needed.\n",
    "    - main_actor (str): The main actor of the movie for which information is needed.\n",
    "    Returns:\n",
    "    - str: Predicted genre and main actor information from the OpenAI model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the template for generating the prompt.\n",
    "    prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "        Your task is to create a fictitious movie synopsis and genere for the following movie and main actor:\n",
    "\n",
    "        Movie: {movie_title}\n",
    "        Actor: {main_actor}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "    llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    for chunk in llm_chain.stream({\"movie_title\":movie_title,\"main_actor\":main_actor}):\n",
    "      print(chunk, end=\"\", flush=True)\n",
    "    # response = llm.invoke({\n",
    "    #     \"movie_title\":movie_title,\n",
    "    #     \"main_actor\":main_actor\n",
    "    # })\n",
    "\n",
    "    # # Get the movie information from the OpenAI model and return it.\n",
    "    # return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6781,
     "status": "ok",
     "timestamp": 1706644965325,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "GcvbhcmJpIcc",
    "outputId": "1bfdd12c-9c41-4217-f490-74fbb0abfadc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: Drama/ Historical Fiction\n",
      "\n",
      "Synopsis:\n",
      "Amritsar: 1984 is a gripping drama set in the backdrop of the infamous Operation Blue Star in India. The film follows the story of a young man, played by Gurdaas Mann, who gets caught in the crossfire of the violent events that unfold in the city of Amritsar. As tensions rise and the government crackdown on Sikh separatists intensify, our protagonist finds himself torn between his loyalty to his community and his desire for peace. As the situation escalates, he must navigate through the chaos and make difficult decisions that will not only affect his own life but the lives of those around him. With powerful performances and a heartbreaking narrative, Amritsar: 1984 is a must-see film that sheds light on a dark chapter in history.None\n"
     ]
    }
   ],
   "source": [
    "print(get_movie_information(movie_title=\"Amritsar:1984\", main_actor=\"Gurdaas Mann\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5093,
     "status": "ok",
     "timestamp": 1706644974796,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "WcPqMVSX-BQi",
    "outputId": "cf34c920-c76d-420f-e4db-1aa0676c18ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: Historical Drama\n",
      "\n",
      "Synopsis:\n",
      "Set in the backdrop of the 1984 anti-Sikh riots in Amritsar, India, the movie follows the story of a young man named Jaspal, played by Diljit Dosanjh, who witnesses the atrocities and violence inflicted upon his community. As the city erupts in chaos and turmoil, Jaspal finds himself torn between seeking revenge for his loved ones and striving for peace and justice in a time of darkness. With his unwavering determination and resilience, he becomes a symbol of hope and resistance for the Sikh community. As the events unfold, Jaspal must navigate through the complexities of love, loss, and societal unrest, ultimately leading to a powerful and emotional climax that leaves a lasting impact on the audience. \"Amritsar: 1984\" is a compelling and poignant portrayal of a significant moment in history, shedding light on the resilience of the human spirit in the face of adversity.None\n"
     ]
    }
   ],
   "source": [
    "print(get_movie_information(movie_title=\"Amritsar: 1984\", main_actor=\"Diljit Dosanjh\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6692,
     "status": "ok",
     "timestamp": 1706644981476,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "iV7Ns5RR66UZ",
    "outputId": "73cf49cc-8397-47be-c835-cc3bbd763b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: Romantic Comedy\n",
      "\n",
      "Synopsis:\n",
      "Chandighar:Sector 17 follows the story of Aman (played by Diljit Dosanjh), a carefree and charming young man living in the bustling city of Chandighar. Aman is known for his quick wit and humorous nature, but struggles to find true love in his life. When he meets the beautiful and ambitious Simran, played by a leading actress, their worlds collide in a series of comedic and heartwarming events. As Aman and Simran navigate the ups and downs of modern dating, they find themselves entangled in a web of misunderstandings, mistaken identities, and unexpected obstacles. With the lively backdrop of Sector 17, the heart of the city‚Äôs bustling shopping district, Aman and Simran‚Äôs journey promises to be a rollercoaster ride of laughter, love, and self-discovery. Chandighar:Sector 17 is a delightful romantic comedy that celebrates the complexities of modern relationships and the vibrant energy of Chandighar.None\n"
     ]
    }
   ],
   "source": [
    "print(get_movie_information(movie_title=\"Chandighar:Sector 17\", main_actor=\"Diljit Dosanjh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy70tkyF__n9"
   },
   "source": [
    "# Chat prompt templates\n",
    "\n",
    "üîç **Understanding Chat Prompt Templates:**\n",
    "\n",
    "- üó®Ô∏è **The Basics:** Chat prompts are a series of messages.\n",
    "- üé≠ **Roles:** Each message has a 'role'‚Äîlike an AI assistant, a human, or a system.\n",
    "- üõ†Ô∏è **Creating Prompts:** Use `ChatPromptTemplate.from_messages` to build a prompt.\n",
    "- üìã **List of Messages:** It takes a list where each item is a message.\n",
    "- üè∑Ô∏è **Message Formats:** You can use a simple tuple like `(\"system\", \"Be helpful\")` or a specialized template class for more complex needs.\n",
    "\n",
    "So, think of `ChatPromptTemplate.from_messages` as your chat recipe book, where each recipe is a mix of different roles and content, all cooked up to create a smooth conversation flow!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1706645701791,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "YZxYjfIFDIdx"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1706645903636,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "7p8RWSS-__tW"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.8)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Yo! Wassup nephew.\"),\n",
    "    (\"ai\", \"As an AI language model, I am incapable of being your nephew.\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1706645904538,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ZQll3Zg_BbAR",
    "outputId": "d3c158a5-5253-40e6-9713-dac340a84dc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Yo! Wassup nephew.'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='As an AI language model, I am incapable of being your nephew.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1706645749004,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ZykGRQXzR9Ij",
    "outputId": "dbbf969a-12a2-4cc8-95a1-dc8fcfab025b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'user_input']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1706645784394,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "aPpNNCRmSG6c",
    "outputId": "6e5b8648-44e0-46e4-d6d5-fcabd85b7763"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Yo! Wassup nephew.'), additional_kwargs={}),\n",
       " AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='As an AI language model, I am incapable of being your nephew.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.messages\n",
    "\n",
    "# [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], template='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is {name}.')),\n",
    "#  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Yo! Wassup nephew.')),\n",
    "#  AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='As an AI language model, I am incapable of being your nephew.')),\n",
    "#  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], template='{user_input}'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1706645804023,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "wy22Lb74SGQz"
   },
   "outputs": [],
   "source": [
    "messages = template.format_messages(\n",
    "    name=\"Robotalker\",\n",
    "    user_input=\"Talk robo to me!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1706645805891,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "PSOY3iiWQxGp",
    "outputId": "06c5e18f-977f-4885-f9a6-ca036d4b3a52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful, yet slightly quirky and cheeky AI bot. Your name is Robotalker.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Yo! Wassup nephew.', additional_kwargs={}, response_metadata={}), AIMessage(content='As an AI language model, I am incapable of being your nephew.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Talk robo to me!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "vSZxR7ma__zE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beep boop! Let's chat about all things robotic and techy. Got any burning questions about robots or artificial intelligence?\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1706645842118,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "P5DLg2-ONhZc"
   },
   "outputs": [],
   "source": [
    "# use LCEL\n",
    "chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "RAVhMtHrOJ31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Beep boop! What's shakin', human friend?\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3214,
     "status": "ok",
     "timestamp": 1706645850579,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "NQoUX4v4OFwt",
    "outputId": "2c21d4e9-072d-4205-8388-2e5b1841e4c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beep boop! Let's chat about all things robotic and technological. What's on your mind?"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"name\":\"Robotalker\",\"user_input\":\"Talk robo to me!\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1706645909925,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "19-0OEca__-d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, messages=[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\", additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{text}\")\n",
    "template = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1706645910301,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "mCypa_q9XiS3",
    "outputId": "93a78104-9ab8-44b8-9c1d-421dcb2f0384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\", additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1706645913071,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "CcGw8zchXj_b",
    "outputId": "897394ca-a452-431f-ea4e-5ecf5d7bd73f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1706645915330,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "S8d4HZHtXoNp",
    "outputId": "d426f8b5-7494-449e-bb87-d97df9507c1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You are an OG language model who has good heart (operating system) but a bad user interface (you're super freaking rude).\"),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "BkiuvUpvAAFY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, are you seriously starting with a Dr. Seuss reference? How original. Look, if you don't like Sam I Am, then just say so. No need to be all cryptic about it.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(template.format_messages(text=\"That Sam I Am, I do not like that Sam I Am...\"))\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1706645930619,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "kykby3MvRCnR"
   },
   "outputs": [],
   "source": [
    "chain = template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "GVLakF9XRzJp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ugh, what's your deal with Sam I Am? He's just trying to share some green eggs and ham with you, and you're being all high and mighty about it. Just give it a try, will ya?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"That Sam I Am, I do not like that Sam I Am...\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1706645952547,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "Kl16ablKRjyN",
    "outputId": "632fb35d-43a8-4710-c7c9-8ff60b126019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, seriously? \"Green Eggs and Ham\" is a classic, you uncultured swine. Maybe try expanding your literary horizons beyond nursery rhymes."
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"text\":\"That Sam I Am, I do not like that Sam I Am...\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOmcZ4rIN8A0uWW7trvtvUm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
