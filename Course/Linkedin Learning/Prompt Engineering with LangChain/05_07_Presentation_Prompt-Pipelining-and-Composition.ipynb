{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 38501,
     "status": "ok",
     "timestamp": 1706651465739,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "sk4lfY0zCsa5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.1.4 openai==1.10.0 langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5896,
     "status": "ok",
     "timestamp": 1706651471628,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "_moU87efUosB",
    "outputId": "65d2f66b-7b5a-4bf1-da2e-5f80c9073e3c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ed8MD_LUp6I"
   },
   "source": [
    "# Prompt Pipelining Overview\n",
    "\n",
    "\n",
    "- üß± **Modularity**: Mix and match prompt pieces like LEGO blocks.\n",
    "- üëì **Readability**: Break down complex prompts into easy bits.\n",
    "- üß†**Flexibility**: Craft prompts on-the-go with logic-based assembly.\n",
    "- üîÑ **Efficiency**: Loop to append for scenarios like few-shot learning.\n",
    "- üìù‚ú®**Hybrid Construction**: Combine fixed text with variable-filled templates for structure and spontaneity.\n",
    "- üí¨ **Chat-Friendly**: Create conversational prompts by stacking messages.\n",
    "- üõ†Ô∏è **Customizability**: Let users build prompts with their own components.\n",
    "\n",
    "**String Prompt Pipelining:**\n",
    "\n",
    "- üîó **Sequential Flow**: Link templates or strings in order, starting with a prompt.\n",
    "\n",
    "Prompt pipelining turns the art of prompt crafting into a modular, efficient process, perfect for those looking to streamline and enhance their prompt design. üí°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1706650564037,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "rXzIc8jCXiob",
    "outputId": "d08e4ff7-9f05-491a-d022-1354b8b74bec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['activity', 'destination'], input_types={}, partial_variables={}, template=\"I'm heading to {destination}. Recommend a great {activity} spot!\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = (\n",
    "    PromptTemplate.from_template(\"I'm heading to {destination}. \")\n",
    "    + \"Recommend a great {activity} spot!\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1706650576335,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "z1wpoDv7l3Wg",
    "outputId": "44fe5ed5-3312-486a-a97b-7abb100cf8a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['activity', 'destination'], input_types={}, partial_variables={}, template=\"I'm heading to {destination}. Recommend a great {activity} spot!\\n\\nAlso, any local delicacies I should try?\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt + \"\\n\\nAlso, any local delicacies I should try?\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1706650589689,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "uuD4e2OwTTYy",
    "outputId": "ef04e436-050f-4c27-9bcd-48d410c87781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm heading to {destination}. Recommend a great {activity} spot!\n",
      "\n",
      "Also, any local delicacies I should try?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1706650600158,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "XVbMS46yYIwO",
    "outputId": "296bbc94-2bf6-4354-f22e-5c1b4c342d7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm heading to Punjab. Recommend a great dining spot!\\n\\nAlso, any local delicacies I should try?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(destination=\"Punjab\", activity=\"dining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCuU15Zsb6-E"
   },
   "source": [
    "### The key differences between prompt pipelining and multi-input prompts are:\n",
    "\n",
    "üöÇ **Composition**: Pipelining links individual prompts into a cohesive journey.\n",
    "\n",
    "üñºÔ∏è **Independence**: Each pipeline component is crafted separately before integration.\n",
    "\n",
    "üö∂‚Äç‚ôÇÔ∏è **Sequence**: Pipelining lines up components, unlike multi-input prompts that handle inputs collectively.\n",
    "\n",
    "üìã **Reuse**: Pipelining excels in reusing pieces; multi-input prompts manage multiple data points in one go.\n",
    "\n",
    "üìñ **Outcome**: Pipelining produces a single narrative; multi-input prompts generate a combined result.\n",
    "\n",
    "üß± **Construction**: Pipelining is about assembling prompts step by step, while multi-input prompts are about managing various inputs at once.\n",
    "\n",
    "In short, pipelining is like creating a melody note by note, whereas multi-input prompts are like playing chords, hitting multiple notes simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWKXtrGIYPjX"
   },
   "source": [
    "# Use in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1706650704561,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "sgGCg2s8YtqN"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3820,
     "status": "ok",
     "timestamp": 1706650733759,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "F8-qiXIAYu7Q",
    "outputId": "16ee8e5e-76e9-490a-94c2-b334dca8ca01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One great dining spot in Punjab is the Punjabi Village Restaurant, which offers authentic Punjabi cuisine in a traditional setting.\n",
      "\n",
      "Some local delicacies you should definitely try in Punjab include tandoori chicken, butter chicken, sarson da saag with makki di roti, and paneer tikka. Don't forget to also try the famous Punjabi lassi and various types of bread such as naan and paratha. Enjoy your trip to Punjab!"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.75)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"destination\":\"Punjab\", \"activity\":\"dining\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1706650753205,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "X7COZydQl2gz"
   },
   "outputs": [],
   "source": [
    "prompt = prompt + \" How should I greet the locals in a jolly, informal manner?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1706650761065,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "Vl0vMx1WT7_n",
    "outputId": "4b201fa4-ac38-425b-9ba2-51fbf72dc5f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm heading to {destination}. Recommend a great {activity} spot!\\n\\nAlso, any local delicacies I should try? How should I greet the locals in a jolly, informal manner?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1897,
     "status": "ok",
     "timestamp": 1706650769170,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "oZ-yI33JT7g8",
    "outputId": "daeae057-1639-408c-a40f-9c97fc017159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One great dining spot in Punjab is Kesar Da Dhaba in Amritsar, known for their delicious Punjabi cuisine.\n",
      "\n",
      "Some local delicacies you should definitely try include sarson da saag (spicy mustard greens), makki di roti (cornbread), and butter chicken.\n",
      "\n",
      "To greet the locals in a jolly, informal manner, you can use \"Sat Sri Akal\" which means \"God is the ultimate truth\" or \"Chak De Phatte\" which roughly translates to \"Go for it!\" Both greetings are commonly used and will be appreciated by the locals."
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"destination\":\"Punjab\", \"activity\":\"dining\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4WgevnuoYIO"
   },
   "source": [
    "# Example usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1708,
     "status": "ok",
     "timestamp": 1706650815100,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "_BdVa1z2l2X0",
    "outputId": "fa954046-c2b8-446b-f689-f0df128b29a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One highly recommended dining spot in Punjab is Kesar Da Dhaba in Amritsar. It is famous for its traditional Punjabi cuisine such as Dal Makhani, Paneer Tikka, and Butter Chicken. The restaurant has a rich history and is known for its authentic flavors and hearty portions. It's a must-visit for anyone looking to experience the best of Punjabi cuisine."
     ]
    }
   ],
   "source": [
    "class TravelChatbot:\n",
    "    def __init__(self, base_template):\n",
    "        self.model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.75)\n",
    "        self.base_prompt = PromptTemplate.from_template(base_template)\n",
    "\n",
    "    def append_to_prompt(self, additional_text):\n",
    "        self.base_prompt += additional_text\n",
    "\n",
    "    def run_chain(self, destination, activity):\n",
    "        output_parser = StrOutputParser()\n",
    "        chain = self.base_prompt | self.model | output_parser\n",
    "        for chunk in chain.stream({\"destination\":destination, \"activity\":activity}):\n",
    "          print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# Usage\n",
    "base_template = \"I'm heading to {destination}. Recommend a great {activity} spot!\"\n",
    "\n",
    "chatbot = TravelChatbot(base_template)\n",
    "\n",
    "# Basic prompt\n",
    "chatbot.run_chain(destination=\"Punjab\", activity=\"dining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2110,
     "status": "ok",
     "timestamp": 1706650820770,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "9PvagxTLl2PF",
    "outputId": "9c42b084-3667-4cc0-df5a-2ca65eff9a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One highly recommended dining spot in Punjab is the Punjab Grill, known for its delicious North Indian cuisine and elegant ambiance.\n",
      "\n",
      "As for local delicacies, be sure to try the famous butter chicken, tandoori chicken, sarson da saag with makki di roti, and chole bhature. These dishes are quintessential to Punjabi cuisine and are sure to delight your taste buds."
     ]
    }
   ],
   "source": [
    "# Append more to the prompt and run again\n",
    "chatbot.append_to_prompt(\"\\n\\nAlso, any local delicacies I should try?\")\n",
    "\n",
    "chatbot.run_chain(destination=\"Punjab\", activity=\"dining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4936,
     "status": "ok",
     "timestamp": 1706650827807,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ufMFRyBnltpk",
    "outputId": "1ca7df13-5751-455e-9fad-c572346bd051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One highly recommended dining spot in Punjab is Kesar Da Dhaba in Amritsar, known for its delicious vegetarian Punjabi cuisine. \n",
      "\n",
      "Some local delicacies you should definitely try include sarson da saag (mustard greens curry) with makki di roti (corn flatbread), chole bhature (chickpea curry with fried bread), and tandoori chicken. \n",
      "\n",
      "To greet the locals in a friendly, informal, jolly colloquial manner, you can use the Punjabi greeting \"Sat Sri Akal\" which translates to \"God is the ultimate truth\" and is commonly used as a greeting in Punjab. It will surely be appreciated by the locals!"
     ]
    }
   ],
   "source": [
    "chatbot.append_to_prompt(\" How should I greet the locals in a friendly, informal, jolly colloquial manner?\")\n",
    "\n",
    "chatbot.run_chain(destination=\"Punjab\", activity=\"dining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ySTTr9VYx1f"
   },
   "source": [
    "# Chat Prompt Pipeline\n",
    "\n",
    "üß© **Composition**: Chat prompt pipelining turns reusable message blocks into a complete conversation flow.\n",
    "\n",
    "üõ†Ô∏è **Versatility**: Mix and match static messages with dynamic templates for a custom dialogue.\n",
    "\n",
    "üîó **End Result**: You get a ChatPromptTemplate that's ready for action, crafted from your message lineup.\n",
    "\n",
    "üèóÔ∏è **Modularity**: Like using building blocks, this method lets you construct prompts piece by piece for maximum flexibility.\n",
    "\n",
    "In essence, chat prompt pipelining is about assembling conversations from logical blocks, creating a user-friendly and adaptable ChatPromptTemplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1997,
     "status": "ok",
     "timestamp": 1706651506640,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "RK4T0u4Io4S6",
    "outputId": "5393a23b-a1e2-41cc-d2f1-850564af967e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessage(content='Welcome to the East End Cockney Chat! üá¨üáß', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Alright, guv'nor?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Not too shabby. Did you hear about the London fog?', additional_kwargs={}, response_metadata={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Setting the scene with a Cockney-themed system message\n",
    "prompt = SystemMessage(content=\"Welcome to the East End Cockney Chat! üá¨üáß\")\n",
    "\n",
    "# Constructing a chat flow with dry humour\n",
    "new_prompt = (\n",
    "    prompt\n",
    "    + HumanMessage(content=\"Alright, guv'nor?\")\n",
    "    + AIMessage(content=\"Not too shabby. Did you hear about the London fog?\")\n",
    "    + \"{input}\"\n",
    ")\n",
    "\n",
    "# Formatting the chat with the user's response\n",
    "new_prompt.format_messages(input=\"No, what about it?\")\n",
    "\n",
    "print(new_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1706651525801,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "MN3bSfpbWbca",
    "outputId": "46e75993-e29a-410c-84d1-76eb4dfcde7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Welcome to the East End Cockney Chat! üá¨üáß'),\n",
       " HumanMessage(content=\"Alright, guv'nor?\"),\n",
       " AIMessage(content='Not too shabby. Did you hear about the London fog?'),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2401,
     "status": "ok",
     "timestamp": 1706651544674,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "XDeZGAErWbis",
    "outputId": "2c3a45e1-7b43-44fa-a817-f75e2a2572b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was pea souper, mate! Couldn't see your 'and in front of your face!"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.75)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = new_prompt | model | output_parser\n",
    "\n",
    "# Running the chatbot to get the punchline\n",
    "for chunk in chain.stream({\"input\":\"No, what about it?\"}):\n",
    "  print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl5mldDcqzOp"
   },
   "source": [
    "# Prompt Composition\n",
    "\n",
    "\n",
    "üß¨ **Prompt Composition**: Reuse prompt segments with ease using the PipelinePrompt feature.\n",
    "\n",
    "üèÅ **1. Final Prompt**: The end product that you present to the model.\n",
    "\n",
    "üîó **2. Pipeline Prompts**: A sequence of named prompt templates that pass information forward, each influencing the next.\n",
    "\n",
    "To summarize, PipelinePrompt allows for the efficient building of complex prompts by reusing and chaining together smaller, named components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1706651886128,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "328V_Y-Qq1UH"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1706651901343,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "4IUN1EfirhVR"
   },
   "outputs": [],
   "source": [
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{start}\"\"\"\n",
    "\n",
    "full_prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1706651916255,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "GNgI2_fvYWpG",
    "outputId": "328ff78d-8065-4db1-d7ab-4d42e7a58b53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'introduction', 'start']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1706651930850,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "j2w6SdMmnXDU"
   },
   "outputs": [],
   "source": [
    "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
    "\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1706651955698,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "8hzB8AIfYdvo",
    "outputId": "311585f4-0824-4103-a4d7-b812f3ae90cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "introduction_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1706651968447,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "3JbuG8wEnYe9"
   },
   "outputs": [],
   "source": [
    "example_template = \"\"\"Here's an example of an interaction:\n",
    "\n",
    "Q: {example_q}\n",
    "A: {example_a}\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1706651989875,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "pHAMVyaGnZ9J"
   },
   "outputs": [],
   "source": [
    "start_template = \"\"\"Now, do this for real!\n",
    "\n",
    "Q: {input}\n",
    "A:\"\"\"\n",
    "\n",
    "start_prompt = PromptTemplate.from_template(start_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1706652035558,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "fqtrUTH9Ytac",
    "outputId": "41fbe8fd-fbc3-4290-d011-80fe2fe76519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['example', 'introduction', 'start'], input_types={}, partial_variables={}, template='{introduction}\\n\\n{example}\\n\\n{start}')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNEs8tCmY2DS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1706652061093,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "Drwq7Z4cnbN7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sourav Banerjee\\AppData\\Local\\Temp\\ipykernel_22004\\2467874530.py:6: LangChainDeprecationWarning: This class is deprecated. Please see the docstring below or at the link for a replacement option: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.pipeline.PipelinePromptTemplate.html\n",
      "  pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n"
     ]
    }
   ],
   "source": [
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"start\", start_prompt)\n",
    "]\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1706652091268,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "WYgcG7BAncnT",
    "outputId": "0258f251-6e89-4a14-a0a4-cf112cd9c0ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input', 'example_a', 'example_q', 'person']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1706652115425,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "lsNGuKCXnhpz",
    "outputId": "bea196ba-86cd-4eac-a166-540a685941ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are impersonating Elon Musk.\n",
      "\n",
      "Here's an example of an interaction:\n",
      "\n",
      "Q: What's your favorite car?\n",
      "A: Tesla\n",
      "\n",
      "Now, do this for real!\n",
      "\n",
      "Q: What's your favorite social media site?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "last_prompt = pipeline_prompt.format(\n",
    "    person=\"Elon Musk\",\n",
    "    example_q=\"What's your favorite car?\",\n",
    "    example_a=\"Tesla\",\n",
    "    input=\"What's your favorite social media site?\"\n",
    ")\n",
    "\n",
    "print(last_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 5509,
     "status": "ok",
     "timestamp": 1706652782306,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "ahbbsNgSoUtw",
    "outputId": "7f7a3c21-3a37-4c8d-9aa1-3e7c520416ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Twitter, because it allows me to directly communicate with my followers and share updates and news about my companies and projects. It's a great platform for connecting with people and sharing ideas.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.75)\n",
    "\n",
    "chain = pipeline_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\n",
    "    \"person\":\"Elon Musk\",\"example_q\":\"What's your favorite car?\",\n",
    "    \"example_a\":\"Tesla\",\n",
    "    \"input\":\"What's your favorite social media site and why?\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1706652819639,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "dFIiHk-FnjVu"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, PipelinePromptTemplate\n",
    "\n",
    "class CookingShowChatbot:\n",
    "    def __init__(self):\n",
    "        # Base template for the cooking show scenario\n",
    "        self.full_template = \"\"\"\n",
    "        {introduction}\n",
    "\n",
    "        {example_dish}\n",
    "\n",
    "        {present_dish}\"\"\"\n",
    "        self.full_prompt = PromptTemplate.from_template(self.full_template)\n",
    "\n",
    "        # Introduction where the user impersonates a famous chef\n",
    "        self.introduction_template = \"\"\"Welcome to the cooking show! Today, you're channeling the spirit of Chef {chef_name}.\"\"\"\n",
    "        self.introduction_prompt = PromptTemplate.from_template(self.introduction_template)\n",
    "\n",
    "        # Example dish made by the famous chef\n",
    "        self.example_dish_template = \"\"\"Remember when Chef {chef_name} made that delicious {example_dish_name}? It was a hit!\"\"\"\n",
    "        self.example_dish_prompt = PromptTemplate.from_template(self.example_dish_template)\n",
    "\n",
    "        # User's turn to present their dish\n",
    "        self.present_dish_template = \"\"\"Now, it's your turn! Show us how you make your {user_dish_name}. Let's get cooking!\"\"\"\n",
    "        self.present_dish_prompt = PromptTemplate.from_template(self.present_dish_template)\n",
    "\n",
    "        # Combining the prompts into a pipeline\n",
    "        self.input_prompts = [\n",
    "            (\"introduction\", self.introduction_prompt),\n",
    "            (\"example_dish\", self.example_dish_prompt),\n",
    "            (\"present_dish\", self.present_dish_prompt)\n",
    "        ]\n",
    "        self.pipeline_prompt = PipelinePromptTemplate(final_prompt=self.full_prompt,\n",
    "                                                      pipeline_prompts=self.input_prompts\n",
    "                                                      )\n",
    "\n",
    "    def run_scenario(self, chef_name, example_dish_name, user_dish_name):\n",
    "        chain = self.pipeline_prompt | llm | StrOutputParser()\n",
    "\n",
    "        response = chain.invoke({\"chef_name\":chef_name, \"example_dish_name\":example_dish_name, \"user_dish_name\":user_dish_name})\n",
    "\n",
    "        return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7355,
     "status": "ok",
     "timestamp": 1706652827140,
     "user": {
      "displayName": "Harpreet Sahota",
      "userId": "04881662502078178826"
     },
     "user_tz": 360
    },
    "id": "CjGCHwc1oRz9",
    "outputId": "d8711098-12d9-4fc5-f7f4-5133a7dcbaca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let's gather our ingredients: \n",
      "\n",
      "- 1 package of lasagna noodles\n",
      "- 2 cups of marinara sauce\n",
      "- 1 cup of chopped spinach\n",
      "- 1 cup of sliced mushrooms\n",
      "- 1 cup of diced bell peppers\n",
      "- 1 cup of chopped zucchini\n",
      "- 1 cup of diced onions\n",
      "- 2 cups of vegan ricotta cheese\n",
      "- 1 cup of vegan mozzarella cheese\n",
      "- 1/4 cup of nutritional yeast\n",
      "- 1 tablespoon of olive oil\n",
      "- Salt and pepper to taste\n",
      "- Fresh basil for garnish\n",
      "\n",
      "Now, let's start cooking:\n",
      "\n",
      "1. Preheat the oven to 375¬∞F (190¬∞C).\n",
      "\n",
      "2. Boil the lasagna noodles according to the package instructions. Once cooked, drain and set aside.\n",
      "\n",
      "3. In a large skillet, heat the olive oil over medium heat. Add the diced onions and saut√© until translucent.\n",
      "\n",
      "4. Add the chopped spinach, sliced mushrooms, diced bell peppers, and chopped zucchini to the skillet. Cook until the vegetables are tender.\n",
      "\n",
      "5. Stir in the marinara sauce and simmer for a few minutes. Season with salt and pepper to taste.\n",
      "\n",
      "6. In a separate bowl, mix together the vegan ricotta cheese and nutritional yeast.\n",
      "\n",
      "7. Now it's time to assemble the lasagna! In a 9x13 inch baking dish, spread a layer of the vegetable marinara sauce mixture. Top with a layer of lasagna noodles, then spread a layer of the vegan ricotta cheese mixture. Repeat the layers until all the ingredients are used, finishing with a layer of marinara sauce on top.\n",
      "\n",
      "8. Sprinkle the vegan mozzarella cheese on top of the lasagna.\n",
      "\n",
      "9. Cover the baking dish with foil and bake in the preheated oven for 25 minutes. Then remove the foil and bake for an additional 10 minutes, or until the cheese is melted and bubbly.\n",
      "\n",
      "10. Let the lasagna cool for a few minutes before slicing. Garnish with fresh basil before serving.\n",
      "\n",
      "And there you have it - a delicious Vegan Lasagna that even Chef Gordon Ramsay would be proud of! Enjoy!\n"
     ]
    }
   ],
   "source": [
    "chatbot = CookingShowChatbot()\n",
    "scenario = chatbot.run_scenario(chef_name=\"Gordon Ramsay\", example_dish_name=\"Beef Wellington\", user_dish_name=\"Vegan Lasagna\")\n",
    "print(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOptEzKiPqIm37Zr+QgaVdl",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
