{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (416319578.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m pip install --upgrade pip setuptools\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m pip install --upgrade pip setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openbb\n",
      "  Downloading openbb-3.2.4-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting FundamentalAnalysis<0.3.0,>=0.2.6 (from openbb)\n",
      "  Downloading fundamentalanalysis-0.2.14-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting GitPython<4.0.0,>=3.1.17 (from openbb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Jinja2<4.0.0,>=3.0.3 (from openbb)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: Pygments<3.0.0,>=2.11.2 in d:\\2. my workspace and notes\\1. demystify folders\\generativeai-demystified\\.venv\\lib\\site-packages (from openbb) (2.19.1)\n",
      "Collecting Quandl<4.0.0,>=3.6.0 (from openbb)\n",
      "  Downloading Quandl-3.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting alpha-vantage<3.0.0,>=2.3.1 (from openbb)\n",
      "  Downloading alpha_vantage-2.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting arch<6.0.0,>=5.5.0 (from openbb)\n",
      "  Downloading arch-5.6.0.tar.gz (3.4 MB)\n",
      "     ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 1.0/3.4 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.1/3.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.1/3.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.1/3.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.1/3.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.1/3.4 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.4/3.4 MB 2.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ascii-magic<2.0,>=1.6 (from openbb)\n",
      "  Downloading ascii_magic-1.6-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting bs4<0.0.2,>=0.0.1 (from openbb)\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting bt<0.3.0,>=0.2.9 (from openbb)\n",
      "  Downloading bt-0.2.9.tar.gz (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.7/2.7 MB 41.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ccxt<5.0.0,>=4.0.33 (from openbb)\n",
      "  Downloading ccxt-4.4.63-py2.py3-none-any.whl.metadata (130 kB)\n",
      "Collecting degiro-connector<3.0.0,>=2.0.3 (from openbb)\n",
      "  Downloading degiro_connector-2.0.23-py3-none-any.whl.metadata (59 kB)\n",
      "Collecting detecta<0.0.6,>=0.0.5 (from openbb)\n",
      "  Downloading detecta-0.0.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting dnspython<3.0.0,>=2.1.0 (from openbb)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting feedparser<7.0.0,>=6.0.10 (from openbb)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting ffn==0.3.6 (from openbb)\n",
      "  Downloading ffn-0.3.6-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting financedatabase==2.1.0 (from openbb)\n",
      "  Downloading financedatabase-2.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting finnhub-python<3.0.0,>=2.4.10 (from openbb)\n",
      "  Downloading finnhub_python-2.4.22-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting finviz<2.0.0,>=1.4.6 (from openbb)\n",
      "  Downloading finviz-1.4.6.tar.gz (15 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting finvizfinance==0.14.7rc1 (from openbb)\n",
      "  Downloading finvizfinance-0.14.7rc1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting fred<4.0,>=3.1 (from openbb)\n",
      "  Downloading fred-3.1.tar.gz (3.9 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting fredapi<0.5.0,>=0.4.3 (from openbb)\n",
      "  Downloading fredapi-0.4.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting grpcio<2.0.0,>=1.51.1 (from openbb)\n",
      "  Using cached grpcio-1.70.0-cp313-cp313-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting holidays<0.15.0,>=0.14.2 (from openbb)\n",
      "  Downloading holidays-0.14.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting html5lib<2.0,>=1.1 (from openbb)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting intrinio-sdk<7.0.0,>=6.22.2 (from openbb)\n",
      "  Downloading intrinio_sdk-6.35.0-py3-none-any.whl.metadata (935 bytes)\n",
      "Collecting ipympl<0.9.0,>=0.8.2 (from openbb)\n",
      "  Downloading ipympl-0.8.8-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting ipython==8.11.0 (from openbb)\n",
      "  Downloading ipython-8.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting ipywidgets<9.0.0,>=8.0.2 (from openbb)\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting iso8601<0.2.0,>=0.1.14 (from openbb)\n",
      "  Downloading iso8601-0.1.16-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting jupyter-client==7.4.1 (from openbb)\n",
      "  Downloading jupyter_client-7.4.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter-server==1.23.6 (from openbb)\n",
      "  Downloading jupyter_server-1.23.6-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting jupyterlab<4.0.0,>=3.2.4 (from openbb)\n",
      "  Downloading jupyterlab-3.6.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting jupyterlab-widgets<4.0.0,>=3.0.3 (from openbb)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting langchain==0.0.266 (from openbb)\n",
      "  Downloading langchain-0.0.266-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting linearmodels<5.0,>=4.25 (from openbb)\n",
      "  Downloading linearmodels-4.31.tar.gz (1.8 MB)\n",
      "     ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.8/1.8 MB 18.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting llama-index<0.9.0,>=0.8.8 (from openbb)\n",
      "  Downloading llama_index-0.8.42-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.3.3 (from openbb)\n",
      "  Downloading matplotlib-3.10.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting mplfinance<0.13.0,>=0.12.7-alpha.7 (from openbb)\n",
      "  Downloading mplfinance-0.12.10b0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting mstarpy<0.0.5,>=0.0.4 (from openbb)\n",
      "  Downloading mstarpy-0.0.4-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting nixtlats<0.2.0,>=0.1.11 (from openbb)\n",
      "  Downloading nixtlats-0.1.21-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from openbb)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy==1.23.4 (from openbb)\n",
      "  Downloading numpy-1.23.4.tar.gz (10.7 MB)\n",
      "     ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/10.7 MB 64.5 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 1.8/10.7 MB 7.6 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.1/10.7 MB 7.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.7/10.7 MB 7.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 10.0/10.7 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.7/10.7 MB 11.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 105, in _run_wrapper\n",
      "    status = _inner_run()\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 96, in _inner_run\n",
      "    return self.run(options, args)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 379, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "        reqs, check_supported_wheels=not options.target_dir\n",
      "    )\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "                            ~~~~~~~~~~~~~~~~^\n",
      "        collected.requirements, max_rounds=limit_how_complex_resolution_can_be\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 427, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 230, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 174, in __bool__\n",
      "    return any(self)\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 162, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "                       ^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 53, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 187, in _make_candidate_from_link\n",
      "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
      "                                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        link, template, name, version\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 233, in _make_base_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "                                       ~~~~~~~~~~~~~^\n",
      "        link,\n",
      "        ^^^^^\n",
      "    ...<3 lines>...\n",
      "        version=version,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in __init__\n",
      "    super().__init__(\n",
      "    ~~~~~~~~~~~~~~~~^\n",
      "        link=link,\n",
      "        ^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "        version=version,\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 159, in __init__\n",
      "    self.dist = self._prepare()\n",
      "                ~~~~~~~~~~~~~^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 236, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 315, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 527, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 642, in _prepare_linked_requirement\n",
      "    dist = _get_prepared_distribution(\n",
      "        req,\n",
      "    ...<3 lines>...\n",
      "        self.check_build_deps,\n",
      "    )\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 72, in _get_prepared_distribution\n",
      "    abstract_dist.prepare_distribution_metadata(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        finder, build_isolation, check_build_deps\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 56, in prepare_distribution_metadata\n",
      "    self._install_build_reqs(finder)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 126, in _install_build_reqs\n",
      "    build_reqs = self._get_build_requires_wheel()\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\distributions\\sdist.py\", line 103, in _get_build_requires_wheel\n",
      "    return backend.get_requires_for_build_wheel()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 701, in get_requires_for_build_wheel\n",
      "    return super().get_requires_for_build_wheel(config_settings=cs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 166, in get_requires_for_build_wheel\n",
      "    return self._call_hook('get_requires_for_build_wheel', {\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        'config_settings': config_settings\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    })\n",
      "    ^^\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_impl.py\", line 321, in _call_hook\n",
      "    raise BackendUnavailable(data.get('traceback', ''))\n",
      "pip._vendor.pyproject_hooks._impl.BackendUnavailable: Traceback (most recent call last):\n",
      "  File \"D:\\2. My WorkSpace and Notes\\1. Demystify Folders\\GenerativeAI-Demystified\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 77, in _build_backend\n",
      "    obj = import_module(mod_path)\n",
      "  File \"C:\\Users\\Sourav Banerjee\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py\", line 88, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Sourav Banerjee\\AppData\\Local\\Temp\\pip-build-env-onoottg9\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 10, in <module>\n",
      "    import distutils.core\n",
      "ModuleNotFoundError: No module named 'distutils'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install openbb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking and tracing\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x0000029AE1E7A900> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000029AE1E8E510> root_client=<openai.OpenAI object at 0x0000029AE1CAD010> root_async_client=<openai.AsyncOpenAI object at 0x0000029AE1E7AA50> model_name='o1-mini' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"o1-mini\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Agentic AI refers to artificial intelligence systems designed with agency, meaning they have the ability to autonomously make decisions and take actions in pursuit of specific goals. These systems are not just passive tools that respond to direct inputs; instead, they can operate independently, perceive their environment, process information, and adapt to changes without direct human intervention.\\n\\nAgentic AI may encompass a variety of capabilities, such as:\\n\\n1. **Autonomy**: The ability to operate without continuous human guidance.\\n2. **Goal-Directed Behavior**: The capacity to pursue objectives or tasks set by developers or learned through interaction.\\n3. **Adaptability**: The potential to modify behavior in response to new information or environmental changes.\\n4. **Problem-Solving**: The skill to analyze situations and devise strategies to overcome challenges.\\n5. **Decision-Making**: The capability to choose between different courses of action based on various criteria or learned experiences.\\n\\nThese characteristics make agentic AI useful for applications such as robotics, autonomous vehicles, virtual assistants, and complex decision-support systems. However, the development of agentic AI also raises ethical and safety concerns, as these systems can operate with significant independence, potentially leading to unpredictable behaviors or unintended consequences.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 246, 'prompt_tokens': 12, 'total_tokens': 258, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None} id='run-c73926f8-2b3b-424f-b168-beb40b5f1041-0' usage_metadata={'input_tokens': 12, 'output_tokens': 246, 'total_tokens': 258, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic AI refers to artificial intelligence systems designed with agency, meaning they have the ability to autonomously make decisions and take actions in pursuit of specific goals. These systems are not just passive tools that respond to direct inputs; instead, they can operate independently, perceive their environment, process information, and adapt to changes without direct human intervention.\n",
      "\n",
      "Agentic AI may encompass a variety of capabilities, such as:\n",
      "\n",
      "1. **Autonomy**: The ability to operate without continuous human guidance.\n",
      "2. **Goal-Directed Behavior**: The capacity to pursue objectives or tasks set by developers or learned through interaction.\n",
      "3. **Adaptability**: The potential to modify behavior in response to new information or environmental changes.\n",
      "4. **Problem-Solving**: The skill to analyze situations and devise strategies to overcome challenges.\n",
      "5. **Decision-Making**: The capability to choose between different courses of action based on various criteria or learned experiences.\n",
      "\n",
      "These characteristics make agentic AI useful for applications such as robotics, autonomous vehicles, virtual assistants, and complex decision-support systems. However, the development of agentic AI also raises ethical and safety concerns, as these systems can operate with significant independence, potentially leading to unpredictable behaviors or unintended consequences.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a tool developed by Langchain, designed to enhance the development and deployment of applications that utilize Large Language Models (LLMs) and AI agents. It provides a comprehensive suite for testing, monitoring, evaluating, and debugging these AI systems. Langsmith aims to streamline the process of building robust LLM-driven applications by offering features that help developers understand and optimize the performance of their models. This includes capabilities for tracking the behavior of models, diagnosing issues, and iteratively improving application logic and performance.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 102, 'prompt_tokens': 32, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None} id='run-828d3a0a-bfde-4840-927e-153ad1a6cfc2-0' usage_metadata={'input_tokens': 32, 'output_tokens': 102, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm \n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a tool developed by Langchain, designed to enhance the development and deployment of applications that utilize Large Language Models (LLMs) and AI agents. It provides a comprehensive suite for testing, monitoring, evaluating, and debugging these AI systems. Langsmith aims to streamline the process of building robust LLM-driven applications by offering features that help developers understand and optimize the performance of their models. This includes capabilities for tracking the behavior of models, diagnosing issues, and iteratively improving application logic and performance.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a platform developed by LangChain that is designed to assist developers in building and debugging LLM (Large Language Model) applications. It provides a suite of tools and frameworks that streamline the development process, making it easier to create, test, and deploy applications that leverage language models. LangSmith offers features such as logging, testing, and monitoring to help developers understand the behavior of their LLM applications and optimize their performance. By simplifying these aspects, LangSmith aims to enhance the efficiency and effectiveness of developing applications that utilize advanced language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is a platform designed to facilitate language learning and proficiency. It offers a range of tools and resources to help users improve their language skills through interactive exercises, personalized learning paths, and community engagement.', 'features': ['Interactive language exercises', 'Personalized learning paths', 'Community forums and discussions', 'Progress tracking and analytics', 'Access to native language speakers for practice'], 'target_audience': 'Language learners of all levels, from beginners to advanced speakers, who are seeking a comprehensive and engaging way to improve their language skills.', 'availability': 'Available as a web-based platform and mobile application on both iOS and Android.', 'website': 'https://www.langsmith.com'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000029AE1EF8CD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000029AE1EFC550>, root_client=<openai.OpenAI object at 0x0000029AE1E89950>, root_async_client=<openai.AsyncOpenAI object at 0x0000029AE1EF8E10>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x29ae1eb78c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://python.langchain.com/docs/tutorials/llm_chain/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nBuild a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates\\nIn this quickstart we\\'ll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it\\'s just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you\\'ll have a high level overview of:\\n\\n\\nUsing language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet\\'s dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\\n\\nPipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.\\n\\nSelect chat model:Groq‚ñæGroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage\\nAIMessage(content=\\'Ciao!\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 3, \\'prompt_tokens\\': 20, \\'total_tokens\\': 23, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_0705bf87c0\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-32654a56-627c-40e1-a141-ad9350bbfd3e-0\\', usage_metadata={\\'input_tokens\\': 20, \\'output_tokens\\': 3, \\'total_tokens\\': 23, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}})\\ntipIf we\\'ve enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\\nNote that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:\\n\\nlanguage: The language to translate text into\\ntext: The text to translate\\n\\nfrom langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])\\nWe can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\\nThat\\'s it! In this tutorial you\\'ve learned how to create your first simple LLM application. You\\'ve learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we\\'ve got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we\\'ve got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\\n\\nChat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document=loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyTutorialsBuild a simple LLM application with chat models and prompt templatesOn this pageBuild a simple LLM application with chat models and prompt templates'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Using language models\\n\\n\\nUsing prompt templates\\n\\n\\nDebugging and tracing your application using LangSmith\\n\\n\\nLet's dive in!\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebooks. Going through guides in an interactive environment is a great way to better understand them. See here for instructions on how to install.\\nInstallation\\u200b\\nTo install LangChain run:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='PipCondapip install langchainconda install langchain -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGSMITH_TRACING=\"true\"export LANGSMITH_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Or, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGSMITH_TRACING\"] = \"true\"os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\nUsing Language Models\\u200b\\nFirst up, let\\'s learn how to use a language model by itself. LangChain supports many different language models that you can use interchangeably. For details on getting started with a specific model, refer to supported integrations.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Select chat model:Groq‚ñæGroqOpenAIAnthropicAzureGoogle VertexAWSCohereNVIDIAFireworks AIMistral AITogether AIIBM watsonxDatabrickspip install -qU \"langchain[groq]\"import getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain.chat_models import init_chat_modelmodel = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\\nLet\\'s first use the model directly. ChatModels are instances of LangChain Runnables, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the .invoke method.\\nfrom langchain_core.messages import HumanMessage, SystemMessagemessages = [    SystemMessage(\"Translate the following from English into Italian\"),    HumanMessage(\"hi!\"),]model.invoke(messages)API Reference:HumanMessage | SystemMessage'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"AIMessage(content='Ciao!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-32654a56-627c-40e1-a141-ad9350bbfd3e-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\\ntipIf we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the LangSmith trace. The LangSmith trace reports token usage information, latency, standard model parameters (such as temperature), and other information.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke(\"Hello\")model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])model.invoke([HumanMessage(\"Hello\")])\\nStreaming\\u200b\\nBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='for token in model.stream(messages):    print(token.content, end=\"|\")\\n|C|iao|!||\\nYou can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\\u200b\\nRight now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\\nPrompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\\nLet\\'s create a prompt template here. It will take in two user variables:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='language: The language to translate text into\\ntext: The text to translate'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='from langchain_core.prompts import ChatPromptTemplatesystem_template = \"Translate the following from English into {language}\"prompt_template = ChatPromptTemplate.from_messages(    [(\"system\", system_template), (\"user\", \"{text}\")])API Reference:ChatPromptTemplate\\nNote that ChatPromptTemplate supports multiple message roles in a single template. We format the language parameter into the system message, and the user text into a user message.\\nThe input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself\\nprompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})prompt\\nChatPromptValue(messages=[SystemMessage(content=\\'Translate the following from English into Italian\\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\\'hi!\\', additional_kwargs={}, response_metadata={})])'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"We can see that it returns a ChatPromptValue that consists of two messages. If we want to access the messages directly we do:\\nprompt.to_messages()\\n[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]\\nFinally, we can invoke the chat model on the formatted prompt:\\nresponse = model.invoke(prompt)print(response.content)\\nCiao!\\ntipMessage content can contain both text and content blocks with additional structure. See this guide for more information.\\nIf we take a look at the LangSmith trace, we can see exactly what prompt the chat model receives, along with token usage information, latency, standard model parameters (such as temperature), and other information.\\nConclusion\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content=\"Conclusion\\u200b\\nThat's it! In this tutorial you've learned how to create your first simple LLM application. You've learned how to work with language models, how to create a prompt template, and how to get great observability into applications you create with LangSmith.\\nThis just scratches the surface of what you will want to learn to become a proficient AI Engineer. Luckily - we've got a lot of other resources!\\nFor further reading on the core concepts of LangChain, we've got detailed Conceptual Guides.\\nIf you have more specific questions on these concepts, check out the following sections of the how-to guides:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/llm_chain/', 'title': 'Build a simple LLM application with chat models and prompt templates | ü¶úÔ∏èüîó LangChain', 'description': \"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\", 'language': 'en'}, page_content='Chat models\\nPrompt templates\\n\\nAnd the LangSmith docs:\\n\\nLangSmith\\nEdit this pageWas this page helpful?PreviousTutorialsNextBuild a ChatbotSetupJupyter NotebookInstallationLangSmithUsing Language ModelsStreamingPrompt TemplatesConclusionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(document)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x29a82f2de80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore=FAISS.from_documents(documents,embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this quickstart we'll show you how to build a simple LLM application with LangChain. This application will translate text from English into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call!\\nAfter reading this tutorial, you'll have a high level overview of:\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"This is a relatively simple LLM application \"\n",
    "\n",
    "result=vectorstore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sourav Banerjee\\AppData\\Local\\Temp\\ipykernel_35884\\1275562393.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000029A82F9FE10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000029A830FCD60>, model_kwargs={}, openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -U langchain langchain-community\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"What are everyone's favorite colors:\\n\\n{context}\")]\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jesse's favorite color is red and Jamal's favorite color is orange.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "\n",
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"What are everyone's favorite colors:\\n\\n{context}\"), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000029A82F9FE10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000029A830FCD60>, model_kwargs={}, openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"Note that ChatModels receive message objects as inpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\":\"Note that ChatModels receive message objects as input\",\n",
    "    \"context\":[Document(page_content=\"Note that ChatModels receive message objects as input and generate message objects as output. In addition to text content, message objects convey conversational roles and hold important data, such as tool calls and token usage counts.\\nLangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\\nmodel.invoke('Hello')model.invoke([{'role': 'user', 'content': 'Hello'}])model.invoke([HumanMessage('Hello')]) StreamingBecause chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\\nfor token in model.stream(messages):    print(token.content, end='|')|C|iao|!|| You can find more details on streaming chat model outputs in this guide.\\nPrompt Templates\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=retrieval_chain.invoke({\"input\":\"Note that ChatModels receive message objects as input\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
